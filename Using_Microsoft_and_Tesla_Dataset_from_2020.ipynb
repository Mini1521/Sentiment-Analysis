{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mini1521/Sentiment-Analysis/blob/main/Using_Microsoft_and_Tesla_Dataset_from_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANN CODE DOES NOT WORK !!!!**"
      ],
      "metadata": {
        "id": "SUofgR3zdkSZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSSbeztl4SuC"
      },
      "source": [
        "# Stock Data- preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8JXK8qKp3rFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e96bbc7-5dbc-478f-ac9d-aca8b20dd083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged CSV saved as 'merged_data.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the two CSV files\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Company.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/CompanyValues.csv\")\n",
        "\n",
        "# Merge the data on a common column (e.g., 'day_date')\n",
        "merged_df = pd.merge(df1, df2, on=\"ticker_symbol\", how=\"left\")  # Use \"outer\", \"left\", or \"right\" if needed\n",
        "\n",
        "# Save the merged dataset to a new CSV file\n",
        "merged_df.to_csv(\"merged_data.csv\", index=False)\n",
        "\n",
        "print(\"Merged CSV saved as 'merged_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Egmn_3Ea2U1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be030e6-84cd-4e09-fd35-1ad2516e128a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete.\n",
            "Saved: 'msft_2019.csv' and 'tsla_2019.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (update the filename if needed)\n",
        "df = pd.read_csv('/content/merged_data.csv')\n",
        "\n",
        "df.rename(columns={'post_date': 'date'}, inplace=True)\n",
        "\n",
        "# Convert 'day_date' to datetime format\n",
        "df['day_date'] = pd.to_datetime(df['day_date'], errors='coerce')\n",
        "\n",
        "# Filter data for the year 2020\n",
        "df_2019 = df[df['day_date'].dt.year == 2019]\n",
        "df_2019 = df_2019.sort_values(by='day_date', ascending=True)\n",
        "\n",
        "# Separate data for Microsoft (MSFT) and Tesla (TSLA)\n",
        "msft_data = df_2019[df_2019['ticker_symbol'] == 'MSFT']\n",
        "tsla_data = df_2019[df_2019['ticker_symbol'] == 'TSLA']\n",
        "\n",
        "# Drop rows with missing values\n",
        "msft_data = msft_data.dropna()\n",
        "tsla_data = tsla_data.dropna()\n",
        "\n",
        "\n",
        "# print(\" Stock Data Date Range:\")\n",
        "# print(f\"Start Date: {msft_data['day_date'].min()} | End Date: {msft_data['day_date'].max()}\")\n",
        "# print(f\"Start Date: {tsla_data['day_date'].min()} | End Date: {tsla_data['day_date'].max()}\")\n",
        "\n",
        "# Save each dataset separately\n",
        "msft_data.to_csv(\"msft_2019.csv\", index=False)\n",
        "tsla_data.to_csv(\"tsla_2019.csv\", index=False)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "print(\"Saved: 'msft_2019.csv' and 'tsla_2019.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtIxEm3w4aUw"
      },
      "source": [
        "# Social Media - preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3xXgbcP14Fe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "company_tweets = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Company_Tweet.csv\")\n",
        "tweets = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Tweet.csv\")\n",
        "\n",
        "# Merge tweets with company information\n",
        "tweets = tweets.merge(company_tweets, how='left', on='tweet_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S8kNuI0l5lL7"
      },
      "outputs": [],
      "source": [
        "# Convert 'post_date' to datetime format\n",
        "tweets['date'] = pd.to_datetime(tweets['post_date'], unit='s').dt.date\n",
        "tweets['date'] = pd.to_datetime(tweets['date'], errors='coerce')\n",
        "# tweets['time'] = pd.to_datetime(tweets['post_date'], unit='s').dt.time\n",
        "\n",
        "tweets.to_csv(\"merged_tweets.csv\", index=False)\n",
        "\n",
        "tweets.drop(columns=['comment_num', 'retweet_num', 'like_num'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rs-K5Kp-5p_e"
      },
      "outputs": [],
      "source": [
        "# Filter for Microsoft (MSFT) and Tesla (TSLA)\n",
        "tweets_filtered = tweets[tweets['ticker_symbol'].isin(['MSFT', 'TSLA'])]\n",
        "\n",
        "# Keep only tweets from 2019\n",
        "tweets_filtered = tweets_filtered[tweets_filtered['date'].dt.year == 2019]\n",
        "\n",
        "# Split into separate datasets\n",
        "msft_tweets = tweets_filtered[tweets_filtered['ticker_symbol'] == 'MSFT']\n",
        "tsla_tweets = tweets_filtered[tweets_filtered['ticker_symbol'] == 'TSLA']\n",
        "\n",
        "# Save to CSV files\n",
        "msft_tweets.to_csv(\"msft_tweets_2019.csv\", index=False)\n",
        "tsla_tweets.to_csv(\"tsla_tweets_2019.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning of microsoft tweets"
      ],
      "metadata": {
        "id": "v7oM2YYybjpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VajmTQ2t5x6T",
        "outputId": "2446cd50-8a9d-409d-e6be-55c8ee66b01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load Twitter dataset\n",
        "tweets_df = pd.read_csv(\"/content/msft_tweets_2019.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-Gn3mF5f7Ho2"
      },
      "outputs": [],
      "source": [
        "# rename columns\n",
        "tweets_df.rename(columns={'body': 'Tweet'}, inplace=True)\n",
        "tweets_df.rename(columns={'post_date': 'day_date'}, inplace=True)\n",
        "\n",
        "# Droping Duplicates\n",
        "tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "\n",
        "# Handle Missing Values\n",
        "tweets_df.dropna(subset=['Tweet'], inplace=True)  # Drop rows where 'Tweet' is empty\n",
        "\n",
        "# Ensure 'date' is in datetime format\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce')\n",
        "\n",
        "tweets_df = tweets_df.sort_values(by='date', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gjEXLpIW7NTx"
      },
      "outputs": [],
      "source": [
        "# Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "agU-7Vpx7OuU"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning function\n",
        "tweets_df['cleaned_tweet'] = tweets_df['Tweet'].astype(str).apply(clean_text)\n",
        "\n",
        "#Saving the cleaned data for further analysis\n",
        "tweets_df.to_csv(\"cleaned_msft_tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning of tesla tweets"
      ],
      "metadata": {
        "id": "l1SpoBmabqms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7NXDc8W87WJ-"
      },
      "outputs": [],
      "source": [
        "# Load Twitter dataset\n",
        "tweets_df = pd.read_csv(\"/content/tsla_tweets_2019.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GNtXLEMH7fOq"
      },
      "outputs": [],
      "source": [
        "# rename columns\n",
        "tweets_df.rename(columns={'body': 'Tweet'}, inplace=True)\n",
        "tweets_df.rename(columns={'post_date': 'day_date'}, inplace=True)\n",
        "\n",
        "# Droping Duplicates\n",
        "tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "\n",
        "# Handle Missing Values\n",
        "tweets_df.dropna(subset=['Tweet'], inplace=True)  # Drop rows where 'Tweet' is empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gHP_1vVj2_rZ"
      },
      "outputs": [],
      "source": [
        "# Ensure 'date' is in datetime format\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce')\n",
        "\n",
        "tweets_df = tweets_df.sort_values(by='date', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uTsoOO7G7kQB"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "L9RWBI4B7l87"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning function\n",
        "tweets_df['cleaned_tweet'] = tweets_df['Tweet'].astype(str).apply(clean_text)\n",
        "\n",
        "#Saving the cleaned data for further analysis\n",
        "tweets_df.to_csv(\"cleaned_tsla_tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-WWdulg7vQ_"
      },
      "source": [
        "# Microsoft Articles - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "73juevOP7sGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea58e6b4-3028-494b-ec80-0fe9e1b919af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset (Update the filename if needed)\n",
        "article = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/msft_articles.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Al3Cv7Pc8DM7"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "article['date'] = pd.to_datetime(article['date'], errors='coerce')\n",
        "\n",
        "# Filter for only the year 2020\n",
        "year_2020 = article[article['date'].dt.year == 2020]\n",
        "year_2020 = year_2020.sort_values(by='date', ascending=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "year_2020 = year_2020.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Rl597yZM8EZ0"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3GKTyi-w8KAO"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the 'text' column\n",
        "year_2020['text'] = year_2020['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "year_2020.to_csv(\"cleaned_msft_articles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmI7toQa7yyK"
      },
      "source": [
        "# Tesla Articles - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6AAa59TO72XZ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset (Update the filename if needed)\n",
        "article = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/tsla_articles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9X4JjhFH8iJo"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "article['date'] = pd.to_datetime(article['date'], errors='coerce')\n",
        "\n",
        "# Filter for only the year 2020\n",
        "year_2020 = article[article['date'].dt.year == 2020]\n",
        "year_2020 = year_2020.sort_values(by='date', ascending=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "year_2020 = year_2020.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tiV_Rxro8jjZ"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XpPxvRcX8kzb"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the 'text' column\n",
        "year_2020['text'] = year_2020['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "year_2020.to_csv(\"cleaned_tsla_articles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwsJ668S--1y"
      },
      "source": [
        "# Sentiment Analysis- microsoft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "_A_K5_sYf9f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b56b7c-4876-4a88-d461-aa1a33f7f44a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "q1LqstO5Wd20"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load datasets\n",
        "tweets_df = pd.read_csv(\"/content/cleaned_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/cleaned_msft_articles.csv\")\n",
        "\n",
        "# Ensure text column is string type\n",
        "tweets_df['Tweet'] = tweets_df['Tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "p-mrbgZ3WnP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808d3cf6-8e2d-447a-edb7-004c51ac843e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# VADER Sentiment Analysis for Tweets\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment score from VADER\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply VADER sentiment analysis to tweets\n",
        "tweets_df['sentiment'] = tweets_df['Tweet'].apply(get_vader_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M_kjNMV7Ww87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8feb245e-101b-4771-daa2-b570165dfb34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# TextBlob Sentiment Analysis for News\n",
        "\n",
        "# Function to get sentiment score from TextBlob\n",
        "def get_textblob_sentiment(text):\n",
        "    score = TextBlob(text).sentiment.polarity\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply TextBlob sentiment analysis to news articles\n",
        "news_df['sentiment'] = news_df['text'].apply(get_textblob_sentiment)\n",
        "\n",
        "# Save sentiment-labeled datasets\n",
        "tweets_df.to_csv(\"sentiment_msft_tweets.csv\", index=False)\n",
        "news_df.to_csv(\"sentiment_msft_news.csv\", index=False)\n",
        "\n",
        "print(\"Sentiment analysis complete! Results saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM for sentiment Analysis"
      ],
      "metadata": {
        "id": "8z3_YOhb2Zj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(\n",
        "    tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(\n",
        "    news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Function to train models\n",
        "def train_model(X_train, y_train, X_test, y_test, model):\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    predictions = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Model {model.__class__.__name__} Accuracy: {accuracy:.4f}\")\n",
        "    return pipeline\n",
        "\n",
        "# Models to Train\n",
        "models = {\n",
        "    'Naïve Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(kernel='linear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "# Train models for Tweets\n",
        "print(\"\\n Training Models for Tweet Sentiment Classification \")\n",
        "trained_models_tweets = {\n",
        "    name: train_model(X_tweets, y_tweets, X_test_tweets, y_test_tweets, model)\n",
        "    for name, model in models.items()\n",
        "}\n",
        "\n",
        "# Train models for News\n",
        "print(\"\\n Training Models for News Sentiment Classification \")\n",
        "trained_models_news = {\n",
        "    name: train_model(X_news, y_news, X_test_news, y_test_news, model)\n",
        "    for name, model in models.items()\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojPQbUo1q3fz",
        "outputId": "a7b4a126-1830-4faa-9a6a-51202ad4a3dc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Models for Tweet Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.7532\n",
            "Model SVC Accuracy: 0.8911\n",
            "Model DecisionTreeClassifier Accuracy: 0.8086\n",
            "Model RandomForestClassifier Accuracy: 0.8573\n",
            "\n",
            " Training Models for News Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.6032\n",
            "Model SVC Accuracy: 0.6032\n",
            "Model DecisionTreeClassifier Accuracy: 0.4444\n",
            "Model RandomForestClassifier Accuracy: 0.5873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models for Tweet Sentiment Classification\n",
        "* Model MultinomialNB Accuracy: 0.7532\n",
        "* Model SVC Accuracy: 0.8911\n",
        "* Model DecisionTreeClassifier Accuracy: 0.8057\n",
        "* Model RandomForestClassifier Accuracy: 0.8523\n",
        "\n",
        "Training Models for News Sentiment Classification\n",
        "* Model MultinomialNB Accuracy: 0.6032\n",
        "* Model SVC Accuracy: 0.6032\n",
        "* Model DecisionTreeClassifier Accuracy: 0.4762\n",
        "* Model RandomForestClassifier Accuracy: 0.5397\n",
        "\n"
      ],
      "metadata": {
        "id": "tcT7bsxtgd40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the best-performing model for predictions\n",
        "# import joblib\n",
        "# best_model = trained_models['Random Forest']  # Assuming Random Forest performed best\n",
        "# joblib.dump(best_model, \"sentiment_model.pkl\")\n",
        "\n",
        "# print(\"Sentiment Analysis Complete\")"
      ],
      "metadata": {
        "id": "Ng7PGv27E76c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ANN for Sentiment Analysis"
      ],
      "metadata": {
        "id": "-yDBO8Uy2frr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE NOT WORKING**"
      ],
      "metadata": {
        "id": "RJu8FUJZVhQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2EJzTzhwiaV",
        "outputId": "9045a84a-348f-46eb-8a37-0bc392566f0a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed datasets for tweets and news\n",
        "tweets_df = pd.read_csv('/content/sentiment_msft_tweets.csv')  # replace with your actual dataset path\n",
        "news_df = pd.read_csv('/content/sentiment_msft_news.csv')  # replace with your actual news dataset path\n",
        "\n",
        "# Assuming that the preprocessed data has the 'cleaned_text' and 'sentiment' columns already.\n",
        "# Step 4: Encoding Sentiment Labels (assuming 'sentiment' column with Positive, Negative, Neutral)\n",
        "encoder = LabelEncoder()\n",
        "tweets_df['encoded_sentiment'] = encoder.fit_transform(tweets_df['sentiment'])\n",
        "news_df['encoded_sentiment'] = encoder.fit_transform(news_df['sentiment'])\n",
        "\n",
        "y_tweets = tweets_df['encoded_sentiment']\n",
        "y_news = news_df['encoded_sentiment']\n",
        "\n",
        "# Step 5: Tokenizing the Text Data (using preprocessed 'cleaned_text')\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(tweets_df['cleaned_tweet'])\n",
        "X_tweets = tokenizer.texts_to_sequences(tweets_df['cleaned_tweet'])\n",
        "X_tweets = pad_sequences(X_tweets, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(news_df['text'])\n",
        "X_news = tokenizer.texts_to_sequences(news_df['text'])\n",
        "X_news = pad_sequences(X_news, maxlen=100)\n",
        "\n",
        "# Step 6: Splitting Data into Training and Testing Sets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Building ANN Model for Tweets Sentiment Classification\n",
        "# tweet_model = Sequential()\n",
        "# # tweet_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "# tweet_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "# tweet_model.add(SpatialDropout1D(0.2))\n",
        "# tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "# tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "# tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "tweet_model = Sequential()\n",
        "tweet_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "tweet_model.add(SpatialDropout1D(0.2))\n",
        "tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "# Step 8: Building ANN Model for News Sentiment Classification\n",
        "news_model = Sequential()\n",
        "# news_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "news_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "\n",
        "news_model.add(SpatialDropout1D(0.2))\n",
        "news_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "news_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "news_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "news_history = news_model.fit(X_train_news, y_train_news, epochs=5, batch_size=64, validation_data=(X_test_news, y_test_news), verbose=2)\n",
        "\n",
        "# Step 9: Evaluating the Models\n",
        "# Evaluate Tweets Model\n",
        "y_pred_tweets = tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)\n",
        "\n",
        "# Evaluate News Model\n",
        "y_pred_news = news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)\n",
        "\n",
        "# Performance Metrics for Tweets\n",
        "print(\"Tweets Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "# Performance Metrics for News\n",
        "print(\"News Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(confusion_matrix(y_test_news, y_pred_news))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWwYaHGC2jhe",
        "outputId": "4b4d38b8-44fc-439e-fdc0-0fce8f0f789c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "719/719 - 226s - 314ms/step - accuracy: 0.7869 - loss: 0.5514 - val_accuracy: 0.8863 - val_loss: 0.3596\n",
            "Epoch 2/5\n",
            "719/719 - 212s - 295ms/step - accuracy: 0.9020 - loss: 0.3098 - val_accuracy: 0.9041 - val_loss: 0.3192\n",
            "Epoch 3/5\n",
            "719/719 - 266s - 370ms/step - accuracy: 0.9146 - loss: 0.2734 - val_accuracy: 0.9034 - val_loss: 0.3156\n",
            "Epoch 4/5\n",
            "719/719 - 257s - 358ms/step - accuracy: 0.9227 - loss: 0.2488 - val_accuracy: 0.9050 - val_loss: 0.3111\n",
            "Epoch 5/5\n",
            "719/719 - 266s - 370ms/step - accuracy: 0.9268 - loss: 0.2299 - val_accuracy: 0.9030 - val_loss: 0.3236\n",
            "Epoch 1/5\n",
            "4/4 - 7s - 2s/step - accuracy: 0.4120 - loss: 1.0942 - val_accuracy: 0.5873 - val_loss: 1.0746\n",
            "Epoch 2/5\n",
            "4/4 - 2s - 506ms/step - accuracy: 0.4920 - loss: 1.0655 - val_accuracy: 0.5873 - val_loss: 1.0254\n",
            "Epoch 3/5\n",
            "4/4 - 2s - 421ms/step - accuracy: 0.4920 - loss: 1.0459 - val_accuracy: 0.5873 - val_loss: 0.9887\n",
            "Epoch 4/5\n",
            "4/4 - 2s - 486ms/step - accuracy: 0.4920 - loss: 1.0027 - val_accuracy: 0.5873 - val_loss: 1.0011\n",
            "Epoch 5/5\n",
            "4/4 - 1s - 306ms/step - accuracy: 0.4920 - loss: 0.9854 - val_accuracy: 0.5873 - val_loss: 0.9984\n",
            "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step\n",
            "Tweets Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.70      0.74      1522\n",
            "           1       0.92      0.94      0.93      4818\n",
            "           2       0.92      0.93      0.92      5160\n",
            "\n",
            "    accuracy                           0.90     11500\n",
            "   macro avg       0.88      0.86      0.86     11500\n",
            "weighted avg       0.90      0.90      0.90     11500\n",
            "\n",
            "[[1059  208  255]\n",
            " [  97 4536  185]\n",
            " [ 183  187 4790]]\n",
            "News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.59      1.00      0.74        37\n",
            "\n",
            "    accuracy                           0.59        63\n",
            "   macro avg       0.20      0.33      0.25        63\n",
            "weighted avg       0.34      0.59      0.43        63\n",
            "\n",
            "[[ 0  0  9]\n",
            " [ 0  0 17]\n",
            " [ 0  0 37]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for compiling is almost 30 mins**"
      ],
      "metadata": {
        "id": "RsELIzCk5qML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/5\n",
        "\n",
        "719/719 - 171s - loss: 0.5244 - accuracy: 0.8012 - val_loss: 0.3485 - val_accuracy: 0.8920 - 171s/epoch - 238ms/step\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "719/719 - 167s - loss: 0.3019 - accuracy: 0.9046 - val_loss: 0.3127 - val_accuracy: 0.9043 - 167s/epoch - 232ms/step\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "719/719 - 167s - loss: 0.2631 - accuracy: 0.9160 - val_loss: 0.3165 - val_accuracy: 0.9011 - 167s/epoch - 232ms/step\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "719/719 - 166s - loss: 0.2374 - accuracy: 0.9238 - val_loss: 0.3271 - val_accuracy: 0.9006 - 166s/epoch - 231ms/step\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "719/719 - 169s - loss: 0.2134 - accuracy: 0.9310 - val_loss: 0.3373 - val_accuracy: 0.9060 - 169s/epoch - 235ms/step\n",
        "\n",
        "Epoch 1/5\n",
        "\n",
        "4/4 - 4s - loss: 1.0942 - accuracy: 0.4520 - val_loss: 1.0805 - val_accuracy: 0.5873 - 4s/epoch - 1s/step\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0685 - accuracy: 0.4920 - val_loss: 1.0473 - val_accuracy: 0.5873 - 988ms/epoch - 247ms/step\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0333 - accuracy: 0.4920 - val_loss: 0.9976 - val_accuracy: 0.5873 - 947ms/epoch - 237ms/step\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0035 - accuracy: 0.4920 - val_loss: 0.9927 - val_accuracy: 0.5873 - 946ms/epoch - 236ms/step\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "4/4 - 1s - loss: 0.9696 - accuracy: 0.4920 - val_loss: 0.9814 - val_accuracy: 0.5873 - 923ms/epoch - 231ms/step\n",
        "\n",
        "360/360 [==============================] - 9s 24ms/step\n",
        "\n",
        "2/2 [==============================] - 0s 16ms/step\n",
        "\n",
        "Tweets Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.64      0.74      1522\n",
        "           1       0.91      0.96      0.93      4818\n",
        "           2       0.91      0.94      0.92      5160\n",
        "\n",
        "    accuracy                           0.91     11500\n",
        "\n",
        "   macro avg       0.89      0.85      0.86     11500\n",
        "\n",
        "weighted avg       0.90      0.91      0.90     11500\n",
        "\n",
        "\n",
        "[[ 979  222  321]\n",
        "\n",
        " [  62 4605  151]\n",
        "\n",
        " [  97  228 4835]]\n",
        "\n",
        "News Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.00      0.00      0.00         9\n",
        "           1       0.00      0.00      0.00        17\n",
        "           2       0.59      1.00      0.74        37\n",
        "\n",
        "    accuracy                           0.59        63\n",
        "\n",
        "   macro avg       0.20      0.33      0.25        63\n",
        "\n",
        "weighted avg       0.34      0.59      0.43        63\n",
        "\n",
        "\n",
        "[[ 0  0  9]\n",
        "\n",
        " [ 0  0 17]\n",
        "\n",
        " [ 0  0 37]]"
      ],
      "metadata": {
        "id": "ufbeO17xjyki"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mz0epRlEoob"
      },
      "source": [
        "# Sentiment Analysis- tesla"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALGORITHMS WORKS !!!"
      ],
      "metadata": {
        "id": "L_E86_qKSxVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KsuScRGEXniM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load datasets\n",
        "tweets_df = pd.read_csv(\"/content/cleaned_tsla_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/cleaned_tsla_articles.csv\")\n",
        "\n",
        "# Ensure text column is string type\n",
        "tweets_df['Tweet'] = tweets_df['Tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "35c_SFF4Xv2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a246090d-bc9b-4a85-b88f-2f1eda37468c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# VADER Sentiment Analysis for Tweets\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "# Function to get sentiment score from VADER\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply VADER sentiment analysis to tweets\n",
        "tweets_df['sentiment'] = tweets_df['Tweet'].apply(get_vader_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "q-26gh4WXxx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4daf4b3c-a5ff-4f23-bf3c-7e92d0409a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# TextBlob Sentiment Analysis for News\n",
        "\n",
        "# Function to get sentiment score from TextBlob\n",
        "def get_textblob_sentiment(text):\n",
        "    score = TextBlob(text).sentiment.polarity\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply TextBlob sentiment analysis to news articles\n",
        "news_df['sentiment'] = news_df['text'].apply(get_textblob_sentiment)\n",
        "news_df['sentiment'] = news_df['sentiment'].astype(int)\n",
        "\n",
        "# Save sentiment-labeled datasets\n",
        "tweets_df.to_csv(\"sentiment_tsla_tweets.csv\", index=False)\n",
        "news_df.to_csv(\"sentiment_tsla_news.csv\", index=False)\n",
        "\n",
        "print(\"Sentiment analysis complete! Results saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM for sentiment analysis"
      ],
      "metadata": {
        "id": "x1ysblnS2won"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "tweets_df = tweets_df.sample(min(10000, len(tweets_df)), random_state=42)\n",
        "news_df = news_df.sample(min(10000, len(news_df)), random_state=42)\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(\n",
        "    tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(\n",
        "    news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Function to train models\n",
        "def train_model(X_train, y_train, X_test, y_test, model):\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    predictions = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Model {model.__class__.__name__} Accuracy: {accuracy:.4f}\")\n",
        "    return pipeline\n",
        "\n",
        "# Models to Train\n",
        "models = {\n",
        "    'Naïve Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(kernel='linear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "# Train models for Tweets\n",
        "print(\"\\n Training Models for Tweet Sentiment Classification \")\n",
        "trained_models_tweets = {\n",
        "    name: train_model(X_tweets, y_tweets, X_test_tweets, y_test_tweets, model)\n",
        "    for name, model in models.items()\n",
        "}\n",
        "\n",
        "# Train models for News\n",
        "print(\"\\n Training Models for News Sentiment Classification \")\n",
        "trained_models_news = {\n",
        "    name: train_model(X_news, y_news, X_test_news, y_test_news, model)\n",
        "    for name, model in models.items()\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WhdsIfQ4Gy-",
        "outputId": "2565c361-3076-419f-c943-be3b08803b9a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Models for Tweet Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.7215\n",
            "Model SVC Accuracy: 0.8285\n",
            "Model DecisionTreeClassifier Accuracy: 0.7750\n",
            "Model RandomForestClassifier Accuracy: 0.7855\n",
            "\n",
            " Training Models for News Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.5873\n",
            "Model SVC Accuracy: 0.5556\n",
            "Model DecisionTreeClassifier Accuracy: 0.5238\n",
            "Model RandomForestClassifier Accuracy: 0.5079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models for Tweet Sentiment Classification\n",
        "*   Model MultinomialNB Accuracy: 0.7215\n",
        "*   Model SVC Accuracy: 0.8285\n",
        "*   Model DecisionTreeClassifier Accuracy: 0.7720\n",
        "*   Model RandomForestClassifier Accuracy: 0.7880\n",
        "\n",
        "\n",
        "Training Models for News Sentiment Classification\n",
        "*   Model MultinomialNB Accuracy: 0.5873\n",
        "*   Model SVC Accuracy: 0.5556\n",
        "*   Model DecisionTreeClassifier Accuracy: 0.5397\n",
        "*   Model RandomForestClassifier Accuracy: 0.5238\n"
      ],
      "metadata": {
        "id": "YR9q619vF528"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ANN for sentiment analysis"
      ],
      "metadata": {
        "id": "hzfKyY0Q2qBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THIS IS CRASHING !!!! FIX IT !!!!**"
      ],
      "metadata": {
        "id": "eCrzKdAZdGBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# # Load data\n",
        "# tweets_df = pd.read_csv(\"/content/sentiment_tsla_tweets.csv\")\n",
        "# news_df = pd.read_csv(\"/content/sentiment_tsla_news.csv\")\n",
        "\n",
        "# # Function for data preprocessing\n",
        "# def preprocess_data(df, text_column):\n",
        "#     df[text_column] = df[text_column].astype(str)\n",
        "#     X = df[text_column].values\n",
        "#     y = df['sentiment'].values\n",
        "\n",
        "#     # Convert labels (-1, 0, 1) into one-hot encoding\n",
        "#     encoder = OneHotEncoder(sparse_output=False)\n",
        "#     y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "#     return X, y\n",
        "\n",
        "# X_tweets, y_tweets = preprocess_data(tweets_df, 'cleaned_tweet')\n",
        "# X_news, y_news = preprocess_data(news_df, 'text')\n",
        "\n",
        "# # TF-IDF Vectorization\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# vectorizer = TfidfVectorizer(max_features=5000)\n",
        "# X_tweets = vectorizer.fit_transform(X_tweets).toarray()\n",
        "# X_news = vectorizer.transform(X_news).toarray()\n",
        "\n",
        "# # Train/Test Split\n",
        "# X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "# X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# # ANN Model\n",
        "# def build_ann_model(input_dim):\n",
        "#     model = Sequential([\n",
        "#         Dense(512, activation='relu', input_shape=(input_dim,)),\n",
        "#         Dropout(0.3),\n",
        "#         Dense(256, activation='relu'),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(3, activation='softmax')  # 3 classes (-1, 0, 1)\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Train ANN for Tweets\n",
        "# ann_tweet_model = build_ann_model(X_train_tweets.shape[1])\n",
        "# ann_tweet_model.fit(X_train_tweets, y_train_tweets, epochs=10, batch_size=32, validation_data=(X_test_tweets, y_test_tweets))\n",
        "\n",
        "# # Train ANN for News\n",
        "# ann_news_model = build_ann_model(X_train_news.shape[1])\n",
        "# ann_news_model.fit(X_train_news, y_train_news, epochs=20, batch_size=32, validation_data=(X_test_news, y_test_news))\n",
        "\n",
        "# # Save models\n",
        "# ann_tweet_model.save(\"ANN_TSLA_TWEET.h5\")\n",
        "# ann_news_model.save(\"ANN_TSLA_NEWS.h5\")\n",
        "\n",
        "# print(\" ANN Sentiment Models Trained and Saved!\")\n",
        "\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed datasets for tweets and news\n",
        "tweets_df = pd.read_csv('/content/sentiment_tsla_tweets.csv')  # replace with your actual dataset path\n",
        "news_df = pd.read_csv('/content/sentiment_tsla_news.csv')  # replace with your actual news dataset path\n",
        "\n",
        "# Replace NaN values with an empty string in both tweet and news data\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].fillna('')\n",
        "news_df['text'] = news_df['text'].fillna('')\n",
        "\n",
        "# Alternatively, you can drop rows with NaN values if you prefer:\n",
        "# tweets_df.dropna(subset=['cleaned_tweet'], inplace=True)\n",
        "# news_df.dropna(subset=['text'], inplace=True)\n",
        "# Ensure the columns are strings\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)\n",
        "\n",
        "\n",
        "# Assuming that the preprocessed data has the 'cleaned_text' and 'sentiment' columns already.\n",
        "# Step 4: Encoding Sentiment Labels (assuming 'sentiment' column with Positive, Negative, Neutral)\n",
        "encoder = LabelEncoder()\n",
        "tweets_df['encoded_sentiment'] = encoder.fit_transform(tweets_df['sentiment'])\n",
        "news_df['encoded_sentiment'] = encoder.fit_transform(news_df['sentiment'])\n",
        "\n",
        "y_tweets = tweets_df['encoded_sentiment']\n",
        "y_news = news_df['encoded_sentiment']\n",
        "\n",
        "# Step 5: Tokenizing the Text Data (using preprocessed 'cleaned_text')\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(tweets_df['cleaned_tweet'])\n",
        "X_tweets = tokenizer.texts_to_sequences(tweets_df['cleaned_tweet'])\n",
        "X_tweets = pad_sequences(X_tweets, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(news_df['text'])\n",
        "X_news = tokenizer.texts_to_sequences(news_df['text'])\n",
        "X_news = pad_sequences(X_news, maxlen=100)\n",
        "\n",
        "# Step 6: Splitting Data into Training and Testing Sets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Building ANN Model for Tweets Sentiment Classification\n",
        "# tweet_model = Sequential()\n",
        "# # tweet_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "# tweet_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "# tweet_model.add(SpatialDropout1D(0.2))\n",
        "# tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "# tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "# tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "tweet_model = Sequential()\n",
        "tweet_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "tweet_model.add(SpatialDropout1D(0.2))\n",
        "tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "# Step 8: Building ANN Model for News Sentiment Classification\n",
        "news_model = Sequential()\n",
        "# news_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "news_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "\n",
        "news_model.add(SpatialDropout1D(0.2))\n",
        "news_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "news_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "news_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "news_history = news_model.fit(X_train_news, y_train_news, epochs=5, batch_size=64, validation_data=(X_test_news, y_test_news), verbose=2)\n",
        "\n",
        "# Step 9: Evaluating the Models\n",
        "# Evaluate Tweets Model\n",
        "y_pred_tweets = tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)\n",
        "\n",
        "# Evaluate News Model\n",
        "y_pred_news = news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)\n",
        "\n",
        "# Performance Metrics for Tweets\n",
        "print(\"Tweets Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "# Performance Metrics for News\n",
        "print(\"News Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(confusion_matrix(y_test_news, y_pred_news))"
      ],
      "metadata": {
        "id": "gx2A2ud-3pPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a23042-a123-4bca-92f5-dcc9a5196642"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "4544/4544 - 1295s - 285ms/step - accuracy: 0.8476 - loss: 0.4395 - val_accuracy: 0.8785 - val_loss: 0.3745\n",
            "Epoch 2/5\n",
            "4544/4544 - 1354s - 298ms/step - accuracy: 0.8808 - loss: 0.3646 - val_accuracy: 0.8867 - val_loss: 0.3514\n",
            "Epoch 3/5\n",
            "4544/4544 - 1314s - 289ms/step - accuracy: 0.8887 - loss: 0.3425 - val_accuracy: 0.8873 - val_loss: 0.3500\n",
            "Epoch 4/5\n",
            "4544/4544 - 1305s - 287ms/step - accuracy: 0.8927 - loss: 0.3280 - val_accuracy: 0.8895 - val_loss: 0.3490\n",
            "Epoch 5/5\n",
            "4544/4544 - 1299s - 286ms/step - accuracy: 0.8956 - loss: 0.3159 - val_accuracy: 0.8879 - val_loss: 0.3549\n",
            "Epoch 1/5\n",
            "3/3 - 8s - 3s/step - accuracy: 0.3803 - loss: 1.0958 - val_accuracy: 0.5833 - val_loss: 1.0691\n",
            "Epoch 2/5\n",
            "3/3 - 1s - 377ms/step - accuracy: 0.5845 - loss: 1.0560 - val_accuracy: 0.5833 - val_loss: 1.0241\n",
            "Epoch 3/5\n",
            "3/3 - 1s - 254ms/step - accuracy: 0.5845 - loss: 0.9970 - val_accuracy: 0.5833 - val_loss: 0.9730\n",
            "Epoch 4/5\n",
            "3/3 - 1s - 395ms/step - accuracy: 0.5845 - loss: 0.9573 - val_accuracy: 0.5833 - val_loss: 0.9622\n",
            "Epoch 5/5\n",
            "3/3 - 1s - 434ms/step - accuracy: 0.5845 - loss: 0.9171 - val_accuracy: 0.5833 - val_loss: 0.9695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ab5ee69300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2272/2272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 48ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470ms/step\n",
            "Tweets Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.81      0.83     20075\n",
            "           1       0.89      0.94      0.91     21900\n",
            "           2       0.90      0.90      0.90     30720\n",
            "\n",
            "    accuracy                           0.89     72695\n",
            "   macro avg       0.88      0.88      0.88     72695\n",
            "weighted avg       0.89      0.89      0.89     72695\n",
            "\n",
            "[[16164  1598  2313]\n",
            " [  571 20637   692]\n",
            " [ 1979   999 27742]]\n",
            "News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.00      0.00      0.00        11\n",
            "           2       0.58      1.00      0.74        21\n",
            "\n",
            "    accuracy                           0.58        36\n",
            "   macro avg       0.19      0.33      0.25        36\n",
            "weighted avg       0.34      0.58      0.43        36\n",
            "\n",
            "[[ 0  0  4]\n",
            " [ 0  0 11]\n",
            " [ 0  0 21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WAIT TIME FOR COMPILING IS ALMOST 2 HOURS**"
      ],
      "metadata": {
        "id": "Ucvv3z7uELdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/5\n",
        "\n",
        "4544/4544 - 1295s - 285ms/step - accuracy: 0.8476 - loss: 0.4395 - val_accuracy: 0.8785 - val_loss: 0.3745\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "4544/4544 - 1354s - 298ms/step - accuracy: 0.8808 - loss: 0.3646 - val_accuracy: 0.8867 - val_loss: 0.3514\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "4544/4544 - 1314s - 289ms/step - accuracy: 0.8887 - loss: 0.3425 - val_accuracy: 0.8873 - val_loss: 0.3500\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "4544/4544 - 1305s - 287ms/step - accuracy: 0.8927 - loss: 0.3280 - val_accuracy: 0.8895 - val_loss: 0.3490\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "4544/4544 - 1299s - 286ms/step - accuracy: 0.8956 - loss: 0.3159 - val_accuracy: 0.8879 - val_loss: 0.3549\n",
        "\n",
        "Epoch 1/5\n",
        "\n",
        "3/3 - 8s - 3s/step - accuracy: 0.3803 - loss: 1.0958 - val_accuracy: 0.5833 - val_loss: 1.0691\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "3/3 - 1s - 377ms/step - accuracy: 0.5845 - loss: 1.0560 - val_accuracy: 0.5833 - val_loss: 1.0241\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "3/3 - 1s - 254ms/step - accuracy: 0.5845 - loss: 0.9970 - val_accuracy: 0.5833 - val_loss: 0.9730\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "3/3 - 1s - 395ms/step - accuracy: 0.5845 - loss: 0.9573 - val_accuracy: 0.5833 - val_loss: 0.9622\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "3/3 - 1s - 434ms/step - accuracy: 0.5845 - loss: 0.9171 - val_accuracy: 0.5833 - val_loss: 0.9695\n",
        "\n",
        "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ab5ee69300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
        "\n",
        "2272/2272 ━━━━━━━━━━━━━━━━━━━━ 109s 48ms/step\n",
        "\n",
        "2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 470ms/step\n",
        "\n",
        "Tweets Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.81      0.83     20075\n",
        "           1       0.89      0.94      0.91     21900\n",
        "           2       0.90      0.90      0.90     30720\n",
        "\n",
        "    accuracy                           0.89     72695\n",
        "\n",
        "   macro avg       0.88      0.88      0.88     72695\n",
        "\n",
        "weighted avg       0.89      0.89      0.89     72695\n",
        "\n",
        "\n",
        "[[16164  1598  2313]\n",
        "\n",
        " [  571 20637   692]\n",
        "\n",
        " [ 1979   999 27742]]\n",
        "\n",
        "News Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.00      0.00      0.00         4\n",
        "           1       0.00      0.00      0.00        11\n",
        "           2       0.58      1.00      0.74        21\n",
        "\n",
        "    accuracy                           0.58        36\n",
        "   macro avg       0.19      0.33      0.25        36\n",
        "\n",
        "weighted avg       0.34      0.58      0.43        36\n",
        "\n",
        "[[ 0  0  4]\n",
        " [ 0  0 11]\n",
        " [ 0  0 21]]\n"
      ],
      "metadata": {
        "id": "tlKzZwb_EP1c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZ_x93P_CL0"
      },
      "source": [
        "# Merging - Microsoft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "MOOOtCrJ_Erw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "stock_data = pd.read_csv(\"/content/msft_2019.csv\")\n",
        "tweets_sentiment = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_sentiment = pd.read_csv(\"/content/sentiment_msft_news.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3meXu3Bn_I4W"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to datetime\n",
        "stock_data['day_date'] = pd.to_datetime(stock_data['day_date'])\n",
        "tweets_sentiment['date'] = pd.to_datetime(tweets_sentiment['date'])\n",
        "news_sentiment['date'] = pd.to_datetime(news_sentiment['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2kEJIXf7_nlC"
      },
      "outputs": [],
      "source": [
        "# Extract Month-Day format (MM-DD)\n",
        "stock_data['month_day'] = stock_data['day_date'].dt.strftime('%m-%d')\n",
        "tweets_sentiment['month_day'] = tweets_sentiment['date'].dt.strftime('%m-%d')\n",
        "news_sentiment['month_day'] = news_sentiment['date'].dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Nu4pkD6WMPd0"
      },
      "outputs": [],
      "source": [
        "# # Drop original 'date' columns (Optional)\n",
        "# stock_data.drop(columns=['day_date'], inplace=True)\n",
        "# tweets_sentiment.drop(columns=['date'], inplace=True)\n",
        "# news_sentiment.drop(columns=['date'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "yGlNKkuHW9uR"
      },
      "outputs": [],
      "source": [
        "# Stock & Tweets\n",
        "stock_tweet_merged = stock_data.merge(tweets_sentiment, on='month_day', how='left')\n",
        "stock_tweet_merged.fillna(0, inplace=True)\n",
        "stock_tweet_merged = stock_tweet_merged.rename(columns={'sentiment_vader': 'tweet_sentiment'})\n",
        "stock_tweet_merged.to_csv(\"Merge_MSFT_TWEET.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "yn7ufmAZL28I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0d4fff-d40d-46da-b127-1309f5f44141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Merging done! Two separate files created: one for Tweets, one for News.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-225be7571668>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
            "  stock_news_merged.fillna(0, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Stock & News\n",
        "stock_news_merged = stock_data.merge(news_sentiment, on='month_day', how='left')\n",
        "stock_news_merged.fillna(0, inplace=True)\n",
        "stock_news_merged = stock_news_merged.rename(columns={'sentiment': 'news_sentiment'})\n",
        "stock_news_merged.to_csv(\"Merge_MSFT_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Merging done! Two separate files created: one for Tweets, one for News.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GdhrQULVY5w"
      },
      "source": [
        "# Merging - Tesla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "DQzd4PPiMgjx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets (assuming they are already preprocessed)\n",
        "stock_data = pd.read_csv(\"/content/tsla_2019.csv\")\n",
        "tweets_sentiment = pd.read_csv(\"/content/sentiment_tsla_tweets.csv\")\n",
        "news_sentiment = pd.read_csv(\"/content/sentiment_tsla_news.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4t4Q5R54VjSf"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to datetime\n",
        "stock_data['day_date'] = pd.to_datetime(stock_data['day_date'])\n",
        "tweets_sentiment['date'] = pd.to_datetime(tweets_sentiment['date'])\n",
        "news_sentiment['date'] = pd.to_datetime(news_sentiment['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "WVJFCM22Vk2l"
      },
      "outputs": [],
      "source": [
        "# Extract Month-Day format (MM-DD)\n",
        "stock_data['month_day'] = stock_data['day_date'].dt.strftime('%m-%d')\n",
        "tweets_sentiment['month_day'] = tweets_sentiment['date'].dt.strftime('%m-%d')\n",
        "news_sentiment['month_day'] = news_sentiment['date'].dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock & Tweets\n",
        "stock_tweet_merged = stock_data.merge(tweets_sentiment, on='month_day', how='left')\n",
        "stock_tweet_merged.fillna(0, inplace=True)\n",
        "stock_tweet_merged = stock_tweet_merged.rename(columns={'sentiment_vader': 'tweet_sentiment'})\n",
        "stock_tweet_merged.to_csv(\"Merge_TSLA_TWEET.csv\", index=False)"
      ],
      "metadata": {
        "id": "3HuglepuUq9p"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock & News\n",
        "stock_news_merged = stock_data.merge(news_sentiment, on='month_day', how='left')\n",
        "stock_news_merged.fillna(0, inplace=True)\n",
        "stock_news_merged = stock_news_merged.rename(columns={'sentiment': 'news_sentiment'})\n",
        "stock_news_merged.to_csv(\"Merge_TSLA_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Merging done! Two separate files created: one for Tweets, one for News.\")"
      ],
      "metadata": {
        "id": "Ut3k4nQaUuRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07362d6c-9d05-4a2d-d2cc-078618f82f1b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Merging done! Two separate files created: one for Tweets, one for News.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-df03a4931140>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
            "  stock_news_merged.fillna(0, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpHk36XNrwZL"
      },
      "source": [
        "# Stock Data Analysis (Microsoft)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "stock_tweet = pd.read_csv(\"/content/Merge_MSFT_TWEET.csv\")\n",
        "stock_news = pd.read_csv(\"/content/Merge_MSFT_NEWS.csv\")\n",
        "\n",
        "# Convert 'month_day' to datetime format\n",
        "stock_tweet['month_day'] = pd.to_datetime(stock_tweet['month_day'], format='%m-%d')\n",
        "stock_news['month_day'] = pd.to_datetime(stock_news['month_day'], format='%m-%d')\n",
        "\n",
        "# Sort data chronologically\n",
        "stock_tweet = stock_tweet.sort_values('month_day')\n",
        "stock_news = stock_news.sort_values('month_day')\n",
        "\n",
        "# Feature Engineering\n",
        "for df in [stock_tweet, stock_news]:\n",
        "    df['5_day_avg'] = df['close_value'].rolling(window=5).mean()\n",
        "    df['10_day_avg'] = df['close_value'].rolling(window=10).mean()\n",
        "    df['volatility'] = df['close_value'].pct_change().rolling(window=5).std()\n",
        "    df.fillna(0, inplace=True)  # Fill missing values\n",
        "\n",
        "# Save new files\n",
        "stock_tweet.to_csv(\"MSFT_FEATURED_TWEET.csv\", index=False)\n",
        "stock_news.to_csv(\"MSFT_FEATURED_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Feature Engineering Completed for Both Files!\")\n"
      ],
      "metadata": {
        "id": "u-udlLdQ0j5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b8e0c9-033d-4f7b-b890-b10ad99ef9c3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature Engineering Completed for Both Files!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Load Tweet Data\n",
        "tweet_data = pd.read_csv(\"/content/MSFT_FEATURED_TWEET.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(tweet_data[['close_value', 'sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X, y = [], []\n",
        "for i in range(30, len(scaled_data)):\n",
        "    X.append(scaled_data[i-30:i])\n",
        "    y.append(scaled_data[i, 0])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_tweet = Sequential()\n",
        "model_tweet.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(LSTM(units=50))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(Dense(units=1))\n",
        "\n",
        "model_tweet.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_tweet.fit(X, y, epochs=10, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_tweet.save(\"LSTM_MSFT_TWEET.h5\")\n",
        "\n",
        "print(\" LSTM Model Trained for Tweet Sentiment!\")\n"
      ],
      "metadata": {
        "id": "Jagdrk5rWK8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a371a18-cfb0-488f-b5ff-c54e68d5b3b7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 32ms/step - loss: 0.0078\n",
            "Epoch 2/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 34ms/step - loss: 0.0011\n",
            "Epoch 3/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 32ms/step - loss: 6.1917e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 4.6589e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 32ms/step - loss: 4.4120e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 33ms/step - loss: 4.1833e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 33ms/step - loss: 4.1166e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 32ms/step - loss: 4.0600e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 33ms/step - loss: 4.0316e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 32ms/step - loss: 3.9575e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LSTM Model Trained for Tweet Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for above compiling is almost 15 mins or so**"
      ],
      "metadata": {
        "id": "Yrcf2iKWFvt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load News Data\n",
        "news_data = pd.read_csv(\"/content/MSFT_FEATURED_NEWS.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaled_news_data = scaler.fit_transform(news_data[['close_value', 'news_sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X_news, y_news = [], []\n",
        "for i in range(30, len(scaled_news_data)):\n",
        "    X_news.append(scaled_news_data[i-30:i])\n",
        "    y_news.append(scaled_news_data[i, 0])\n",
        "\n",
        "X_news, y_news = np.array(X_news), np.array(y_news)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_news = Sequential()\n",
        "model_news.add(LSTM(units=50, return_sequences=True, input_shape=(X_news.shape[1], X_news.shape[2])))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(LSTM(units=50))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(Dense(units=1))\n",
        "\n",
        "model_news.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_news.fit(X_news, y_news, epochs=20, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_news.save(\"LSTM_MSFT_NEWS.h5\")\n",
        "\n",
        "print(\"✅ LSTM Model Trained for News Sentiment!\")\n"
      ],
      "metadata": {
        "id": "lmGhYB19Hsdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2860a05-af9e-44db-b6a0-ec788de9a581"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.1149\n",
            "Epoch 2/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0234\n",
            "Epoch 3/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0120\n",
            "Epoch 4/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0106\n",
            "Epoch 5/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0081\n",
            "Epoch 6/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0065\n",
            "Epoch 7/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0058\n",
            "Epoch 8/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0057\n",
            "Epoch 9/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0068\n",
            "Epoch 10/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0058\n",
            "Epoch 11/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0051\n",
            "Epoch 12/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0049\n",
            "Epoch 13/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0048\n",
            "Epoch 14/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0047\n",
            "Epoch 15/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0046\n",
            "Epoch 16/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0042\n",
            "Epoch 17/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0039\n",
            "Epoch 18/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0048\n",
            "Epoch 19/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0044\n",
            "Epoch 20/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LSTM Model Trained for News Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Predictions\n",
        "tweet_pred = model_tweet.predict(X)\n",
        "news_pred = model_news.predict(X_news)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    print(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "evaluate_model(y, tweet_pred, \"LSTM Tweet Model\")\n",
        "evaluate_model(y_news, news_pred, \"LSTM News Model\")\n",
        "# y_pred = model_tweet.predict(X_test)  # Ensure model exists and has been trained"
      ],
      "metadata": {
        "id": "fHmV6i3TMtch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558f36f5-733e-4083-f880-3d7c55e2f931"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 2275 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ab3f3ee980> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
            "LSTM Tweet Model - MAE: 0.0034, RMSE: 0.0053\n",
            "LSTM News Model - MAE: 0.0337, RMSE: 0.0430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ANN"
      ],
      "metadata": {
        "id": "HVlqEerr6Syq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load feature-engineered datasets\n",
        "tweet_data = pd.read_csv(\"/content/MSFT_FEATURED_TWEET.csv\")\n",
        "news_data = pd.read_csv(\"/content/MSFT_FEATURED_NEWS.csv\")\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_tweets = scaler.fit_transform(tweet_data[['sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "y_tweets = scaler.fit_transform(tweet_data[['close_value']])\n",
        "\n",
        "X_news = scaler.fit_transform(news_data[['news_sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "y_news = scaler.fit_transform(news_data[['close_value']])\n",
        "\n",
        "# Train/Test Split\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# ANN Model for Stock Prediction\n",
        "def build_ann_stock_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)  # Single output neuron for regression\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Train ANN for Tweet-based Stock Prediction\n",
        "ann_stock_tweet_model = build_ann_stock_model(X_train_tweets.shape[1])\n",
        "ann_stock_tweet_model.fit(X_train_tweets, y_train_tweets, epochs=10, batch_size=32, validation_data=(X_test_tweets, y_test_tweets))\n",
        "\n",
        "# Train ANN for News-based Stock Prediction\n",
        "ann_stock_news_model = build_ann_stock_model(X_train_news.shape[1])\n",
        "ann_stock_news_model.fit(X_train_news, y_train_news, epochs=10, batch_size=32, validation_data=(X_test_news, y_test_news))\n",
        "\n",
        "# Save models\n",
        "ann_stock_tweet_model.save(\"ANN_MSFT_STOCK_TWEET.h5\")\n",
        "ann_stock_news_model.save(\"ANN_MSFT_STOCK_NEWS.h5\")\n",
        "\n",
        "print(\" ANN Stock Prediction Models Trained and Saved!\")\n"
      ],
      "metadata": {
        "id": "O6h0Fxf86Vd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Data Analysis (Tesla)"
      ],
      "metadata": {
        "id": "W6lUyopWDhaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "stock_tweet = pd.read_csv(\"/content/Merge_TSLA_TWEET.csv\")\n",
        "stock_news = pd.read_csv(\"/content/Merge_TSLA_NEWS.csv\")\n",
        "\n",
        "# Convert 'month_day' to datetime format\n",
        "stock_tweet['month_day'] = pd.to_datetime(stock_tweet['month_day'], format='%m-%d')\n",
        "stock_news['month_day'] = pd.to_datetime(stock_news['month_day'], format='%m-%d')\n",
        "\n",
        "# Sort data chronologically\n",
        "stock_tweet = stock_tweet.sort_values('month_day')\n",
        "stock_news = stock_news.sort_values('month_day')\n",
        "\n",
        "# Feature Engineering\n",
        "for df in [stock_tweet, stock_news]:\n",
        "    df['5_day_avg'] = df['close_value'].rolling(window=5).mean()\n",
        "    df['10_day_avg'] = df['close_value'].rolling(window=10).mean()\n",
        "    df['volatility'] = df['close_value'].pct_change().rolling(window=5).std()\n",
        "    df.fillna(0, inplace=True)  # Fill missing values\n",
        "\n",
        "# Save new files\n",
        "stock_tweet.to_csv(\"TSLA_FEATURED_TWEET.csv\", index=False)\n",
        "stock_news.to_csv(\"TSLA_FEATURED_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Feature Engineering Completed for Both Files!\")\n"
      ],
      "metadata": {
        "id": "NUqsjBYBPrdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafca7d3-56e5-446b-aa2a-a008d7d5d958"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature Engineering Completed for Both Files!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Load Tweet Data\n",
        "tweet_data = pd.read_csv(\"/content/TSLA_FEATURED_TWEET.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(tweet_data[['close_value', 'sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X, y = [], []\n",
        "for i in range(30, len(scaled_data)):\n",
        "    X.append(scaled_data[i-30:i])\n",
        "    y.append(scaled_data[i, 0])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_tweet = Sequential()\n",
        "model_tweet.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(LSTM(units=50))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(Dense(units=1))\n",
        "\n",
        "model_tweet.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_tweet.fit(X, y, epochs=7, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_tweet.save(\"LSTM_TSLA_TWEET.h5\")\n",
        "\n",
        "print(\" LSTM Model Trained for Tweet Sentiment!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs8reiH1Dooh",
        "outputId": "997107c2-9753-4ec4-9584-5bbbcd1432de"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 33ms/step - loss: 0.0012\n",
            "Epoch 2/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 33ms/step - loss: 3.1885e-04\n",
            "Epoch 3/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 33ms/step - loss: 3.0808e-04\n",
            "Epoch 4/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 33ms/step - loss: 2.9224e-04\n",
            "Epoch 5/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 34ms/step - loss: 2.8691e-04\n",
            "Epoch 6/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 33ms/step - loss: 2.8371e-04\n",
            "Epoch 7/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 33ms/step - loss: 2.7626e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LSTM Model Trained for Tweet Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for above compiling is almost 45 mins**"
      ],
      "metadata": {
        "id": "ecSFBf8uqZH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load News Data\n",
        "news_data = pd.read_csv(\"/content/TSLA_FEATURED_NEWS.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaled_news_data = scaler.fit_transform(news_data[['close_value', 'news_sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X_news, y_news = [], []\n",
        "for i in range(30, len(scaled_news_data)):\n",
        "    X_news.append(scaled_news_data[i-30:i])\n",
        "    y_news.append(scaled_news_data[i, 0])\n",
        "\n",
        "X_news, y_news = np.array(X_news), np.array(y_news)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_news = Sequential()\n",
        "model_news.add(LSTM(units=50, return_sequences=True, input_shape=(X_news.shape[1], X_news.shape[2])))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(LSTM(units=50))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(Dense(units=1))\n",
        "\n",
        "model_news.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_news.fit(X_news, y_news, epochs=25, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_news.save(\"LSTM_TSLA_NEWS.h5\")\n",
        "\n",
        "print(\"✅ LSTM Model Trained for News Sentiment!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2HAp1_UDtoV",
        "outputId": "7d577b9b-9919-4a32-847c-9b5d57498691"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0682\n",
            "Epoch 2/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0243\n",
            "Epoch 3/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0140\n",
            "Epoch 4/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0074\n",
            "Epoch 5/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0080\n",
            "Epoch 6/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0064\n",
            "Epoch 7/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0060\n",
            "Epoch 8/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0066\n",
            "Epoch 9/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0064\n",
            "Epoch 10/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0058\n",
            "Epoch 11/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0047\n",
            "Epoch 12/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0059\n",
            "Epoch 13/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0052\n",
            "Epoch 14/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0044\n",
            "Epoch 15/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0045\n",
            "Epoch 16/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0050\n",
            "Epoch 17/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0060\n",
            "Epoch 18/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0047\n",
            "Epoch 19/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0054\n",
            "Epoch 20/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0046\n",
            "Epoch 21/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0037\n",
            "Epoch 22/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0044\n",
            "Epoch 23/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0042\n",
            "Epoch 24/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0041\n",
            "Epoch 25/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LSTM Model Trained for News Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Predictions\n",
        "tweet_pred = model_tweet.predict(X)\n",
        "news_pred = model_news.predict(X_news)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    print(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "evaluate_model(y, tweet_pred, \"LSTM Tweet Model\")\n",
        "evaluate_model(y_news, news_pred, \"LSTM News Model\")\n",
        "# y_pred = model_tweet.predict(X_test)  # Ensure model exists and has been trained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvt0-ZeQDuQ4",
        "outputId": "bd88e275-9cb3-41b0-f416-582e7cfe9513"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 10ms/step\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
            "LSTM Tweet Model - MAE: 0.0030, RMSE: 0.0041\n",
            "LSTM News Model - MAE: 0.0366, RMSE: 0.0499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "orZZNtcP7u2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# import joblib\n",
        "\n",
        "# # Assume X_train_tweets, y_train_tweets exist\n",
        "# rf_tweet_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# rf_tweet_model.fit(X_train_tweets, y_train_tweets)\n",
        "\n",
        "# rf_news_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# rf_news_model.fit(X_train_news, y_train_news)\n",
        "\n",
        "# # Save the trained models\n",
        "# joblib.dump(rf_tweet_model, \"RF_TWEET.pkl\")\n",
        "# joblib.dump(rf_news_model, \"RF_NEWS.pkl\")\n",
        "\n",
        "# print(\" Random Forest models trained and saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBbIie0nBhnb",
        "outputId": "e2d3291d-b7f1-4d03-edcd-726f324d0d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Random Forest models trained and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import numpy as np\n",
        "# # from sklearn.ensemble import VotingRegressor\n",
        "# # from tensorflow.keras.models import load_model\n",
        "\n",
        "# # # Load trained models\n",
        "# # lstm_tweet_model = load_model(\"LSTM_MSFT_TWEET.h5\")\n",
        "# # lstm_news_model = load_model(\"LSTM_MSFT_NEWS.h5\")\n",
        "# # ann_stock_tweet_model = load_model(\"ANN_MSFT_STOCK_TWEET.h5\")\n",
        "# # ann_stock_news_model = load_model(\"ANN_MSFT_STOCK_NEWS.h5\")\n",
        "\n",
        "# # # Load ML Model (Random Forest)\n",
        "# # rf_tweet_model = trained_models['Random Forest']  # Assuming trained_models contains the trained models\n",
        "# # rf_news_model = trained_models['Random Forest']\n",
        "\n",
        "# # # Ensemble Voting Regressor\n",
        "# # ensemble_tweet = VotingRegressor([\n",
        "# #     ('lstm', lstm_tweet_model),\n",
        "# #     ('ann', ann_stock_tweet_model),\n",
        "# #     ('rf', rf_tweet_model)\n",
        "# # ])\n",
        "\n",
        "# # ensemble_news = VotingRegressor([\n",
        "# #     ('lstm', lstm_news_model),\n",
        "# #     ('ann', ann_stock_news_model),\n",
        "# #     ('rf', rf_news_model)\n",
        "# # ])\n",
        "\n",
        "# # # Train Ensemble Models\n",
        "# # ensemble_tweet.fit(X_train_tweets, y_train_tweets)\n",
        "# # ensemble_news.fit(X_train_news, y_train_news)\n",
        "\n",
        "# # # Save Ensemble Models\n",
        "# # import joblib\n",
        "# # joblib.dump(ensemble_tweet, \"ENSEMBLE_TWEET.pkl\")\n",
        "# # joblib.dump(ensemble_news, \"ENSEMBLE_NEWS.pkl\")\n",
        "\n",
        "# # print(\" Ensemble Learning Models Trained and Saved!\")\n",
        "\n",
        "\n",
        "# import joblib\n",
        "# import numpy as np\n",
        "# from sklearn.ensemble import VotingRegressor\n",
        "# from tensorflow.keras.models import load_model\n",
        "\n",
        "# #  Load trained models (Ensure these files exist)\n",
        "# lstm_tweet_model = load_model(\"LSTM_MSFT_TWEET.h5\")\n",
        "# lstm_news_model = load_model(\"LSTM_MSFT_NEWS.h5\")\n",
        "# ann_stock_tweet_model = load_model(\"ANN_MSFT_STOCK_TWEET.h5\")\n",
        "# ann_stock_news_model = load_model(\"ANN_MSFT_STOCK_NEWS.h5\")\n",
        "\n",
        "# #  Load pre-trained Random Forest models\n",
        "# rf_tweet_model = joblib.load(\"RF_TWEET.pkl\")  # Ensure this file exists\n",
        "# rf_news_model = joblib.load(\"RF_NEWS.pkl\")  # Ensure this file exists\n",
        "\n",
        "# # Define the ensemble model (DO NOT train)\n",
        "# ensemble_tweet = VotingRegressor([\n",
        "#     ('lstm', lstm_tweet_model),\n",
        "#     ('ann', ann_stock_tweet_model),\n",
        "#     ('rf', rf_tweet_model)\n",
        "# ])\n",
        "\n",
        "# ensemble_news = VotingRegressor([\n",
        "#     ('lstm', lstm_news_model),\n",
        "#     ('ann', ann_stock_news_model),\n",
        "#     ('rf', rf_news_model)\n",
        "# ])\n",
        "\n",
        "# #  Save Ensemble Models without training\n",
        "# joblib.dump(ensemble_tweet, \"ENSEMBLE_TWEET.pkl\")\n",
        "# joblib.dump(ensemble_news, \"ENSEMBLE_NEWS.pkl\")\n",
        "\n",
        "# print(\" Ensemble Models Loaded and Saved (No Training Required!)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr5dtzia7tMk",
        "outputId": "e5ab4fe9-d937-446b-965f-2ce948dd9c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ensemble Models Loaded and Saved (No Training Required!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "dg-7nrJt7zV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestRegressor  # Correct import\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# # Hyperparameter tuning for Random Forest (Regression)\n",
        "# param_grid = {'n_estimators': [50, 100, 200]}\n",
        "# grid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv=3)  # Use RandomForestRegressor\n",
        "# grid_rf.fit(X_train_tweets, y_train_tweets)\n",
        "\n",
        "# print(f\"Best parameters for RF: {grid_rf.best_params_}\")\n",
        "\n",
        "# # Hyperparameter tuning for LSTM\n",
        "# def build_lstm_model(learning_rate=0.001):\n",
        "#     model = Sequential([\n",
        "#         LSTM(50, return_sequences=True, input_shape=(30, 5)),\n",
        "#         Dropout(0.2),\n",
        "#         LSTM(50),\n",
        "#         Dense(1)\n",
        "#     ])\n",
        "#     model.compile(optimizer=Adam(learning_rate), loss='mean_squared_error')\n",
        "#     return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IZP2uZT7zyu",
        "outputId": "bbe126d7-10a7-4e12-bc9c-dcb6745c1e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Best parameters for RF: {'n_estimators': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "VWn9OHdNEFHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot Predictions vs Actual for Tweet Sentiment Model\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(y, label=\"Actual Stock Price\", color=\"blue\")\n",
        "plt.plot(tweet_pred, label=\"Predicted Price (Tweet Sentiment)\", color=\"green\", linestyle=\"dashed\")\n",
        "plt.title(\"Stock Price Prediction Using LSTM (Tweet Sentiment)\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Predictions vs Actual for News Sentiment Model\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(y_news, label=\"Actual Stock Price\", color=\"blue\")\n",
        "plt.plot(news_pred, label=\"Predicted Price (News Sentiment)\", color=\"red\", linestyle=\"dashed\")\n",
        "plt.title(\"Stock Price Prediction Using LSTM (News Sentiment)\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1muF_pVazCCq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare LSTM with ML models\n",
        "print(\" Model Performance Comparison:\")\n",
        "print(\"LSTM Tweet Sentiment Model - MAE: {:.4f}, RMSE: {:.4f}\".format(mean_absolute_error(y, tweet_pred), np.sqrt(mean_squared_error(y, tweet_pred))))\n",
        "print(\"LSTM News Sentiment Model - MAE: {:.4f}, RMSE: {:.4f}\".format(mean_absolute_error(y_news, news_pred), np.sqrt(mean_squared_error(y_news, news_pred))))\n",
        "\n",
        "\n",
        "correlation_tweet = np.corrcoef(y.flatten(), tweet_pred.flatten())[0, 1]\n",
        "correlation_news = np.corrcoef(y_news.flatten(), news_pred.flatten())[0, 1]\n",
        "\n",
        "print(f\" Correlation Between Tweet Sentiment & Stock Price: {correlation_tweet:.3f}\")\n",
        "print(f\" Correlation Between News Sentiment & Stock Price: {correlation_news:.3f}\")\n"
      ],
      "metadata": {
        "id": "NiVGKiRTzDkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved models\n",
        "lstm_msft_tweet = load_model(\"LSTM_MSFT_TWEET.h5\")\n",
        "lstm_msft_news = load_model(\"LSTM_MSFT_NEWS.h5\")\n",
        "lstm_tsla_tweet = load_model(\"LSTM_TSLA_TWEET.h5\")\n",
        "lstm_tsla_news = load_model(\"LSTM_TSLA_NEWS.h5\")\n",
        "\n",
        "# Load the actual stock price data (Replace with your actual dataset)\n",
        "y_msft = pd.read_csv(\"/content/msft_2019.csv\")  # Microsoft actual prices\n",
        "y_tsla = pd.read_csv(\"/content/tsla_2019.csv\")  # Tesla actual prices\n",
        "\n",
        "# Load the predicted prices (Replace with your prediction output)\n",
        "msft_tweet_pred = pd.read_csv(\"/content/Merge_MSFT_TWEET.csv\")  # Microsoft predicted (Tweet Sentiment)\n",
        "msft_news_pred = pd.read_csv(\"/content/Merge_MSFT_NEWS.csv\")  # Microsoft predicted (News Sentiment)\n",
        "tsla_tweet_pred = pd.read_csv(\"/content/Merge_TSLA_TWEET.csv\")  # Tesla predicted (Tweet Sentiment)\n",
        "tsla_news_pred = pd.read_csv(\"/content/Merge_TSLA_NEWS.csv\")  # Tesla predicted (News Sentiment)\n",
        "\n",
        "# Plot for Microsoft (Tweet Sentiment)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_msft, label=\"Actual Stock Price\", color=\"blue\")\n",
        "plt.plot(msft_tweet_pred, label=\"Predicted Price (Tweet Sentiment)\", color=\"green\", linestyle=\"dashed\")\n",
        "plt.title(\"Stock Price Prediction for Microsoft Using LSTM (Tweet Sentiment)\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot for Microsoft (News Sentiment)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_msft, label=\"Actual Stock Price\", color=\"blue\")\n",
        "plt.plot(msft_news_pred, label=\"Predicted Price (News Sentiment)\", color=\"red\", linestyle=\"dashed\")\n",
        "plt.title(\"Stock Price Prediction for Microsoft Using LSTM (News Sentiment)\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot for Tesla (Tweet Sentiment)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_tsla, label=\"Actual Stock Price\", color=\"blue\")\n",
        "plt.plot(tsla_tweet_pred, label=\"Predicted Price (Tweet Sentiment)\", color=\"green\", linestyle=\"dashed\")\n",
        "plt.title(\"Stock Price Prediction for Tesla Using LSTM (Tweet Sentiment)\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot for Tesla (News Sentiment)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_tsla, label=\"Actual Stock Price\", color=\"blue\")\n",
        "plt.plot(tsla_news_pred, label=\"Predicted Price (News Sentiment)\", color=\"red\", linestyle=\"dashed\")\n",
        "plt.title(\"Stock Price Prediction for Tesla Using LSTM (News Sentiment)\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yvpX2BkRzHRP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "outputId": "2feadead-e7dc-4efd-f65b-fef2dffac4da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'numpy.ndarray'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-60047444ba40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Plot for Microsoft (Tweet Sentiment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_msft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Actual Stock Price\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsft_tweet_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Predicted Price (Tweet Sentiment)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dashed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stock Price Prediction for Microsoft Using LSTM (Tweet Sentiment)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m ) -> list[Line2D]:\n\u001b[0;32m-> 3829\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3830\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m         \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \"\"\"\n\u001b[1;32m   1776\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mupdate_units\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0mneednew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_converter\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m         \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/category.py\u001b[0m in \u001b[0;36mdefault_units\u001b[0;34m(data, axis)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# the conversion call stack is default_units -> axis_info -> convert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/category.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/category.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# check if convertible to number:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mconvertible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0;31m# OrderedDict just iterates over unique values in data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_isinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8vFXfVpldeV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "collapsed_sections": [
        "RSSbeztl4SuC",
        "rtIxEm3w4aUw",
        "v7oM2YYybjpJ",
        "l1SpoBmabqms",
        "n-WWdulg7vQ_",
        "mmI7toQa7yyK",
        "bwsJ668S--1y",
        "8z3_YOhb2Zj-",
        "RwZ_x93P_CL0",
        "6GdhrQULVY5w",
        "HVlqEerr6Syq",
        "orZZNtcP7u2Z",
        "dg-7nrJt7zV4",
        "VWn9OHdNEFHb"
      ],
      "mount_file_id": "11AmYuY6p0-2vnU8ZxORnVG2nVYHtb-Nj",
      "authorship_tag": "ABX9TyP7zABA/S91toOUGPTlP2ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}