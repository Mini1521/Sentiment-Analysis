{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mini1521/Sentiment-Analysis/blob/main/Using_Microsoft_and_Tesla_Dataset_from_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSSbeztl4SuC"
      },
      "source": [
        "# Stock Data- preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8JXK8qKp3rFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfef9bff-90b9-4dd4-d340-2ba6f7f4768b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged CSV saved as 'merged_data.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the two CSV files\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Company.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/CompanyValues.csv\")\n",
        "\n",
        "# Merge the data on a common column (e.g., 'day_date')\n",
        "merged_df = pd.merge(df1, df2, on=\"ticker_symbol\", how=\"left\")  # Use \"outer\", \"left\", or \"right\" if needed\n",
        "\n",
        "# Save the merged dataset to a new CSV file\n",
        "merged_df.to_csv(\"merged_data.csv\", index=False)\n",
        "\n",
        "print(\"Merged CSV saved as 'merged_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Egmn_3Ea2U1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97b55e5-dce7-44f6-ae1c-ac743e5aa33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete.\n",
            "Saved: 'msft_2019.csv' and 'tsla_2019.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (update the filename if needed)\n",
        "df = pd.read_csv('/content/merged_data.csv')\n",
        "\n",
        "df.rename(columns={'post_date': 'date'}, inplace=True)\n",
        "\n",
        "# Convert 'day_date' to datetime format\n",
        "df['day_date'] = pd.to_datetime(df['day_date'], errors='coerce')\n",
        "\n",
        "# Filter data for the year 2020\n",
        "df_2019 = df[df['day_date'].dt.year == 2019]\n",
        "df_2019 = df_2019.sort_values(by='day_date', ascending=True)\n",
        "\n",
        "# Separate data for Microsoft (MSFT) and Tesla (TSLA)\n",
        "msft_data = df_2019[df_2019['ticker_symbol'] == 'MSFT']\n",
        "tsla_data = df_2019[df_2019['ticker_symbol'] == 'TSLA']\n",
        "\n",
        "# Drop rows with missing values\n",
        "msft_data = msft_data.dropna()\n",
        "tsla_data = tsla_data.dropna()\n",
        "\n",
        "\n",
        "# print(\" Stock Data Date Range:\")\n",
        "# print(f\"Start Date: {msft_data['day_date'].min()} | End Date: {msft_data['day_date'].max()}\")\n",
        "# print(f\"Start Date: {tsla_data['day_date'].min()} | End Date: {tsla_data['day_date'].max()}\")\n",
        "\n",
        "# Save each dataset separately\n",
        "msft_data.to_csv(\"msft_2019.csv\", index=False)\n",
        "tsla_data.to_csv(\"tsla_2019.csv\", index=False)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "print(\"Saved: 'msft_2019.csv' and 'tsla_2019.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtIxEm3w4aUw"
      },
      "source": [
        "# Social Media - preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3xXgbcP14Fe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "company_tweets = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Company_Tweet.csv\")\n",
        "tweets = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Tweet.csv\")\n",
        "\n",
        "# Merge tweets with company information\n",
        "tweets = tweets.merge(company_tweets, how='left', on='tweet_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S8kNuI0l5lL7"
      },
      "outputs": [],
      "source": [
        "# Convert 'post_date' to datetime format\n",
        "tweets['date'] = pd.to_datetime(tweets['post_date'], unit='s').dt.date\n",
        "tweets['date'] = pd.to_datetime(tweets['date'], errors='coerce')\n",
        "# tweets['time'] = pd.to_datetime(tweets['post_date'], unit='s').dt.time\n",
        "\n",
        "tweets.to_csv(\"merged_tweets.csv\", index=False)\n",
        "\n",
        "tweets.drop(columns=['comment_num', 'retweet_num', 'like_num'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rs-K5Kp-5p_e"
      },
      "outputs": [],
      "source": [
        "# Filter for Microsoft (MSFT) and Tesla (TSLA)\n",
        "tweets_filtered = tweets[tweets['ticker_symbol'].isin(['MSFT', 'TSLA'])]\n",
        "\n",
        "# Keep only tweets from 2019\n",
        "tweets_filtered = tweets_filtered[tweets_filtered['date'].dt.year == 2019]\n",
        "\n",
        "# Split into separate datasets\n",
        "msft_tweets = tweets_filtered[tweets_filtered['ticker_symbol'] == 'MSFT']\n",
        "tsla_tweets = tweets_filtered[tweets_filtered['ticker_symbol'] == 'TSLA']\n",
        "\n",
        "# Save to CSV files\n",
        "msft_tweets.to_csv(\"msft_tweets_2019.csv\", index=False)\n",
        "tsla_tweets.to_csv(\"tsla_tweets_2019.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning of microsoft tweets"
      ],
      "metadata": {
        "id": "v7oM2YYybjpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VajmTQ2t5x6T",
        "outputId": "49cb3dc7-9212-481f-f671-0797d8677ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load Twitter dataset\n",
        "tweets_df = pd.read_csv(\"/content/msft_tweets_2019.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-Gn3mF5f7Ho2"
      },
      "outputs": [],
      "source": [
        "# rename columns\n",
        "tweets_df.rename(columns={'body': 'Tweet'}, inplace=True)\n",
        "tweets_df.rename(columns={'post_date': 'day_date'}, inplace=True)\n",
        "\n",
        "# Droping Duplicates\n",
        "tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "\n",
        "# Handle Missing Values\n",
        "tweets_df.dropna(subset=['Tweet'], inplace=True)  # Drop rows where 'Tweet' is empty\n",
        "\n",
        "# Ensure 'date' is in datetime format\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce')\n",
        "\n",
        "tweets_df = tweets_df.sort_values(by='date', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gjEXLpIW7NTx"
      },
      "outputs": [],
      "source": [
        "# Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "agU-7Vpx7OuU"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning function\n",
        "tweets_df['cleaned_tweet'] = tweets_df['Tweet'].astype(str).apply(clean_text)\n",
        "\n",
        "#Saving the cleaned data for further analysis\n",
        "tweets_df.to_csv(\"cleaned_msft_tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning of tesla tweets"
      ],
      "metadata": {
        "id": "l1SpoBmabqms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7NXDc8W87WJ-"
      },
      "outputs": [],
      "source": [
        "# Load Twitter dataset\n",
        "tweets_df = pd.read_csv(\"/content/tsla_tweets_2019.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GNtXLEMH7fOq"
      },
      "outputs": [],
      "source": [
        "# rename columns\n",
        "tweets_df.rename(columns={'body': 'Tweet'}, inplace=True)\n",
        "tweets_df.rename(columns={'post_date': 'day_date'}, inplace=True)\n",
        "\n",
        "# Droping Duplicates\n",
        "tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "\n",
        "# Handle Missing Values\n",
        "tweets_df.dropna(subset=['Tweet'], inplace=True)  # Drop rows where 'Tweet' is empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gHP_1vVj2_rZ"
      },
      "outputs": [],
      "source": [
        "# Ensure 'date' is in datetime format\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce')\n",
        "\n",
        "tweets_df = tweets_df.sort_values(by='date', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uTsoOO7G7kQB"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "L9RWBI4B7l87"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning function\n",
        "tweets_df['cleaned_tweet'] = tweets_df['Tweet'].astype(str).apply(clean_text)\n",
        "\n",
        "#Saving the cleaned data for further analysis\n",
        "tweets_df.to_csv(\"cleaned_tsla_tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-WWdulg7vQ_"
      },
      "source": [
        "# Microsoft Articles - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "73juevOP7sGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d1c2ff-2338-44e1-99ba-20d33f81e783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset (Update the filename if needed)\n",
        "article = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/msft_articles.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Al3Cv7Pc8DM7"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "article['date'] = pd.to_datetime(article['date'], errors='coerce')\n",
        "\n",
        "# Filter for only the year 2020\n",
        "year_2020 = article[article['date'].dt.year == 2020]\n",
        "year_2020 = year_2020.sort_values(by='date', ascending=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "year_2020 = year_2020.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Rl597yZM8EZ0"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3GKTyi-w8KAO"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the 'text' column\n",
        "year_2020['text'] = year_2020['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "year_2020.to_csv(\"cleaned_msft_articles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmI7toQa7yyK"
      },
      "source": [
        "# Tesla Articles - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6AAa59TO72XZ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset (Update the filename if needed)\n",
        "article = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/tsla_articles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9X4JjhFH8iJo"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "article['date'] = pd.to_datetime(article['date'], errors='coerce')\n",
        "\n",
        "# Filter for only the year 2020\n",
        "year_2020 = article[article['date'].dt.year == 2020]\n",
        "year_2020 = year_2020.sort_values(by='date', ascending=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "year_2020 = year_2020.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tiV_Rxro8jjZ"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XpPxvRcX8kzb"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the 'text' column\n",
        "year_2020['text'] = year_2020['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "year_2020.to_csv(\"cleaned_tsla_articles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwsJ668S--1y"
      },
      "source": [
        "# Sentiment Analysis- microsoft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "_A_K5_sYf9f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da4e2da-98be-4a94-fc9e-c2020925e415"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "q1LqstO5Wd20"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load datasets\n",
        "tweets_df = pd.read_csv(\"/content/cleaned_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/cleaned_msft_articles.csv\")\n",
        "\n",
        "# Ensure text column is string type\n",
        "tweets_df['Tweet'] = tweets_df['Tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "p-mrbgZ3WnP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567f45aa-17f8-4424-f11a-24cbcc9ff0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# VADER Sentiment Analysis for Tweets\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment score from VADER\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply VADER sentiment analysis to tweets\n",
        "tweets_df['sentiment'] = tweets_df['Tweet'].apply(get_vader_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M_kjNMV7Ww87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bcdb28c-9aec-4b56-adb6-cd080f429e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# TextBlob Sentiment Analysis for News\n",
        "\n",
        "# Function to get sentiment score from TextBlob\n",
        "def get_textblob_sentiment(text):\n",
        "    score = TextBlob(text).sentiment.polarity\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply TextBlob sentiment analysis to news articles\n",
        "news_df['sentiment'] = news_df['text'].apply(get_textblob_sentiment)\n",
        "\n",
        "# Save sentiment-labeled datasets\n",
        "tweets_df.to_csv(\"sentiment_msft_tweets.csv\", index=False)\n",
        "news_df.to_csv(\"sentiment_msft_news.csv\", index=False)\n",
        "\n",
        "print(\"Sentiment analysis complete! Results saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Traditional ML methods for sentiment Analysis -MSFT"
      ],
      "metadata": {
        "id": "8z3_YOhb2Zj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(\n",
        "    tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(\n",
        "    news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Function to train models\n",
        "def train_model(X_train, y_train, X_test, y_test, model):\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    predictions = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Model {model.__class__.__name__} Accuracy: {accuracy:.4f}\")\n",
        "    return pipeline\n",
        "\n",
        "# Models to Train\n",
        "models = {\n",
        "    'Naïve Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(kernel='linear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "# Train models for Tweets\n",
        "print(\"\\n Training Models for Tweet Sentiment Classification \")\n",
        "trained_models_tweets = {\n",
        "    name: train_model(X_tweets, y_tweets, X_test_tweets, y_test_tweets, model)\n",
        "    for name, model in models.items()\n",
        "}\n",
        "\n",
        "# Train models for News\n",
        "print(\"\\n Training Models for News Sentiment Classification \")\n",
        "trained_models_news = {\n",
        "    name: train_model(X_news, y_news, X_test_news, y_test_news, model)\n",
        "    for name, model in models.items()\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojPQbUo1q3fz",
        "outputId": "50c3909f-2060-4b8c-a9ce-f0781fa19473"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Models for Tweet Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.7534\n",
            "Model SVC Accuracy: 0.8918\n",
            "Model DecisionTreeClassifier Accuracy: 0.8016\n",
            "Model RandomForestClassifier Accuracy: 0.8549\n",
            "\n",
            " Training Models for News Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.6032\n",
            "Model SVC Accuracy: 0.6032\n",
            "Model DecisionTreeClassifier Accuracy: 0.4286\n",
            "Model RandomForestClassifier Accuracy: 0.5238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models for Tweet Sentiment Classification\n",
        "* Model MultinomialNB Accuracy: 0.7532\n",
        "* Model SVC Accuracy: 0.8911\n",
        "* Model DecisionTreeClassifier Accuracy: 0.8057\n",
        "* Model RandomForestClassifier Accuracy: 0.8523\n",
        "\n",
        "Training Models for News Sentiment Classification\n",
        "* Model MultinomialNB Accuracy: 0.6032\n",
        "* Model SVC Accuracy: 0.6032\n",
        "* Model DecisionTreeClassifier Accuracy: 0.4762\n",
        "* Model RandomForestClassifier Accuracy: 0.5397\n",
        "\n"
      ],
      "metadata": {
        "id": "tcT7bsxtgd40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ANN for Sentiment Analysis - MSFT"
      ],
      "metadata": {
        "id": "YVbBVat5FBW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2EJzTzhwiaV",
        "outputId": "bd942a81-c5da-4386-aa6f-f07c5715ed4d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, markdown, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 markdown-3.7 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tweets = vectorizer.fit_transform(X_tweets).toarray()\n",
        "X_test_tweets = vectorizer.transform(X_test_tweets).toarray()\n",
        "X_news = vectorizer.fit_transform(X_news).toarray()\n",
        "X_test_news = vectorizer.transform(X_test_news).toarray()\n",
        "\n",
        "# Build ANN model\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert Sentiment Labels into One-Hot Encoding\n",
        "y_tweets = to_categorical(y_tweets, num_classes=3)\n",
        "y_test_tweets = to_categorical(y_test_tweets, num_classes=3)\n",
        "y_news = to_categorical(y_news, num_classes=3)\n",
        "y_test_news = to_categorical(y_test_news, num_classes=3)\n",
        "\n",
        "# Train ANN model\n",
        "print(\"\\n Training ANN Model for Tweets Sentiment Classification \")\n",
        "ann_tweet_model = build_ann(X_tweets.shape[1])\n",
        "ann_tweet_model.fit(X_tweets, y_tweets, epochs=10, batch_size=32, validation_data=(X_test_tweets, y_test_tweets))\n",
        "\n",
        "# Train ANN for News\n",
        "print(\"\\n Training ANN Model for News Sentiment Classification \")\n",
        "ann_news_model = build_ann(X_news.shape[1])\n",
        "ann_news_model.fit(X_news, y_news, epochs=10, batch_size=32, validation_data=(X_test_news, y_test_news))\n",
        "\n",
        "\n",
        "y_test_tweets_labels = np.argmax(y_test_tweets, axis=1)\n",
        "y_pred_tweets = ann_tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)  # Convert probabilities to class labels\n",
        "print(\" ANN Tweet Sentiment Classification Report:\")   #  ANN Evaluation for Tweets\n",
        "print(classification_report(y_test_tweets_labels, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets_labels, y_pred_tweets))\n",
        "\n",
        "y_test_news_labels = np.argmax(y_test_news, axis=1)\n",
        "y_pred_news = ann_news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)  # Convert probabilities to class labels\n",
        "print(\"\\n ANN News Sentiment Classification Report:\")  # ANN Evaluation for News\n",
        "print(classification_report(y_test_news_labels, y_pred_news))\n",
        "print(confusion_matrix(y_test_news_labels, y_pred_news))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXBdwB_txv4b",
        "outputId": "64f4bcbc-443c-449a-c6ba-fbff06ab6223"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training ANN Model for Tweets Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.7264 - loss: 0.6543 - val_accuracy: 0.8784 - val_loss: 0.3599\n",
            "Epoch 2/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9088 - loss: 0.2768 - val_accuracy: 0.8858 - val_loss: 0.3467\n",
            "Epoch 3/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9352 - loss: 0.1985 - val_accuracy: 0.8887 - val_loss: 0.3520\n",
            "Epoch 4/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9520 - loss: 0.1437 - val_accuracy: 0.8874 - val_loss: 0.3909\n",
            "Epoch 5/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9701 - loss: 0.0932 - val_accuracy: 0.8856 - val_loss: 0.4454\n",
            "Epoch 6/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9822 - loss: 0.0581 - val_accuracy: 0.8888 - val_loss: 0.4855\n",
            "Epoch 7/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9860 - loss: 0.0441 - val_accuracy: 0.8870 - val_loss: 0.5910\n",
            "Epoch 8/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9881 - loss: 0.0383 - val_accuracy: 0.8892 - val_loss: 0.6289\n",
            "Epoch 9/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9905 - loss: 0.0295 - val_accuracy: 0.8883 - val_loss: 0.6680\n",
            "Epoch 10/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9926 - loss: 0.0229 - val_accuracy: 0.8870 - val_loss: 0.7587\n",
            "\n",
            " Training ANN Model for News Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.4274 - loss: 1.0917 - val_accuracy: 0.5873 - val_loss: 1.0599\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4896 - loss: 1.0470 - val_accuracy: 0.5873 - val_loss: 1.0184\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5114 - loss: 0.9831 - val_accuracy: 0.5873 - val_loss: 0.9782\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4697 - loss: 0.9270 - val_accuracy: 0.5873 - val_loss: 0.9451\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4147 - loss: 0.8697 - val_accuracy: 0.5873 - val_loss: 0.9261\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6252 - loss: 0.6812 - val_accuracy: 0.5873 - val_loss: 0.9144\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7612 - loss: 0.5633 - val_accuracy: 0.5873 - val_loss: 0.9071\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9315 - loss: 0.4037 - val_accuracy: 0.5714 - val_loss: 0.9078\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9973 - loss: 0.2657 - val_accuracy: 0.5397 - val_loss: 0.9081\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.1716 - val_accuracy: 0.5556 - val_loss: 0.9089\n",
            "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            " ANN Tweet Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.91      4818\n",
            "           1       0.91      0.91      0.91      5160\n",
            "           2       0.80      0.64      0.71      1522\n",
            "\n",
            "    accuracy                           0.89     11500\n",
            "   macro avg       0.86      0.83      0.84     11500\n",
            "weighted avg       0.88      0.89      0.88     11500\n",
            "\n",
            "[[4546  185   87]\n",
            " [ 317 4683  160]\n",
            " [ 269  282  971]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step \n",
            "\n",
            " ANN News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.18      0.30        17\n",
            "           1       0.62      0.84      0.71        37\n",
            "           2       0.10      0.11      0.11         9\n",
            "\n",
            "    accuracy                           0.56        63\n",
            "   macro avg       0.57      0.38      0.37        63\n",
            "weighted avg       0.65      0.56      0.51        63\n",
            "\n",
            "[[ 3 11  3]\n",
            " [ 0 31  6]\n",
            " [ 0  8  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM for Sentiment Analysis - MSFT"
      ],
      "metadata": {
        "id": "-yDBO8Uy2frr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed datasets for tweets and news\n",
        "tweets_df = pd.read_csv('/content/sentiment_msft_tweets.csv')\n",
        "news_df = pd.read_csv('/content/sentiment_msft_news.csv')\n",
        "\n",
        "# Replace NaN values with an empty string in both tweet and news data\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].fillna('')\n",
        "news_df['text'] = news_df['text'].fillna('')\n",
        "\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)\n",
        "\n",
        "\n",
        "\n",
        "encoder = LabelEncoder()     # Encoding Sentiment Labels (assuming 'sentiment' column with Positive, Negative, Neutral)\n",
        "tweets_df['encoded_sentiment'] = encoder.fit_transform(tweets_df['sentiment'])\n",
        "news_df['encoded_sentiment'] = encoder.fit_transform(news_df['sentiment'])\n",
        "\n",
        "y_tweets = tweets_df['encoded_sentiment']\n",
        "y_news = news_df['encoded_sentiment']\n",
        "\n",
        "# Tokenizing the Text Data (using preprocessed text column)\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(tweets_df['cleaned_tweet'])\n",
        "X_tweets = tokenizer.texts_to_sequences(tweets_df['cleaned_tweet'])\n",
        "X_tweets = pad_sequences(X_tweets, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(news_df['text'])\n",
        "X_news = tokenizer.texts_to_sequences(news_df['text'])\n",
        "X_news = pad_sequences(X_news, maxlen=100)\n",
        "\n",
        "# Step 6: Splitting Data into Training and Testing Sets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# Building LSTM Model\n",
        "tweet_model = Sequential()\n",
        "tweet_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "tweet_model.add(SpatialDropout1D(0.2))\n",
        "tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "# Building LSTM Model for News Sentiment Classification\n",
        "news_model = Sequential()\n",
        "news_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "news_model.add(SpatialDropout1D(0.2))\n",
        "news_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "news_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "news_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "news_history = news_model.fit(X_train_news, y_train_news, epochs=5, batch_size=64, validation_data=(X_test_news, y_test_news), verbose=2)\n",
        "\n",
        "\n",
        "y_pred_tweets = tweet_model.predict(X_test_tweets) # Evaluate Tweets Model\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)\n",
        "\n",
        "y_pred_news = news_model.predict(X_test_news)      # Evaluate News Model\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)\n",
        "\n",
        "print(\"Tweets Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "\n",
        "print(\"News Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(confusion_matrix(y_test_news, y_pred_news))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWwYaHGC2jhe",
        "outputId": "6755a01c-4458-4931-fee2-f85aac61533d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "719/719 - 134s - 186ms/step - accuracy: 0.7902 - loss: 0.5489 - val_accuracy: 0.8879 - val_loss: 0.3484\n",
            "Epoch 2/5\n",
            "719/719 - 127s - 177ms/step - accuracy: 0.9023 - loss: 0.3049 - val_accuracy: 0.9043 - val_loss: 0.3127\n",
            "Epoch 3/5\n",
            "719/719 - 127s - 177ms/step - accuracy: 0.9156 - loss: 0.2673 - val_accuracy: 0.9071 - val_loss: 0.3067\n",
            "Epoch 4/5\n",
            "719/719 - 127s - 177ms/step - accuracy: 0.9234 - loss: 0.2420 - val_accuracy: 0.9039 - val_loss: 0.3117\n",
            "Epoch 5/5\n",
            "719/719 - 128s - 178ms/step - accuracy: 0.9284 - loss: 0.2222 - val_accuracy: 0.9050 - val_loss: 0.3173\n",
            "Epoch 1/5\n",
            "4/4 - 5s - 1s/step - accuracy: 0.4520 - loss: 1.0919 - val_accuracy: 0.5873 - val_loss: 1.0648\n",
            "Epoch 2/5\n",
            "4/4 - 1s - 199ms/step - accuracy: 0.4920 - loss: 1.0580 - val_accuracy: 0.5873 - val_loss: 1.0028\n",
            "Epoch 3/5\n",
            "4/4 - 1s - 195ms/step - accuracy: 0.4920 - loss: 1.0351 - val_accuracy: 0.5873 - val_loss: 0.9999\n",
            "Epoch 4/5\n",
            "4/4 - 1s - 195ms/step - accuracy: 0.4920 - loss: 1.0081 - val_accuracy: 0.5873 - val_loss: 1.0023\n",
            "Epoch 5/5\n",
            "4/4 - 1s - 206ms/step - accuracy: 0.4920 - loss: 0.9919 - val_accuracy: 0.5873 - val_loss: 0.9892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b885e81b9c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378ms/step\n",
            "Tweets Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.68      0.74      1522\n",
            "           1       0.92      0.94      0.93      4818\n",
            "           2       0.91      0.93      0.92      5160\n",
            "\n",
            "    accuracy                           0.90     11500\n",
            "   macro avg       0.88      0.85      0.87     11500\n",
            "weighted avg       0.90      0.90      0.90     11500\n",
            "\n",
            "[[1038  206  278]\n",
            " [  89 4546  183]\n",
            " [ 148  189 4823]]\n",
            "News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.59      1.00      0.74        37\n",
            "\n",
            "    accuracy                           0.59        63\n",
            "   macro avg       0.20      0.33      0.25        63\n",
            "weighted avg       0.34      0.59      0.43        63\n",
            "\n",
            "[[ 0  0  9]\n",
            " [ 0  0 17]\n",
            " [ 0  0 37]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for compiling is almost 30 mins**"
      ],
      "metadata": {
        "id": "RsELIzCk5qML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/5\n",
        "\n",
        "719/719 - 171s - loss: 0.5244 - accuracy: 0.8012 - val_loss: 0.3485 - val_accuracy: 0.8920 - 171s/epoch - 238ms/step\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "719/719 - 167s - loss: 0.3019 - accuracy: 0.9046 - val_loss: 0.3127 - val_accuracy: 0.9043 - 167s/epoch - 232ms/step\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "719/719 - 167s - loss: 0.2631 - accuracy: 0.9160 - val_loss: 0.3165 - val_accuracy: 0.9011 - 167s/epoch - 232ms/step\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "719/719 - 166s - loss: 0.2374 - accuracy: 0.9238 - val_loss: 0.3271 - val_accuracy: 0.9006 - 166s/epoch - 231ms/step\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "719/719 - 169s - loss: 0.2134 - accuracy: 0.9310 - val_loss: 0.3373 - val_accuracy: 0.9060 - 169s/epoch - 235ms/step\n",
        "\n",
        "Epoch 1/5\n",
        "\n",
        "4/4 - 4s - loss: 1.0942 - accuracy: 0.4520 - val_loss: 1.0805 - val_accuracy: 0.5873 - 4s/epoch - 1s/step\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0685 - accuracy: 0.4920 - val_loss: 1.0473 - val_accuracy: 0.5873 - 988ms/epoch - 247ms/step\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0333 - accuracy: 0.4920 - val_loss: 0.9976 - val_accuracy: 0.5873 - 947ms/epoch - 237ms/step\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0035 - accuracy: 0.4920 - val_loss: 0.9927 - val_accuracy: 0.5873 - 946ms/epoch - 236ms/step\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "4/4 - 1s - loss: 0.9696 - accuracy: 0.4920 - val_loss: 0.9814 - val_accuracy: 0.5873 - 923ms/epoch - 231ms/step\n",
        "\n",
        "360/360 [==============================] - 9s 24ms/step\n",
        "\n",
        "2/2 [==============================] - 0s 16ms/step\n",
        "\n",
        "Tweets Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.64      0.74      1522\n",
        "           1       0.91      0.96      0.93      4818\n",
        "           2       0.91      0.94      0.92      5160\n",
        "\n",
        "    accuracy                           0.91     11500\n",
        "\n",
        "   macro avg       0.89      0.85      0.86     11500\n",
        "\n",
        "weighted avg       0.90      0.91      0.90     11500\n",
        "\n",
        "\n",
        "[[ 979  222  321]\n",
        "\n",
        " [  62 4605  151]\n",
        "\n",
        " [  97  228 4835]]\n",
        "\n",
        "News Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.00      0.00      0.00         9\n",
        "           1       0.00      0.00      0.00        17\n",
        "           2       0.59      1.00      0.74        37\n",
        "\n",
        "    accuracy                           0.59        63\n",
        "\n",
        "   macro avg       0.20      0.33      0.25        63\n",
        "\n",
        "weighted avg       0.34      0.59      0.43        63\n",
        "\n",
        "\n",
        "[[ 0  0  9]\n",
        "\n",
        " [ 0  0 17]\n",
        "\n",
        " [ 0  0 37]]"
      ],
      "metadata": {
        "id": "ufbeO17xjyki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using AdaBoost for Sentiment Analysis - MSFT\n"
      ],
      "metadata": {
        "id": "fWJFW4g25jQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RE-RUN THIS CODE**"
      ],
      "metadata": {
        "id": "XuxATEuyyAiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Data\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "# Replace -1 with 0 (assuming -1 represents negative sentiment)\n",
        "tweets_df['sentiment'] = tweets_df['sentiment'].replace(-1, 0)\n",
        "news_df['sentiment'] = news_df['sentiment'].replace(-1, 0)\n",
        "\n",
        "# Convert text into TF-IDF vectors for tweets\n",
        "vectorizer_tweets = TfidfVectorizer(max_features=5000)\n",
        "X_tweets = vectorizer_tweets.fit_transform(tweets_df['cleaned_tweet'].fillna('')).toarray()\n",
        "y_tweets = tweets_df['sentiment']\n",
        "\n",
        "# Convert text into TF-IDF vectors for news\n",
        "vectorizer_news = TfidfVectorizer(max_features=5000)\n",
        "X_news = vectorizer_news.fit_transform(news_df['text'].fillna('')).toarray()\n",
        "y_news = news_df['sentiment']\n",
        "\n",
        "# Train-test split for tweets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train-test split for news\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# AdaBoost Classifier with Decision Tree as base learner\n",
        "adaboost_model_tweets = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.5, random_state=42)\n",
        "adaboost_model_news = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.5, random_state=42)\n",
        "\n",
        "# Train models\n",
        "adaboost_model_tweets.fit(X_train_tweets, y_train_tweets)\n",
        "adaboost_model_news.fit(X_train_news, y_train_news)\n",
        "\n",
        "# Predictions\n",
        "y_pred_tweets = adaboost_model_tweets.predict(X_test_tweets)\n",
        "y_pred_news = adaboost_model_news.predict(X_test_news)\n",
        "\n",
        "# Evaluation for tweets\n",
        "print(\"AdaBoost Sentiment Analysis Performance (Tweets):\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(\"Accuracy:\", accuracy_score(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "# Evaluation for news\n",
        "print(\"AdaBoost Sentiment Analysis Performance (News):\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(\"Accuracy:\", accuracy_score(y_test_news, y_pred_news))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU15IskaqE6g",
        "outputId": "807331eb-a68b-493c-a23c-e77d5a3d55af"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Sentiment Analysis Performance (Tweets):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.98      0.76      6340\n",
            "           1       0.91      0.29      0.44      5160\n",
            "\n",
            "    accuracy                           0.67     11500\n",
            "   macro avg       0.77      0.63      0.60     11500\n",
            "weighted avg       0.75      0.67      0.62     11500\n",
            "\n",
            "Accuracy: 0.6688695652173913\n",
            "AdaBoost Sentiment Analysis Performance (News):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.85      0.60        26\n",
            "           1       0.75      0.32      0.45        37\n",
            "\n",
            "    accuracy                           0.54        63\n",
            "   macro avg       0.61      0.59      0.53        63\n",
            "weighted avg       0.63      0.54      0.51        63\n",
            "\n",
            "Accuracy: 0.5396825396825397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers datasets torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruqLGYFErZy8",
        "outputId": "fdc04bd2-e73b-4e66-da8e-6744d3535777"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
            "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 datasets-3.4.1 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multiprocess-0.70.16 propcache-0.3.0 xxhash-3.5.0 yarl-1.18.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import pandas as pd\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# # Load Data\n",
        "# tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "# news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "# # Replace -1 with 0 (assuming -1 represents negative sentiment)\n",
        "# tweets_df['sentiment'] = tweets_df['sentiment'].replace(-1, 0)\n",
        "# news_df['sentiment'] = news_df['sentiment'].replace(-1, 0)\n",
        "\n",
        "# # Train-test split for Tweets\n",
        "# X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(\n",
        "#     tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# # Train-test split for News\n",
        "# X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(\n",
        "#     news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# # Load Pretrained BERT Tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Convert Text to Tokenized Input\n",
        "# class SentimentDataset(Dataset):\n",
        "#     def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "#         self.texts = texts.tolist()\n",
        "#         self.labels = labels.tolist()\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_len = max_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         encoding = self.tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "#         return {\n",
        "#             'input_ids': encoding['input_ids'].squeeze(0),\n",
        "#             'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "#             'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# # Convert Data into PyTorch Datasets\n",
        "# train_dataset_tweets = SentimentDataset(X_train_tweets, y_train_tweets, tokenizer)\n",
        "# test_dataset_tweets = SentimentDataset(X_test_tweets, y_test_tweets, tokenizer)\n",
        "\n",
        "# train_dataset_news = SentimentDataset(X_train_news, y_train_news, tokenizer)\n",
        "# test_dataset_news = SentimentDataset(X_test_news, y_test_news, tokenizer)\n",
        "\n",
        "# # Dataloader\n",
        "# train_loader_tweets = DataLoader(train_dataset_tweets, batch_size=16, shuffle=True)\n",
        "# test_loader_tweets = DataLoader(test_dataset_tweets, batch_size=16)\n",
        "\n",
        "# train_loader_news = DataLoader(train_dataset_news, batch_size=16, shuffle=True)\n",
        "# test_loader_news = DataLoader(test_dataset_news, batch_size=16)\n",
        "\n",
        "# # Load Pretrained BERT Model\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model_tweets = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(device)\n",
        "# model_news = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(device)\n",
        "\n",
        "# # Optimizer & Loss\n",
        "# optimizer_tweets = torch.optim.AdamW(model_tweets.parameters(), lr=2e-5)\n",
        "# optimizer_news = torch.optim.AdamW(model_news.parameters(), lr=2e-5)\n",
        "# loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# # Training Function\n",
        "# def train_model(model, train_loader, optimizer):\n",
        "#     model.train()\n",
        "#     for batch in train_loader:\n",
        "#         input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#         loss = loss_fn(outputs.logits, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "# # Train for 3 epochs\n",
        "# for epoch in range(3):\n",
        "#     print(f\"Epoch {epoch+1}/3\")\n",
        "#     train_model(model_tweets, train_loader_tweets, optimizer_tweets)\n",
        "#     train_model(model_news, train_loader_news, optimizer_news)\n",
        "\n",
        "# # Evaluation Function\n",
        "# def evaluate_model(model, test_loader):\n",
        "#     model.eval()\n",
        "#     y_pred, y_true = [], []\n",
        "#     for batch in test_loader:\n",
        "#         input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits\n",
        "#         y_pred.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "#         y_true.extend(labels.cpu().numpy())\n",
        "#     return y_true, y_pred\n",
        "\n",
        "# # Evaluate Tweets Model\n",
        "# y_true_tweets, y_pred_tweets = evaluate_model(model_tweets, test_loader_tweets)\n",
        "# print(\"BERT Sentiment Analysis Performance (Tweets):\")\n",
        "# print(classification_report(y_true_tweets, y_pred_tweets))\n",
        "# print(\"Accuracy:\", accuracy_score(y_true_tweets, y_pred_tweets))\n",
        "\n",
        "# # Evaluate News Model\n",
        "# y_true_news, y_pred_news = evaluate_model(model_news, test_loader_news)\n",
        "# print(\"\\n BERT Sentiment Analysis Performance (News):\")\n",
        "# print(classification_report(y_true_news, y_pred_news))\n",
        "# print(\"Accuracy:\", accuracy_score(y_true_news, y_pred_news))\n"
      ],
      "metadata": {
        "id": "46y3MIb9rfS1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Unique Sentiment Labels in Tweets:\", y_train_tweets.unique())\n",
        "# print(\"Unique Sentiment Labels in News:\", y_train_news.unique())\n"
      ],
      "metadata": {
        "id": "eI0HqXFHr9Fl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mz0epRlEoob"
      },
      "source": [
        "# Sentiment Analysis- tesla"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALGORITHMS WORKS !!!"
      ],
      "metadata": {
        "id": "L_E86_qKSxVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KsuScRGEXniM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load datasets\n",
        "tweets_df = pd.read_csv(\"/content/cleaned_tsla_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/cleaned_tsla_articles.csv\")\n",
        "\n",
        "# Ensure text column is string type\n",
        "tweets_df['Tweet'] = tweets_df['Tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "35c_SFF4Xv2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d22dcd-118f-4fe2-e1e5-f1e250eebea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# VADER Sentiment Analysis for Tweets\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "# Function to get sentiment score from VADER\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply VADER sentiment analysis to tweets\n",
        "tweets_df['sentiment'] = tweets_df['Tweet'].apply(get_vader_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "q-26gh4WXxx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1d35d8-75ca-4d60-8525-b5541b7fd5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# TextBlob Sentiment Analysis for News\n",
        "\n",
        "# Function to get sentiment score from TextBlob\n",
        "def get_textblob_sentiment(text):\n",
        "    score = TextBlob(text).sentiment.polarity\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply TextBlob sentiment analysis to news articles\n",
        "news_df['sentiment'] = news_df['text'].apply(get_textblob_sentiment)\n",
        "news_df['sentiment'] = news_df['sentiment'].astype(int)\n",
        "\n",
        "# Save sentiment-labeled datasets\n",
        "tweets_df.to_csv(\"sentiment_tsla_tweets.csv\", index=False)\n",
        "news_df.to_csv(\"sentiment_tsla_news.csv\", index=False)\n",
        "\n",
        "print(\"Sentiment analysis complete! Results saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Traditional ML methods for sentiment analysis"
      ],
      "metadata": {
        "id": "x1ysblnS2won"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "tweets_df = tweets_df.sample(min(10000, len(tweets_df)), random_state=42)\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(\n",
        "    tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(\n",
        "    news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Function to train models\n",
        "def train_model(X_train, y_train, X_test, y_test, model):\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    predictions = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Model {model.__class__.__name__} Accuracy: {accuracy:.4f}\")\n",
        "    return pipeline\n",
        "\n",
        "# Models to Train\n",
        "models = {\n",
        "    'Naïve Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(kernel='linear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "# Train models for Tweets\n",
        "print(\"\\n Training Models for Tweet Sentiment Classification \")\n",
        "trained_models_tweets = {\n",
        "    name: train_model(X_tweets, y_tweets, X_test_tweets, y_test_tweets, model)\n",
        "    for name, model in models.items()\n",
        "}\n",
        "\n",
        "# Train models for News\n",
        "print(\"\\n Training Models for News Sentiment Classification \")\n",
        "trained_models_news = {\n",
        "    name: train_model(X_news, y_news, X_test_news, y_test_news, model)\n",
        "    for name, model in models.items()\n",
        "}\n"
      ],
      "metadata": {
        "id": "0WhdsIfQ4Gy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f841c5b3-1532-4e48-c1e8-9e77258a0e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Models for Tweet Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.7215\n",
            "Model SVC Accuracy: 0.8285\n",
            "Model DecisionTreeClassifier Accuracy: 0.7805\n",
            "Model RandomForestClassifier Accuracy: 0.7870\n",
            "\n",
            " Training Models for News Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.6032\n",
            "Model SVC Accuracy: 0.6032\n",
            "Model DecisionTreeClassifier Accuracy: 0.4444\n",
            "Model RandomForestClassifier Accuracy: 0.5238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models for Tweet Sentiment Classification\n",
        "*   Model MultinomialNB Accuracy: 0.7215\n",
        "*   Model SVC Accuracy: 0.8285\n",
        "*   Model DecisionTreeClassifier Accuracy: 0.7720\n",
        "*   Model RandomForestClassifier Accuracy: 0.7880\n",
        "\n",
        "\n",
        "Training Models for News Sentiment Classification\n",
        "*   Model MultinomialNB Accuracy: 0.5873\n",
        "*   Model SVC Accuracy: 0.5556\n",
        "*   Model DecisionTreeClassifier Accuracy: 0.5397\n",
        "*   Model RandomForestClassifier Accuracy: 0.5238\n"
      ],
      "metadata": {
        "id": "YR9q619vF528"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ANN for Sentiment Analysis"
      ],
      "metadata": {
        "id": "DRJBd8JSFRzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_tsla_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_tsla_news.csv\")\n",
        "\n",
        "# Replace NaN values with an empty string in both tweet and news data\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].fillna('')\n",
        "news_df['text'] = news_df['text'].fillna('')\n",
        "\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)\n",
        "\n",
        "# Reduce dataset size to prevent memory overload\n",
        "tweets_df = tweets_df.sample(min(50000, len(tweets_df)), random_state=42)\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tweets = vectorizer.fit_transform(X_tweets).toarray()\n",
        "X_test_tweets = vectorizer.transform(X_test_tweets).toarray()\n",
        "X_news = vectorizer.fit_transform(X_news).toarray()\n",
        "X_test_news = vectorizer.transform(X_test_news).toarray()\n",
        "\n",
        "# Build ANN model\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential([\n",
        "       Input(shape=(input_dim,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert Sentiment Labels into One-Hot Encoding\n",
        "y_tweets = to_categorical(y_tweets, num_classes=3)\n",
        "y_test_tweets = to_categorical(y_test_tweets, num_classes=3)\n",
        "y_news = to_categorical(y_news, num_classes=3)\n",
        "y_test_news = to_categorical(y_test_news, num_classes=3)\n",
        "\n",
        "# Train ANN model\n",
        "print(\"\\n Training ANN Model for Tweets Sentiment Classification \")\n",
        "ann_tweet_model = build_ann(X_tweets.shape[1])\n",
        "ann_tweet_model.fit(X_tweets, y_tweets, epochs=10, batch_size=32, validation_data=(X_test_tweets, y_test_tweets))\n",
        "\n",
        "# Train ANN for News\n",
        "print(\"\\n Training ANN Model for News Sentiment Classification \")\n",
        "ann_news_model = build_ann(X_news.shape[1])\n",
        "ann_news_model.fit(X_news, y_news, epochs=10, batch_size=32, validation_data=(X_test_news, y_test_news))\n",
        "\n",
        "y_test_tweets_labels = np.argmax(y_test_tweets, axis=1)\n",
        "y_pred_tweets = ann_tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)  # Convert probabilities to class labels\n",
        "print(\" ANN Tweet Sentiment Classification Report:\")   #  ANN Evaluation for Tweets\n",
        "print(classification_report(y_test_tweets_labels, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets_labels, y_pred_tweets))\n",
        "\n",
        "y_test_news_labels = np.argmax(y_test_news, axis=1)\n",
        "y_pred_news = ann_news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)  # Convert probabilities to class labels\n",
        "print(\"\\n ANN News Sentiment Classification Report:\")  # ANN Evaluation for News\n",
        "print(classification_report(y_test_news_labels, y_pred_news))\n",
        "print(confusion_matrix(y_test_news_labels, y_pred_news))\n"
      ],
      "metadata": {
        "id": "cMFlhzcjEe2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed462b43-0dfd-4c95-c843-abc18f2d473a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training ANN Model for Tweets Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.5954 - loss: 0.8592 - val_accuracy: 0.8195 - val_loss: 0.5095\n",
            "Epoch 2/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.8563 - loss: 0.4200 - val_accuracy: 0.8312 - val_loss: 0.4758\n",
            "Epoch 3/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.8834 - loss: 0.3475 - val_accuracy: 0.8335 - val_loss: 0.4737\n",
            "Epoch 4/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.8979 - loss: 0.3051 - val_accuracy: 0.8287 - val_loss: 0.4857\n",
            "Epoch 5/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9065 - loss: 0.2751 - val_accuracy: 0.8326 - val_loss: 0.5032\n",
            "Epoch 6/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9187 - loss: 0.2444 - val_accuracy: 0.8287 - val_loss: 0.5225\n",
            "Epoch 7/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9286 - loss: 0.2177 - val_accuracy: 0.8274 - val_loss: 0.5480\n",
            "Epoch 8/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9432 - loss: 0.1850 - val_accuracy: 0.8236 - val_loss: 0.5748\n",
            "Epoch 9/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9535 - loss: 0.1547 - val_accuracy: 0.8241 - val_loss: 0.6109\n",
            "Epoch 10/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - accuracy: 0.9621 - loss: 0.1301 - val_accuracy: 0.8237 - val_loss: 0.6525\n",
            "\n",
            " Training ANN Model for News Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.3411 - loss: 1.0948 - val_accuracy: 0.5556 - val_loss: 1.0709\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6497 - loss: 1.0626 - val_accuracy: 0.6111 - val_loss: 1.0583\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6366 - loss: 1.0318 - val_accuracy: 0.5833 - val_loss: 1.0457\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6529 - loss: 0.9907 - val_accuracy: 0.5833 - val_loss: 1.0327\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6496 - loss: 0.9608 - val_accuracy: 0.5833 - val_loss: 1.0199\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6454 - loss: 0.9265 - val_accuracy: 0.5833 - val_loss: 1.0069\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6603 - loss: 0.8774 - val_accuracy: 0.5833 - val_loss: 0.9939\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6713 - loss: 0.8481 - val_accuracy: 0.5833 - val_loss: 0.9822\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6573 - loss: 0.8101 - val_accuracy: 0.5833 - val_loss: 0.9726\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7021 - loss: 0.7406 - val_accuracy: 0.5833 - val_loss: 0.9644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a0e51bcafc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            " ANN Tweet Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87      3003\n",
            "           1       0.85      0.83      0.84      4236\n",
            "           2       0.76      0.74      0.75      2761\n",
            "\n",
            "    accuracy                           0.82     10000\n",
            "   macro avg       0.82      0.82      0.82     10000\n",
            "weighted avg       0.82      0.82      0.82     10000\n",
            "\n",
            "[[2659  167  177]\n",
            " [ 239 3534  463]\n",
            " [ 237  480 2044]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            " ANN News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        11\n",
            "           1       0.58      1.00      0.74        21\n",
            "           2       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.58        36\n",
            "   macro avg       0.19      0.33      0.25        36\n",
            "weighted avg       0.34      0.58      0.43        36\n",
            "\n",
            "[[ 0 11  0]\n",
            " [ 0 21  0]\n",
            " [ 0  4  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM for sentiment analysis"
      ],
      "metadata": {
        "id": "hzfKyY0Q2qBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IT WORKS !!!!!**"
      ],
      "metadata": {
        "id": "eCrzKdAZdGBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed datasets for tweets and news\n",
        "tweets_df = pd.read_csv('/content/sentiment_tsla_tweets.csv')  # replace with your actual dataset path\n",
        "news_df = pd.read_csv('/content/sentiment_tsla_news.csv')  # replace with your actual news dataset path\n",
        "\n",
        "# Replace NaN values with an empty string in both tweet and news data\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].fillna('')\n",
        "news_df['text'] = news_df['text'].fillna('')\n",
        "\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)\n",
        "\n",
        "\n",
        "# Encoding Sentiment Labels (assuming 'sentiment' column with Positive, Negative, Neutral)\n",
        "encoder = LabelEncoder()\n",
        "tweets_df['encoded_sentiment'] = encoder.fit_transform(tweets_df['sentiment'])\n",
        "news_df['encoded_sentiment'] = encoder.fit_transform(news_df['sentiment'])\n",
        "\n",
        "y_tweets = tweets_df['encoded_sentiment']\n",
        "y_news = news_df['encoded_sentiment']\n",
        "\n",
        "#Tokenizing the Text Data (using preprocessed 'cleaned_text')\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(tweets_df['cleaned_tweet'])\n",
        "X_tweets = tokenizer.texts_to_sequences(tweets_df['cleaned_tweet'])\n",
        "X_tweets = pad_sequences(X_tweets, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(news_df['text'])\n",
        "X_news = tokenizer.texts_to_sequences(news_df['text'])\n",
        "X_news = pad_sequences(X_news, maxlen=100)\n",
        "\n",
        "#Splitting Data into Training and Testing Sets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "#Building ANN Model for Tweets Sentiment Classification\n",
        "tweet_model = Sequential()\n",
        "tweet_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "tweet_model.add(SpatialDropout1D(0.2))\n",
        "tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "#Building ANN Model for News Sentiment Classification\n",
        "news_model = Sequential()\n",
        "news_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "news_model.add(SpatialDropout1D(0.2))\n",
        "news_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "news_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "news_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "news_history = news_model.fit(X_train_news, y_train_news, epochs=5, batch_size=64, validation_data=(X_test_news, y_test_news), verbose=2)\n",
        "\n",
        "# Step 9: Evaluating the Models\n",
        "# Evaluate Tweets Model\n",
        "y_pred_tweets = tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)\n",
        "\n",
        "# Evaluate News Model\n",
        "y_pred_news = news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)\n",
        "\n",
        "# Performance Metrics for Tweets\n",
        "print(\"Tweets Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "# Performance Metrics for News\n",
        "print(\"News Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(confusion_matrix(y_test_news, y_pred_news))"
      ],
      "metadata": {
        "id": "gx2A2ud-3pPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41943ea-1570-493f-c127-4314630bbf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "4544/4544 - 1334s - 294ms/step - accuracy: 0.8483 - loss: 0.4363 - val_accuracy: 0.8775 - val_loss: 0.3774\n",
            "Epoch 2/5\n",
            "4544/4544 - 1355s - 298ms/step - accuracy: 0.8803 - loss: 0.3644 - val_accuracy: 0.8867 - val_loss: 0.3531\n",
            "Epoch 3/5\n",
            "4544/4544 - 1327s - 292ms/step - accuracy: 0.8886 - loss: 0.3426 - val_accuracy: 0.8871 - val_loss: 0.3518\n",
            "Epoch 4/5\n",
            "4544/4544 - 1308s - 288ms/step - accuracy: 0.8923 - loss: 0.3281 - val_accuracy: 0.8883 - val_loss: 0.3496\n",
            "Epoch 5/5\n",
            "4544/4544 - 1304s - 287ms/step - accuracy: 0.8953 - loss: 0.3164 - val_accuracy: 0.8855 - val_loss: 0.3578\n",
            "Epoch 1/5\n",
            "3/3 - 7s - 2s/step - accuracy: 0.4437 - loss: 1.0925 - val_accuracy: 0.5833 - val_loss: 1.0673\n",
            "Epoch 2/5\n",
            "3/3 - 1s - 446ms/step - accuracy: 0.5845 - loss: 1.0506 - val_accuracy: 0.5833 - val_loss: 1.0140\n",
            "Epoch 3/5\n",
            "3/3 - 1s - 474ms/step - accuracy: 0.5845 - loss: 0.9891 - val_accuracy: 0.5833 - val_loss: 0.9804\n",
            "Epoch 4/5\n",
            "3/3 - 2s - 695ms/step - accuracy: 0.5845 - loss: 0.9610 - val_accuracy: 0.5833 - val_loss: 0.9789\n",
            "Epoch 5/5\n",
            "3/3 - 1s - 359ms/step - accuracy: 0.5845 - loss: 0.9229 - val_accuracy: 0.5833 - val_loss: 0.9701\n",
            "\u001b[1m2272/2272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 43ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step\n",
            "Tweets Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83     20075\n",
            "           1       0.89      0.94      0.91     21900\n",
            "           2       0.91      0.89      0.90     30720\n",
            "\n",
            "    accuracy                           0.89     72695\n",
            "   macro avg       0.88      0.88      0.88     72695\n",
            "weighted avg       0.89      0.89      0.89     72695\n",
            "\n",
            "[[16471  1578  2026]\n",
            " [  624 20633   643]\n",
            " [ 2401  1055 27264]]\n",
            "News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.00      0.00      0.00        11\n",
            "           2       0.58      1.00      0.74        21\n",
            "\n",
            "    accuracy                           0.58        36\n",
            "   macro avg       0.19      0.33      0.25        36\n",
            "weighted avg       0.34      0.58      0.43        36\n",
            "\n",
            "[[ 0  0  4]\n",
            " [ 0  0 11]\n",
            " [ 0  0 21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WAIT TIME FOR COMPILING IS ALMOST 2 HOURS**"
      ],
      "metadata": {
        "id": "Ucvv3z7uELdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/5\n",
        "\n",
        "4544/4544 - 1295s - 285ms/step - accuracy: 0.8476 - loss: 0.4395 - val_accuracy: 0.8785 - val_loss: 0.3745\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "4544/4544 - 1354s - 298ms/step - accuracy: 0.8808 - loss: 0.3646 - val_accuracy: 0.8867 - val_loss: 0.3514\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "4544/4544 - 1314s - 289ms/step - accuracy: 0.8887 - loss: 0.3425 - val_accuracy: 0.8873 - val_loss: 0.3500\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "4544/4544 - 1305s - 287ms/step - accuracy: 0.8927 - loss: 0.3280 - val_accuracy: 0.8895 - val_loss: 0.3490\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "4544/4544 - 1299s - 286ms/step - accuracy: 0.8956 - loss: 0.3159 - val_accuracy: 0.8879 - val_loss: 0.3549\n",
        "\n",
        "Epoch 1/5\n",
        "\n",
        "3/3 - 8s - 3s/step - accuracy: 0.3803 - loss: 1.0958 - val_accuracy: 0.5833 - val_loss: 1.0691\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "3/3 - 1s - 377ms/step - accuracy: 0.5845 - loss: 1.0560 - val_accuracy: 0.5833 - val_loss: 1.0241\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "3/3 - 1s - 254ms/step - accuracy: 0.5845 - loss: 0.9970 - val_accuracy: 0.5833 - val_loss: 0.9730\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "3/3 - 1s - 395ms/step - accuracy: 0.5845 - loss: 0.9573 - val_accuracy: 0.5833 - val_loss: 0.9622\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "3/3 - 1s - 434ms/step - accuracy: 0.5845 - loss: 0.9171 - val_accuracy: 0.5833 - val_loss: 0.9695\n",
        "\n",
        "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ab5ee69300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
        "\n",
        "2272/2272 ━━━━━━━━━━━━━━━━━━━━ 109s 48ms/step\n",
        "\n",
        "2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 470ms/step\n",
        "\n",
        "Tweets Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.81      0.83     20075\n",
        "           1       0.89      0.94      0.91     21900\n",
        "           2       0.90      0.90      0.90     30720\n",
        "\n",
        "    accuracy                           0.89     72695\n",
        "\n",
        "   macro avg       0.88      0.88      0.88     72695\n",
        "\n",
        "weighted avg       0.89      0.89      0.89     72695\n",
        "\n",
        "\n",
        "[[16164  1598  2313]\n",
        "\n",
        " [  571 20637   692]\n",
        "\n",
        " [ 1979   999 27742]]\n",
        "\n",
        "News Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.00      0.00      0.00         4\n",
        "           1       0.00      0.00      0.00        11\n",
        "           2       0.58      1.00      0.74        21\n",
        "\n",
        "    accuracy                           0.58        36\n",
        "   macro avg       0.19      0.33      0.25        36\n",
        "\n",
        "weighted avg       0.34      0.58      0.43        36\n",
        "\n",
        "[[ 0  0  4]\n",
        " [ 0  0 11]\n",
        " [ 0  0 21]]\n"
      ],
      "metadata": {
        "id": "tlKzZwb_EP1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using AdaBoost for Sentiment Analysis - TSLA"
      ],
      "metadata": {
        "id": "stCczchf5xJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Data\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_tsla_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_tsla_news.csv\")\n",
        "\n",
        "# Replace -1 with 0 (assuming -1 represents negative sentiment)\n",
        "tweets_df['sentiment'] = tweets_df['sentiment'].replace(-1, 0)\n",
        "news_df['sentiment'] = news_df['sentiment'].replace(-1, 0)\n",
        "\n",
        "# Convert text into TF-IDF vectors for tweets\n",
        "vectorizer_tweets = TfidfVectorizer(max_features=5000)\n",
        "X_tweets = vectorizer_tweets.fit_transform(tweets_df['cleaned_tweet'].fillna('')).toarray()\n",
        "y_tweets = tweets_df['sentiment']\n",
        "\n",
        "# Convert text into TF-IDF vectors for news\n",
        "vectorizer_news = TfidfVectorizer(max_features=5000)\n",
        "X_news = vectorizer_news.fit_transform(news_df['text'].fillna('')).toarray()\n",
        "y_news = news_df['sentiment']\n",
        "\n",
        "#  Reduce Data Size (if crashing persists)\n",
        "X_tweets, y_tweets = X_tweets[:20000], y_tweets[:20000]  # Limit to 10,000 samples\n",
        "\n",
        "\n",
        "# Train-test split for tweets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train-test split for news\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# AdaBoost Classifier with Decision Tree as base learner\n",
        "adaboost_model_tweets = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.5, random_state=42)\n",
        "adaboost_model_news = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.5, random_state=42)\n",
        "\n",
        "# Train models\n",
        "adaboost_model_tweets.fit(X_train_tweets, y_train_tweets)\n",
        "adaboost_model_news.fit(X_train_news, y_train_news)\n",
        "\n",
        "# Predictions\n",
        "y_pred_tweets = adaboost_model_tweets.predict(X_test_tweets)\n",
        "y_pred_news = adaboost_model_news.predict(X_test_news)\n",
        "\n",
        "# Evaluation for tweets\n",
        "print(\"AdaBoost Sentiment Analysis Performance (Tweets):\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(\"Accuracy:\", accuracy_score(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "# Evaluation for news\n",
        "print(\"AdaBoost Sentiment Analysis Performance (News):\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(\"Accuracy:\", accuracy_score(y_test_news, y_pred_news))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXtg74Jg5SrO",
        "outputId": "79f7a32b-f201-4566-bb14-7fab9149aa0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Sentiment Analysis Performance (Tweets):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.98      0.74      2265\n",
            "           1       0.84      0.15      0.25      1735\n",
            "\n",
            "    accuracy                           0.62      4000\n",
            "   macro avg       0.72      0.56      0.50      4000\n",
            "weighted avg       0.70      0.62      0.53      4000\n",
            "\n",
            "Accuracy: 0.61825\n",
            "AdaBoost Sentiment Analysis Performance (News):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.07      0.12        15\n",
            "           1       0.60      1.00      0.75        21\n",
            "\n",
            "    accuracy                           0.61        36\n",
            "   macro avg       0.80      0.53      0.44        36\n",
            "weighted avg       0.77      0.61      0.49        36\n",
            "\n",
            "Accuracy: 0.6111111111111112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZ_x93P_CL0"
      },
      "source": [
        "# Merging - Microsoft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOOOtCrJ_Erw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "stock_data = pd.read_csv(\"/content/msft_2019.csv\")\n",
        "tweets_sentiment = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_sentiment = pd.read_csv(\"/content/sentiment_msft_news.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3meXu3Bn_I4W"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to datetime\n",
        "stock_data['day_date'] = pd.to_datetime(stock_data['day_date'])\n",
        "tweets_sentiment['date'] = pd.to_datetime(tweets_sentiment['date'])\n",
        "news_sentiment['date'] = pd.to_datetime(news_sentiment['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kEJIXf7_nlC"
      },
      "outputs": [],
      "source": [
        "# Extract Month-Day format (MM-DD)\n",
        "stock_data['month_day'] = stock_data['day_date'].dt.strftime('%m-%d')\n",
        "tweets_sentiment['month_day'] = tweets_sentiment['date'].dt.strftime('%m-%d')\n",
        "news_sentiment['month_day'] = news_sentiment['date'].dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu4pkD6WMPd0"
      },
      "outputs": [],
      "source": [
        "# # Drop original 'date' columns (Optional)\n",
        "# stock_data.drop(columns=['day_date'], inplace=True)\n",
        "# tweets_sentiment.drop(columns=['date'], inplace=True)\n",
        "# news_sentiment.drop(columns=['date'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGlNKkuHW9uR"
      },
      "outputs": [],
      "source": [
        "# Stock & Tweets\n",
        "stock_tweet_merged = stock_data.merge(tweets_sentiment, on='month_day', how='left')\n",
        "stock_tweet_merged.fillna(0, inplace=True)\n",
        "stock_tweet_merged = stock_tweet_merged.rename(columns={'sentiment_vader': 'tweet_sentiment'})\n",
        "stock_tweet_merged.to_csv(\"Merge_MSFT_TWEET.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn7ufmAZL28I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6866a6-cb30-461d-a97d-f97b80447432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Merging done! Two separate files created: one for Tweets, one for News.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-225be7571668>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
            "  stock_news_merged.fillna(0, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Stock & News\n",
        "stock_news_merged = stock_data.merge(news_sentiment, on='month_day', how='left')\n",
        "stock_news_merged.fillna(0, inplace=True)\n",
        "stock_news_merged = stock_news_merged.rename(columns={'sentiment': 'news_sentiment'})\n",
        "stock_news_merged.to_csv(\"Merge_MSFT_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Merging done! Two separate files created: one for Tweets, one for News.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GdhrQULVY5w"
      },
      "source": [
        "# Merging - Tesla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQzd4PPiMgjx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets (assuming they are already preprocessed)\n",
        "stock_data = pd.read_csv(\"/content/tsla_2019.csv\")\n",
        "tweets_sentiment = pd.read_csv(\"/content/sentiment_tsla_tweets.csv\")\n",
        "news_sentiment = pd.read_csv(\"/content/sentiment_tsla_news.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t4Q5R54VjSf"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to datetime\n",
        "stock_data['day_date'] = pd.to_datetime(stock_data['day_date'])\n",
        "tweets_sentiment['date'] = pd.to_datetime(tweets_sentiment['date'])\n",
        "news_sentiment['date'] = pd.to_datetime(news_sentiment['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVJFCM22Vk2l"
      },
      "outputs": [],
      "source": [
        "# Extract Month-Day format (MM-DD)\n",
        "stock_data['month_day'] = stock_data['day_date'].dt.strftime('%m-%d')\n",
        "tweets_sentiment['month_day'] = tweets_sentiment['date'].dt.strftime('%m-%d')\n",
        "news_sentiment['month_day'] = news_sentiment['date'].dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock & Tweets\n",
        "stock_tweet_merged = stock_data.merge(tweets_sentiment, on='month_day', how='left')\n",
        "stock_tweet_merged.fillna(0, inplace=True)\n",
        "stock_tweet_merged = stock_tweet_merged.rename(columns={'sentiment_vader': 'tweet_sentiment'})\n",
        "stock_tweet_merged.to_csv(\"Merge_TSLA_TWEET.csv\", index=False)"
      ],
      "metadata": {
        "id": "3HuglepuUq9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock & News\n",
        "stock_news_merged = stock_data.merge(news_sentiment, on='month_day', how='left')\n",
        "stock_news_merged.fillna(0, inplace=True)\n",
        "stock_news_merged = stock_news_merged.rename(columns={'sentiment': 'news_sentiment'})\n",
        "stock_news_merged.to_csv(\"Merge_TSLA_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Merging done! Two separate files created: one for Tweets, one for News.\")"
      ],
      "metadata": {
        "id": "Ut3k4nQaUuRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "974e70db-b46a-4219-d4bc-a7e1435d5efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Merging done! Two separate files created: one for Tweets, one for News.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-df03a4931140>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
            "  stock_news_merged.fillna(0, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpHk36XNrwZL"
      },
      "source": [
        "# Stock Data Analysis (Microsoft)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "stock_tweet = pd.read_csv(\"/content/Merge_MSFT_TWEET.csv\")\n",
        "stock_news = pd.read_csv(\"/content/Merge_MSFT_NEWS.csv\")\n",
        "\n",
        "# Convert 'month_day' to datetime format\n",
        "stock_tweet['month_day'] = pd.to_datetime(stock_tweet['month_day'], format='%m-%d')\n",
        "stock_news['month_day'] = pd.to_datetime(stock_news['month_day'], format='%m-%d')\n",
        "\n",
        "# Sort data chronologically\n",
        "stock_tweet = stock_tweet.sort_values('month_day')\n",
        "stock_news = stock_news.sort_values('month_day')\n",
        "\n",
        "# Feature Engineering\n",
        "for df in [stock_tweet, stock_news]:\n",
        "    df['5_day_avg'] = df['close_value'].rolling(window=5).mean()\n",
        "    df['10_day_avg'] = df['close_value'].rolling(window=10).mean()\n",
        "    df['volatility'] = df['close_value'].pct_change().rolling(window=5).std()\n",
        "    df.fillna(0, inplace=True)  # Fill missing values\n",
        "\n",
        "# Save new files\n",
        "stock_tweet.to_csv(\"MSFT_FEATURED_TWEET.csv\", index=False)\n",
        "stock_news.to_csv(\"MSFT_FEATURED_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Feature Engineering Completed for Both Files!\")\n"
      ],
      "metadata": {
        "id": "u-udlLdQ0j5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420c2787-58ff-4447-e9be-e5bc29dce972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature Engineering Completed for Both Files!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Load Tweet Data\n",
        "tweet_data = pd.read_csv(\"/content/MSFT_FEATURED_TWEET.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(tweet_data[['close_value', 'sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X, y = [], []\n",
        "for i in range(30, len(scaled_data)):\n",
        "    X.append(scaled_data[i-30:i])\n",
        "    y.append(scaled_data[i, 0])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_tweet = Sequential()\n",
        "model_tweet.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(LSTM(units=50))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(Dense(units=1))\n",
        "\n",
        "model_tweet.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_tweet.fit(X, y, epochs=10, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_tweet.save(\"LSTM_MSFT_TWEET.h5\")\n",
        "\n",
        "print(\" LSTM Model Trained for Tweet Sentiment!\")\n"
      ],
      "metadata": {
        "id": "Jagdrk5rWK8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359bce56-c79f-4697-b144-23dc6d8f05b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 32ms/step - loss: 0.0074\n",
            "Epoch 2/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 33ms/step - loss: 0.0011\n",
            "Epoch 3/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 32ms/step - loss: 6.0460e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 33ms/step - loss: 4.7164e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 31ms/step - loss: 4.3132e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 33ms/step - loss: 4.1755e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 31ms/step - loss: 4.1286e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 33ms/step - loss: 4.0734e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 31ms/step - loss: 4.0394e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 4.0733e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LSTM Model Trained for Tweet Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for above compiling is almost 15 mins or so**"
      ],
      "metadata": {
        "id": "Yrcf2iKWFvt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load News Data\n",
        "news_data = pd.read_csv(\"/content/MSFT_FEATURED_NEWS.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaled_news_data = scaler.fit_transform(news_data[['close_value', 'news_sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X_news, y_news = [], []\n",
        "for i in range(30, len(scaled_news_data)):\n",
        "    X_news.append(scaled_news_data[i-30:i])\n",
        "    y_news.append(scaled_news_data[i, 0])\n",
        "\n",
        "X_news, y_news = np.array(X_news), np.array(y_news)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_news = Sequential()\n",
        "model_news.add(LSTM(units=50, return_sequences=True, input_shape=(X_news.shape[1], X_news.shape[2])))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(LSTM(units=50))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(Dense(units=1))\n",
        "\n",
        "model_news.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_news.fit(X_news, y_news, epochs=20, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_news.save(\"LSTM_MSFT_NEWS.h5\")\n",
        "\n",
        "print(\"✅ LSTM Model Trained for News Sentiment!\")\n"
      ],
      "metadata": {
        "id": "lmGhYB19Hsdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c8f04f-4052-45aa-9e11-ebf8625364fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0896\n",
            "Epoch 2/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0204\n",
            "Epoch 3/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0098\n",
            "Epoch 4/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0076\n",
            "Epoch 5/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0061\n",
            "Epoch 6/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0047\n",
            "Epoch 7/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0058\n",
            "Epoch 8/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0052\n",
            "Epoch 9/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0046\n",
            "Epoch 10/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0044\n",
            "Epoch 11/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0055\n",
            "Epoch 12/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0038\n",
            "Epoch 13/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0045\n",
            "Epoch 14/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0036\n",
            "Epoch 15/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0047\n",
            "Epoch 16/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0037\n",
            "Epoch 17/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0041\n",
            "Epoch 18/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0040\n",
            "Epoch 19/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0039\n",
            "Epoch 20/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LSTM Model Trained for News Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Predictions\n",
        "tweet_pred = model_tweet.predict(X)\n",
        "news_pred = model_news.predict(X_news)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    print(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "evaluate_model(y, tweet_pred, \"LSTM Tweet Model\")\n",
        "evaluate_model(y_news, news_pred, \"LSTM News Model\")\n",
        "# y_pred = model_tweet.predict(X_test)  # Ensure model exists and has been trained"
      ],
      "metadata": {
        "id": "fHmV6i3TMtch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15ff767-1fc7-4042-d536-b58d893c4455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
            "LSTM Tweet Model - MAE: 0.0111, RMSE: 0.0145\n",
            "LSTM News Model - MAE: 0.0279, RMSE: 0.0346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Data Analysis (Tesla)"
      ],
      "metadata": {
        "id": "W6lUyopWDhaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load both datasets\n",
        "stock_tweet = pd.read_csv(\"/content/Merge_TSLA_TWEET.csv\")\n",
        "stock_news = pd.read_csv(\"/content/Merge_TSLA_NEWS.csv\")\n",
        "\n",
        "# Convert 'month_day' to datetime format\n",
        "stock_tweet['month_day'] = pd.to_datetime(stock_tweet['month_day'], format='%m-%d')\n",
        "stock_news['month_day'] = pd.to_datetime(stock_news['month_day'], format='%m-%d')\n",
        "\n",
        "# Sort data chronologically\n",
        "stock_tweet = stock_tweet.sort_values('month_day')\n",
        "stock_news = stock_news.sort_values('month_day')\n",
        "\n",
        "# Feature Engineering\n",
        "for df in [stock_tweet, stock_news]:\n",
        "    df['5_day_avg'] = df['close_value'].rolling(window=5).mean()\n",
        "    df['10_day_avg'] = df['close_value'].rolling(window=10).mean()\n",
        "    df['volatility'] = df['close_value'].pct_change().rolling(window=5).std()\n",
        "    df.fillna(0, inplace=True)  # Fill missing values\n",
        "\n",
        "# Save new files\n",
        "stock_tweet.to_csv(\"TSLA_FEATURED_TWEET.csv\", index=False)\n",
        "stock_news.to_csv(\"TSLA_FEATURED_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Feature Engineering Completed for Both Files!\")\n"
      ],
      "metadata": {
        "id": "NUqsjBYBPrdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6cb6801-61ae-4536-e57c-69180ec639d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature Engineering Completed for Both Files!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Load Tweet Data\n",
        "tweet_data = pd.read_csv(\"/content/TSLA_FEATURED_TWEET.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(tweet_data[['close_value', 'sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X, y = [], []\n",
        "for i in range(30, len(scaled_data)):\n",
        "    X.append(scaled_data[i-30:i])\n",
        "    y.append(scaled_data[i, 0])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_tweet = Sequential()\n",
        "model_tweet.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(LSTM(units=50))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(Dense(units=1))\n",
        "\n",
        "model_tweet.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_tweet.fit(X, y, epochs=7, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_tweet.save(\"LSTM_TSLA_TWEET.h5\")\n",
        "\n",
        "print(\" LSTM Model Trained for Tweet Sentiment!\")\n"
      ],
      "metadata": {
        "id": "Zs8reiH1Dooh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd97d192-7988-40ca-97dc-19a6d90b91bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 33ms/step - loss: 0.0015\n",
            "Epoch 2/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 32ms/step - loss: 3.1537e-04\n",
            "Epoch 3/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 33ms/step - loss: 3.0349e-04\n",
            "Epoch 4/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 32ms/step - loss: 2.9387e-04\n",
            "Epoch 5/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 32ms/step - loss: 2.8767e-04\n",
            "Epoch 6/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 32ms/step - loss: 2.8335e-04\n",
            "Epoch 7/7\n",
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 33ms/step - loss: 2.8066e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LSTM Model Trained for Tweet Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for above compiling is almost 45 mins**"
      ],
      "metadata": {
        "id": "ecSFBf8uqZH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load News Data\n",
        "news_data = pd.read_csv(\"/content/TSLA_FEATURED_NEWS.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaled_news_data = scaler.fit_transform(news_data[['close_value', 'news_sentiment', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X_news, y_news = [], []\n",
        "for i in range(30, len(scaled_news_data)):\n",
        "    X_news.append(scaled_news_data[i-30:i])\n",
        "    y_news.append(scaled_news_data[i, 0])\n",
        "\n",
        "X_news, y_news = np.array(X_news), np.array(y_news)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_news = Sequential()\n",
        "model_news.add(LSTM(units=50, return_sequences=True, input_shape=(X_news.shape[1], X_news.shape[2])))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(LSTM(units=50))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(Dense(units=1))\n",
        "\n",
        "model_news.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_news.fit(X_news, y_news, epochs=25, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_news.save(\"LSTM_TSLA_NEWS.h5\")\n",
        "\n",
        "print(\"✅ LSTM Model Trained for News Sentiment!\")\n"
      ],
      "metadata": {
        "id": "G2HAp1_UDtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4fb170-0429-4706-9a7d-e4b9a1ac5f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - loss: 0.0930\n",
            "Epoch 2/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0241\n",
            "Epoch 3/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0096\n",
            "Epoch 4/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0088\n",
            "Epoch 5/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0069\n",
            "Epoch 6/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0060\n",
            "Epoch 7/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0060\n",
            "Epoch 8/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0052\n",
            "Epoch 9/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0059\n",
            "Epoch 10/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0059\n",
            "Epoch 11/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0048\n",
            "Epoch 12/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0064\n",
            "Epoch 13/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0060\n",
            "Epoch 14/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0055\n",
            "Epoch 15/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0055\n",
            "Epoch 16/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0049\n",
            "Epoch 17/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0044\n",
            "Epoch 18/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0045\n",
            "Epoch 19/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0041\n",
            "Epoch 20/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0045\n",
            "Epoch 21/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0044\n",
            "Epoch 22/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0047\n",
            "Epoch 23/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0042\n",
            "Epoch 24/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0042\n",
            "Epoch 25/25\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LSTM Model Trained for News Sentiment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Predictions\n",
        "tweet_pred = model_tweet.predict(X)\n",
        "news_pred = model_news.predict(X_news)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    print(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "evaluate_model(y, tweet_pred, \"LSTM Tweet Model\")\n",
        "evaluate_model(y_news, news_pred, \"LSTM News Model\")\n",
        "# y_pred = model_tweet.predict(X_test)  # Ensure model exists and has been trained"
      ],
      "metadata": {
        "id": "Fvt0-ZeQDuQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbf0b25-7a1f-4097-b90d-9b5b669f796b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11358/11358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 10ms/step\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
            "LSTM Tweet Model - MAE: 0.0020, RMSE: 0.0028\n",
            "LSTM News Model - MAE: 0.0345, RMSE: 0.0475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "VWn9OHdNEFHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# # Load sentiment datasets\n",
        "# tweets_df = pd.read_csv(\"/content/Merge_MSFT_TWEET.csv\")\n",
        "# news_df = pd.read_csv(\"/content/Merge_MSFT_NEWS.csv\")\n",
        "\n",
        "# # 1️⃣ PIE CHART: Proportion of Sentiments\n",
        "# plt.figure(figsize=(12, 5))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# tweets_df['sentiment'].value_counts().plot.pie(autopct='%1.1f%%', colors=['red', 'blue', 'green'])\n",
        "# plt.title(\"Tweet Sentiment Distribution\")\n",
        "# plt.ylabel(\"\")\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# news_df['news_sentiment'].value_counts().plot.pie(autopct='%1.1f%%', colors=['red', 'blue', 'green'])\n",
        "# plt.title(\"News Sentiment Distribution\")\n",
        "# plt.ylabel(\"\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# # # 3️⃣ CONFUSION MATRIX: LSTM vs ANN\n",
        "# # lstm_y_true = [...]  # Replace with actual y_test labels\n",
        "# # lstm_y_pred = [...]  # Replace with LSTM predictions\n",
        "# # ann_y_true = [...]  # Replace with actual y_test labels\n",
        "# # ann_y_pred = [...]  # Replace with ANN predictions\n",
        "\n",
        "# # fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# # for ax, y_true, y_pred, title in zip(axes, [lstm_y_true, ann_y_true], [lstm_y_pred, ann_y_pred], [\"LSTM Model\", \"ANN Model\"]):\n",
        "# #     cm = confusion_matrix(y_true, y_pred)\n",
        "# #     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "# #     disp.plot(ax=ax, cmap='Blues')\n",
        "# #     ax.set_title(title)\n",
        "# # plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n8vFXfVpldeV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "0ff176dc-94f3-4b2a-9599-04ba9082b16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2️⃣ LINE PLOT: Stock Price vs. Sentiment\n",
        "# stock_df = pd.read_csv(\"/content/Merge_MSFT_TWEET.csv\")  # Ensure dataset has 'date', 'sentiment_score', 'stock_price'\n",
        "# stock_df['date'] = pd.to_datetime(stock_df['date'])\n",
        "\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.lineplot(x=stock_df['date'], y=stock_df['sentiment'], label=\"Daily Sentiment Score\", color='blue')\n",
        "# sns.lineplot(x=stock_df['date'], y=stock_df['low_value'], label=\"Stock Price\", color='green')\n",
        "# plt.xticks(rotation=45)\n",
        "# # plt.title(\"Stock Price vs. Sentiment Over Time\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "qQi_b_1NVUxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 4️⃣ BAR CHART: Accuracy of Models\n",
        "# models = ['Naïve Bayes', 'SVM', 'Decision Tree', 'Random Forest', 'ANN', 'LSTM']\n",
        "# accuracy_scores = [0.7532, 0.8911, 0.8110, 0.8534, 0.88, 0.90]  # Replace with actual accuracy values\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.barplot(x=models, y=accuracy_scores, palette='viridis')\n",
        "# plt.ylim(0, 1)\n",
        "# plt.title(\"Model Accuracy Comparison\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "qKv72ajxV7g5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "af39558e-e991-4266-a7f6-63ab2eedf236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-093bbc228084>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracy_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.7532\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8911\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8534\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.88\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Replace with actual accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JU-u56vL4Elz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "collapsed_sections": [
        "RSSbeztl4SuC",
        "rtIxEm3w4aUw",
        "v7oM2YYybjpJ",
        "l1SpoBmabqms",
        "n-WWdulg7vQ_",
        "mmI7toQa7yyK",
        "bwsJ668S--1y",
        "8z3_YOhb2Zj-",
        "YVbBVat5FBW-",
        "1Mz0epRlEoob",
        "x1ysblnS2won",
        "DRJBd8JSFRzg",
        "hzfKyY0Q2qBJ",
        "RwZ_x93P_CL0",
        "6GdhrQULVY5w",
        "lpHk36XNrwZL",
        "W6lUyopWDhaD",
        "VWn9OHdNEFHb"
      ],
      "mount_file_id": "11AmYuY6p0-2vnU8ZxORnVG2nVYHtb-Nj",
      "authorship_tag": "ABX9TyNvpomxzHyOcctF3vDdtpyx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}