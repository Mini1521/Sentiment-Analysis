{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mini1521/Sentiment-Analysis/blob/main/Using_Microsoft_and_Tesla_Dataset_from_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSSbeztl4SuC"
      },
      "source": [
        "# Stock Data- preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8JXK8qKp3rFC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f3b9a6dd-b1ca-4087-80d6-b7e55719d3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged CSV saved as 'merged_data.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the two CSV files\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Company.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/CompanyValues.csv\")\n",
        "\n",
        "# Merge the data on a common column (e.g., 'day_date')\n",
        "merged_df = pd.merge(df1, df2, on=\"ticker_symbol\", how=\"left\")  # Use \"outer\", \"left\", or \"right\" if needed\n",
        "\n",
        "# Save the merged dataset to a new CSV file\n",
        "merged_df.to_csv(\"merged_data.csv\", index=False)\n",
        "\n",
        "print(\"Merged CSV saved as 'merged_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Egmn_3Ea2U1x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ec473b09-fd4c-46c8-9b8a-ef3e403a2a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete.\n",
            "Saved: 'msft_2019.csv' and 'tsla_2019.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (update the filename if needed)\n",
        "df = pd.read_csv('/content/merged_data.csv')\n",
        "\n",
        "df.rename(columns={'post_date': 'date'}, inplace=True)\n",
        "\n",
        "# Convert 'day_date' to datetime format\n",
        "df['day_date'] = pd.to_datetime(df['day_date'], errors='coerce')\n",
        "\n",
        "# Filter data for the year 2020\n",
        "df_2019 = df[df['day_date'].dt.year == 2019]\n",
        "df_2019 = df_2019.sort_values(by='day_date', ascending=True)\n",
        "\n",
        "# Separate data for Microsoft (MSFT) and Tesla (TSLA)\n",
        "msft_data = df_2019[df_2019['ticker_symbol'] == 'MSFT']\n",
        "tsla_data = df_2019[df_2019['ticker_symbol'] == 'TSLA']\n",
        "\n",
        "# Drop rows with missing values\n",
        "msft_data = msft_data.dropna()\n",
        "tsla_data = tsla_data.dropna()\n",
        "\n",
        "\n",
        "# print(\" Stock Data Date Range:\")\n",
        "# print(f\"Start Date: {msft_data['day_date'].min()} | End Date: {msft_data['day_date'].max()}\")\n",
        "# print(f\"Start Date: {tsla_data['day_date'].min()} | End Date: {tsla_data['day_date'].max()}\")\n",
        "\n",
        "# Save each dataset separately\n",
        "msft_data.to_csv(\"msft_2019.csv\", index=False)\n",
        "tsla_data.to_csv(\"tsla_2019.csv\", index=False)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "print(\"Saved: 'msft_2019.csv' and 'tsla_2019.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtIxEm3w4aUw"
      },
      "source": [
        "# Social Media - preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3xXgbcP14Fe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "company_tweets = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Company_Tweet.csv\")\n",
        "tweets = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/Tweet.csv\")\n",
        "\n",
        "# Merge tweets with company information\n",
        "tweets = tweets.merge(company_tweets, how='left', on='tweet_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "S8kNuI0l5lL7"
      },
      "outputs": [],
      "source": [
        "# Convert 'post_date' to datetime format\n",
        "tweets['date'] = pd.to_datetime(tweets['post_date'], unit='s').dt.date\n",
        "tweets['date'] = pd.to_datetime(tweets['date'], errors='coerce')\n",
        "# tweets['time'] = pd.to_datetime(tweets['post_date'], unit='s').dt.time\n",
        "\n",
        "tweets.to_csv(\"merged_tweets.csv\", index=False)\n",
        "\n",
        "tweets.drop(columns=['comment_num', 'retweet_num', 'like_num'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rs-K5Kp-5p_e"
      },
      "outputs": [],
      "source": [
        "# Filter for Microsoft (MSFT) and Tesla (TSLA)\n",
        "tweets_filtered = tweets[tweets['ticker_symbol'].isin(['MSFT', 'TSLA'])]\n",
        "\n",
        "# Keep only tweets from 2019\n",
        "tweets_filtered = tweets_filtered[tweets_filtered['date'].dt.year == 2019]\n",
        "\n",
        "# Split into separate datasets\n",
        "msft_tweets = tweets_filtered[tweets_filtered['ticker_symbol'] == 'MSFT']\n",
        "tsla_tweets = tweets_filtered[tweets_filtered['ticker_symbol'] == 'TSLA']\n",
        "\n",
        "# Save to CSV files\n",
        "msft_tweets.to_csv(\"msft_tweets_2019.csv\", index=False)\n",
        "tsla_tweets.to_csv(\"tsla_tweets_2019.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning of microsoft tweets"
      ],
      "metadata": {
        "id": "v7oM2YYybjpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VajmTQ2t5x6T",
        "outputId": "122faf10-35fa-4964-a07f-29588ef3123c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load Twitter dataset\n",
        "tweets_df = pd.read_csv(\"/content/msft_tweets_2019.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-Gn3mF5f7Ho2"
      },
      "outputs": [],
      "source": [
        "# rename columns\n",
        "tweets_df.rename(columns={'body': 'Tweet'}, inplace=True)\n",
        "tweets_df.rename(columns={'post_date': 'day_date'}, inplace=True)\n",
        "\n",
        "# Droping Duplicates\n",
        "tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "\n",
        "# Handle Missing Values\n",
        "tweets_df.dropna(subset=['Tweet'], inplace=True)  # Drop rows where 'Tweet' is empty\n",
        "\n",
        "# Ensure 'date' is in datetime format\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce')\n",
        "\n",
        "tweets_df = tweets_df.sort_values(by='date', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "gjEXLpIW7NTx"
      },
      "outputs": [],
      "source": [
        "# Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "agU-7Vpx7OuU"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning function\n",
        "tweets_df['cleaned_tweet'] = tweets_df['Tweet'].astype(str).apply(clean_text)\n",
        "\n",
        "#Saving the cleaned data for further analysis\n",
        "tweets_df.to_csv(\"cleaned_msft_tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning of tesla tweets"
      ],
      "metadata": {
        "id": "l1SpoBmabqms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7NXDc8W87WJ-"
      },
      "outputs": [],
      "source": [
        "# Load Twitter dataset\n",
        "tweets_df = pd.read_csv(\"/content/tsla_tweets_2019.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GNtXLEMH7fOq"
      },
      "outputs": [],
      "source": [
        "# rename columns\n",
        "tweets_df.rename(columns={'body': 'Tweet'}, inplace=True)\n",
        "tweets_df.rename(columns={'post_date': 'day_date'}, inplace=True)\n",
        "\n",
        "# Droping Duplicates\n",
        "tweets_df.drop_duplicates(subset=['Tweet'], inplace=True)\n",
        "\n",
        "# Handle Missing Values\n",
        "tweets_df.dropna(subset=['Tweet'], inplace=True)  # Drop rows where 'Tweet' is empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "gHP_1vVj2_rZ"
      },
      "outputs": [],
      "source": [
        "# Ensure 'date' is in datetime format\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce')\n",
        "\n",
        "tweets_df = tweets_df.sort_values(by='date', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uTsoOO7G7kQB"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "L9RWBI4B7l87"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning function\n",
        "tweets_df['cleaned_tweet'] = tweets_df['Tweet'].astype(str).apply(clean_text)\n",
        "\n",
        "#Saving the cleaned data for further analysis\n",
        "tweets_df.to_csv(\"cleaned_tsla_tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-WWdulg7vQ_"
      },
      "source": [
        "# Microsoft Articles - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "73juevOP7sGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "34fb2e9a-e441-4cc0-8472-1630e8fd83e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset (Update the filename if needed)\n",
        "article = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/msft_articles.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Al3Cv7Pc8DM7"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "article['date'] = pd.to_datetime(article['date'], errors='coerce')\n",
        "\n",
        "# Filter for only the year 2020\n",
        "year_2020 = article[article['date'].dt.year == 2020]\n",
        "year_2020 = year_2020.sort_values(by='date', ascending=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "year_2020 = year_2020.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Rl597yZM8EZ0"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "3GKTyi-w8KAO"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the 'text' column\n",
        "year_2020['text'] = year_2020['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "year_2020.to_csv(\"cleaned_msft_articles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmI7toQa7yyK"
      },
      "source": [
        "# Tesla Articles - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6AAa59TO72XZ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset (Update the filename if needed)\n",
        "article = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/OG - Datasets/tsla_articles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9X4JjhFH8iJo"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "article['date'] = pd.to_datetime(article['date'], errors='coerce')\n",
        "\n",
        "# Filter for only the year 2020\n",
        "year_2020 = article[article['date'].dt.year == 2020]\n",
        "year_2020 = year_2020.sort_values(by='date', ascending=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "year_2020 = year_2020.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "tiV_Rxro8jjZ"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "XpPxvRcX8kzb"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the 'text' column\n",
        "year_2020['text'] = year_2020['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "year_2020.to_csv(\"cleaned_tsla_articles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwsJ668S--1y"
      },
      "source": [
        "# Sentiment Analysis- microsoft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "_A_K5_sYf9f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eae13b25-ccf5-489c-804c-a7a2fc025f13"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "q1LqstO5Wd20"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load datasets\n",
        "tweets_df = pd.read_csv(\"/content/cleaned_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/cleaned_msft_articles.csv\")\n",
        "\n",
        "# Ensure text column is string type\n",
        "tweets_df['Tweet'] = tweets_df['Tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "p-mrbgZ3WnP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8d832ee6-45b4-477a-f6b7-1b0f50bcea56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# VADER Sentiment Analysis for Tweets\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment score from VADER\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply VADER sentiment analysis to tweets\n",
        "tweets_df['sentiment'] = tweets_df['Tweet'].apply(get_vader_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "M_kjNMV7Ww87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "38a2ef88-8eb4-4da0-8bb2-57f8a7947529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# TextBlob Sentiment Analysis for News\n",
        "\n",
        "# Function to get sentiment score from TextBlob\n",
        "def get_textblob_sentiment(text):\n",
        "    score = TextBlob(text).sentiment.polarity\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply TextBlob sentiment analysis to news articles\n",
        "news_df['sentiment'] = news_df['text'].apply(get_textblob_sentiment)\n",
        "\n",
        "# Save sentiment-labeled datasets\n",
        "tweets_df.to_csv(\"sentiment_msft_tweets.csv\", index=False)\n",
        "news_df.to_csv(\"sentiment_msft_news.csv\", index=False)\n",
        "\n",
        "print(\"Sentiment analysis complete! Results saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Traditional ML methods for sentiment Analysis -MSFT"
      ],
      "metadata": {
        "id": "8z3_YOhb2Zj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(\n",
        "    tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(\n",
        "    news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Function to train models\n",
        "def train_model(X_train, y_train, X_test, y_test, model):\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    predictions = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Model {model.__class__.__name__} Accuracy: {accuracy:.4f}\")\n",
        "    return pipeline\n",
        "\n",
        "# Models to Train\n",
        "models = {\n",
        "    'Naïve Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(kernel='linear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "# Train models for Tweets\n",
        "print(\"\\n Training Models for Tweet Sentiment Classification \")\n",
        "trained_models_tweets = {\n",
        "    name: train_model(X_tweets, y_tweets, X_test_tweets, y_test_tweets, model)\n",
        "    for name, model in models.items()\n",
        "}\n",
        "\n",
        "# Train models for News\n",
        "print(\"\\n Training Models for News Sentiment Classification \")\n",
        "trained_models_news = {\n",
        "    name: train_model(X_news, y_news, X_test_news, y_test_news, model)\n",
        "    for name, model in models.items()\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojPQbUo1q3fz",
        "outputId": "3bfa44b8-0116-45cb-ea55-8ae4de376193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Models for Tweet Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.7531\n",
            "Model SVC Accuracy: 0.8916\n",
            "Model DecisionTreeClassifier Accuracy: 0.8017\n",
            "Model RandomForestClassifier Accuracy: 0.8540\n",
            "\n",
            " Training Models for News Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.6032\n",
            "Model SVC Accuracy: 0.6032\n",
            "Model DecisionTreeClassifier Accuracy: 0.4603\n",
            "Model RandomForestClassifier Accuracy: 0.5238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models for Tweet Sentiment Classification\n",
        "* Model MultinomialNB Accuracy: 0.7532\n",
        "* Model SVC Accuracy: 0.8911\n",
        "* Model DecisionTreeClassifier Accuracy: 0.8057\n",
        "* Model RandomForestClassifier Accuracy: 0.8523\n",
        "\n",
        "Training Models for News Sentiment Classification\n",
        "* Model MultinomialNB Accuracy: 0.6032\n",
        "* Model SVC Accuracy: 0.6032\n",
        "* Model DecisionTreeClassifier Accuracy: 0.4762\n",
        "* Model RandomForestClassifier Accuracy: 0.5397\n",
        "\n"
      ],
      "metadata": {
        "id": "tcT7bsxtgd40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ANN for Sentiment Analysis - MSFT"
      ],
      "metadata": {
        "id": "YVbBVat5FBW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_2EJzTzhwiaV",
        "outputId": "ac1a4f2a-8b18-41eb-af94-8b1520bdc2a3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tweets = vectorizer.fit_transform(X_tweets).toarray()\n",
        "X_test_tweets = vectorizer.transform(X_test_tweets).toarray()\n",
        "X_news = vectorizer.fit_transform(X_news).toarray()\n",
        "X_test_news = vectorizer.transform(X_test_news).toarray()\n",
        "\n",
        "# Build ANN model\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert Sentiment Labels into One-Hot Encoding\n",
        "y_tweets = to_categorical(y_tweets, num_classes=3)\n",
        "y_test_tweets = to_categorical(y_test_tweets, num_classes=3)\n",
        "y_news = to_categorical(y_news, num_classes=3)\n",
        "y_test_news = to_categorical(y_test_news, num_classes=3)\n",
        "\n",
        "# Train ANN model\n",
        "print(\"\\n Training ANN Model for Tweets Sentiment Classification \")\n",
        "ann_tweet_model = build_ann(X_tweets.shape[1])\n",
        "ann_tweet_model.fit(X_tweets, y_tweets, epochs=10, batch_size=32, validation_data=(X_test_tweets, y_test_tweets))\n",
        "\n",
        "# Train ANN for News\n",
        "print(\"\\n Training ANN Model for News Sentiment Classification \")\n",
        "ann_news_model = build_ann(X_news.shape[1])\n",
        "ann_news_model.fit(X_news, y_news, epochs=10, batch_size=32, validation_data=(X_test_news, y_test_news))\n",
        "\n",
        "\n",
        "y_test_tweets_labels = np.argmax(y_test_tweets, axis=1)\n",
        "y_pred_tweets = ann_tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)  # Convert probabilities to class labels\n",
        "print(\" ANN Tweet Sentiment Classification Report:\")   #  ANN Evaluation for Tweets\n",
        "print(classification_report(y_test_tweets_labels, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets_labels, y_pred_tweets))\n",
        "\n",
        "y_test_news_labels = np.argmax(y_test_news, axis=1)\n",
        "y_pred_news = ann_news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)  # Convert probabilities to class labels\n",
        "print(\"\\n ANN News Sentiment Classification Report:\")  # ANN Evaluation for News\n",
        "print(classification_report(y_test_news_labels, y_pred_news))\n",
        "print(confusion_matrix(y_test_news_labels, y_pred_news))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXBdwB_txv4b",
        "outputId": "75ab90d3-92d7-49dd-fba6-943a8efcb4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training ANN Model for Tweets Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.7269 - loss: 0.6509 - val_accuracy: 0.8769 - val_loss: 0.3592\n",
            "Epoch 2/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9083 - loss: 0.2782 - val_accuracy: 0.8830 - val_loss: 0.3544\n",
            "Epoch 3/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9339 - loss: 0.2039 - val_accuracy: 0.8882 - val_loss: 0.3550\n",
            "Epoch 4/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9519 - loss: 0.1470 - val_accuracy: 0.8893 - val_loss: 0.3825\n",
            "Epoch 5/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9679 - loss: 0.1012 - val_accuracy: 0.8890 - val_loss: 0.4364\n",
            "Epoch 6/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9799 - loss: 0.0661 - val_accuracy: 0.8871 - val_loss: 0.5024\n",
            "Epoch 7/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9855 - loss: 0.0451 - val_accuracy: 0.8837 - val_loss: 0.5734\n",
            "Epoch 8/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9889 - loss: 0.0352 - val_accuracy: 0.8849 - val_loss: 0.6329\n",
            "Epoch 9/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9898 - loss: 0.0320 - val_accuracy: 0.8857 - val_loss: 0.6820\n",
            "Epoch 10/10\n",
            "\u001b[1m1438/1438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9918 - loss: 0.0264 - val_accuracy: 0.8920 - val_loss: 0.7425\n",
            "\n",
            " Training ANN Model for News Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.4591 - loss: 1.0950 - val_accuracy: 0.5873 - val_loss: 1.0697\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5127 - loss: 1.0574 - val_accuracy: 0.5873 - val_loss: 1.0373\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5061 - loss: 1.0060 - val_accuracy: 0.5873 - val_loss: 1.0002\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5111 - loss: 0.9268 - val_accuracy: 0.5873 - val_loss: 0.9610\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4634 - loss: 0.8540 - val_accuracy: 0.5873 - val_loss: 0.9317\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5586 - loss: 0.7210 - val_accuracy: 0.5873 - val_loss: 0.9056\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8112 - loss: 0.5746 - val_accuracy: 0.6032 - val_loss: 0.8930\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9802 - loss: 0.4364 - val_accuracy: 0.6190 - val_loss: 0.8894\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.2714 - val_accuracy: 0.6032 - val_loss: 0.8901\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.1476 - val_accuracy: 0.5873 - val_loss: 0.9001\n",
            "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            " ANN Tweet Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.92      4818\n",
            "           1       0.90      0.92      0.91      5160\n",
            "           2       0.80      0.66      0.72      1522\n",
            "\n",
            "    accuracy                           0.89     11500\n",
            "   macro avg       0.87      0.84      0.85     11500\n",
            "weighted avg       0.89      0.89      0.89     11500\n",
            "\n",
            "[[4495  229   94]\n",
            " [ 247 4762  151]\n",
            " [ 236  285 1001]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\n",
            " ANN News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.35      0.44        17\n",
            "           1       0.65      0.81      0.72        37\n",
            "           2       0.14      0.11      0.12         9\n",
            "\n",
            "    accuracy                           0.59        63\n",
            "   macro avg       0.47      0.42      0.43        63\n",
            "weighted avg       0.57      0.59      0.56        63\n",
            "\n",
            "[[ 6  9  2]\n",
            " [ 3 30  4]\n",
            " [ 1  7  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM for Sentiment Analysis - MSFT"
      ],
      "metadata": {
        "id": "-yDBO8Uy2frr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed datasets for tweets and news\n",
        "tweets_df = pd.read_csv('/content/sentiment_msft_tweets.csv')\n",
        "news_df = pd.read_csv('/content/sentiment_msft_news.csv')\n",
        "\n",
        "# Replace NaN values with an empty string in both tweet and news data\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].fillna('')\n",
        "news_df['text'] = news_df['text'].fillna('')\n",
        "\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)\n",
        "\n",
        "\n",
        "\n",
        "encoder = LabelEncoder()     # Encoding Sentiment Labels (assuming 'sentiment' column with Positive, Negative, Neutral)\n",
        "tweets_df['encoded_sentiment'] = encoder.fit_transform(tweets_df['sentiment'])\n",
        "news_df['encoded_sentiment'] = encoder.fit_transform(news_df['sentiment'])\n",
        "\n",
        "y_tweets = tweets_df['encoded_sentiment']\n",
        "y_news = news_df['encoded_sentiment']\n",
        "\n",
        "# Tokenizing the Text Data (using preprocessed text column)\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(tweets_df['cleaned_tweet'])\n",
        "X_tweets = tokenizer.texts_to_sequences(tweets_df['cleaned_tweet'])\n",
        "X_tweets = pad_sequences(X_tweets, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(news_df['text'])\n",
        "X_news = tokenizer.texts_to_sequences(news_df['text'])\n",
        "X_news = pad_sequences(X_news, maxlen=100)\n",
        "\n",
        "# Step 6: Splitting Data into Training and Testing Sets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "# Building LSTM Model\n",
        "tweet_model = Sequential()\n",
        "tweet_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "tweet_model.add(SpatialDropout1D(0.2))\n",
        "tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "# Building LSTM Model for News Sentiment Classification\n",
        "news_model = Sequential()\n",
        "news_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "news_model.add(SpatialDropout1D(0.2))\n",
        "news_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "news_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "news_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "news_history = news_model.fit(X_train_news, y_train_news, epochs=5, batch_size=64, validation_data=(X_test_news, y_test_news), verbose=2)\n",
        "\n",
        "\n",
        "y_pred_tweets = tweet_model.predict(X_test_tweets) # Evaluate Tweets Model\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)\n",
        "\n",
        "y_pred_news = news_model.predict(X_test_news)      # Evaluate News Model\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)\n",
        "\n",
        "print(\"Tweets Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "\n",
        "print(\"News Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(confusion_matrix(y_test_news, y_pred_news))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWwYaHGC2jhe",
        "outputId": "33c1696b-69b1-479d-dfd7-83c00b9b3664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "719/719 - 133s - 186ms/step - accuracy: 0.7915 - loss: 0.5508 - val_accuracy: 0.8902 - val_loss: 0.3441\n",
            "Epoch 2/5\n",
            "719/719 - 128s - 178ms/step - accuracy: 0.9026 - loss: 0.3082 - val_accuracy: 0.9030 - val_loss: 0.3142\n",
            "Epoch 3/5\n",
            "719/719 - 127s - 177ms/step - accuracy: 0.9160 - loss: 0.2696 - val_accuracy: 0.9032 - val_loss: 0.3173\n",
            "Epoch 4/5\n",
            "719/719 - 127s - 177ms/step - accuracy: 0.9220 - loss: 0.2476 - val_accuracy: 0.9041 - val_loss: 0.3281\n",
            "Epoch 5/5\n",
            "719/719 - 128s - 177ms/step - accuracy: 0.9268 - loss: 0.2290 - val_accuracy: 0.9044 - val_loss: 0.3190\n",
            "Epoch 1/5\n",
            "4/4 - 5s - 1s/step - accuracy: 0.4440 - loss: 1.0916 - val_accuracy: 0.5873 - val_loss: 1.0706\n",
            "Epoch 2/5\n",
            "4/4 - 1s - 201ms/step - accuracy: 0.4920 - loss: 1.0615 - val_accuracy: 0.5873 - val_loss: 1.0083\n",
            "Epoch 3/5\n",
            "4/4 - 1s - 192ms/step - accuracy: 0.4920 - loss: 1.0248 - val_accuracy: 0.5873 - val_loss: 0.9894\n",
            "Epoch 4/5\n",
            "4/4 - 1s - 189ms/step - accuracy: 0.4920 - loss: 1.0057 - val_accuracy: 0.5873 - val_loss: 1.0021\n",
            "Epoch 5/5\n",
            "4/4 - 1s - 190ms/step - accuracy: 0.4960 - loss: 0.9737 - val_accuracy: 0.5873 - val_loss: 0.9945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7eff827a2fc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398ms/step\n",
            "Tweets Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.70      0.75      1522\n",
            "           1       0.92      0.94      0.93      4818\n",
            "           2       0.91      0.93      0.92      5160\n",
            "\n",
            "    accuracy                           0.90     11500\n",
            "   macro avg       0.88      0.86      0.87     11500\n",
            "weighted avg       0.90      0.90      0.90     11500\n",
            "\n",
            "[[1063  202  257]\n",
            " [  86 4518  214]\n",
            " [ 175  165 4820]]\n",
            "News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.00      0.00      0.00        17\n",
            "           2       0.59      1.00      0.74        37\n",
            "\n",
            "    accuracy                           0.59        63\n",
            "   macro avg       0.20      0.33      0.25        63\n",
            "weighted avg       0.34      0.59      0.43        63\n",
            "\n",
            "[[ 0  0  9]\n",
            " [ 0  0 17]\n",
            " [ 0  0 37]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for compiling is almost 30 mins**"
      ],
      "metadata": {
        "id": "RsELIzCk5qML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/5\n",
        "\n",
        "719/719 - 171s - loss: 0.5244 - accuracy: 0.8012 - val_loss: 0.3485 - val_accuracy: 0.8920 - 171s/epoch - 238ms/step\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "719/719 - 167s - loss: 0.3019 - accuracy: 0.9046 - val_loss: 0.3127 - val_accuracy: 0.9043 - 167s/epoch - 232ms/step\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "719/719 - 167s - loss: 0.2631 - accuracy: 0.9160 - val_loss: 0.3165 - val_accuracy: 0.9011 - 167s/epoch - 232ms/step\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "719/719 - 166s - loss: 0.2374 - accuracy: 0.9238 - val_loss: 0.3271 - val_accuracy: 0.9006 - 166s/epoch - 231ms/step\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "719/719 - 169s - loss: 0.2134 - accuracy: 0.9310 - val_loss: 0.3373 - val_accuracy: 0.9060 - 169s/epoch - 235ms/step\n",
        "\n",
        "Epoch 1/5\n",
        "\n",
        "4/4 - 4s - loss: 1.0942 - accuracy: 0.4520 - val_loss: 1.0805 - val_accuracy: 0.5873 - 4s/epoch - 1s/step\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0685 - accuracy: 0.4920 - val_loss: 1.0473 - val_accuracy: 0.5873 - 988ms/epoch - 247ms/step\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0333 - accuracy: 0.4920 - val_loss: 0.9976 - val_accuracy: 0.5873 - 947ms/epoch - 237ms/step\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "4/4 - 1s - loss: 1.0035 - accuracy: 0.4920 - val_loss: 0.9927 - val_accuracy: 0.5873 - 946ms/epoch - 236ms/step\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "4/4 - 1s - loss: 0.9696 - accuracy: 0.4920 - val_loss: 0.9814 - val_accuracy: 0.5873 - 923ms/epoch - 231ms/step\n",
        "\n",
        "360/360 [==============================] - 9s 24ms/step\n",
        "\n",
        "2/2 [==============================] - 0s 16ms/step\n",
        "\n",
        "Tweets Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.64      0.74      1522\n",
        "           1       0.91      0.96      0.93      4818\n",
        "           2       0.91      0.94      0.92      5160\n",
        "\n",
        "    accuracy                           0.91     11500\n",
        "\n",
        "   macro avg       0.89      0.85      0.86     11500\n",
        "\n",
        "weighted avg       0.90      0.91      0.90     11500\n",
        "\n",
        "\n",
        "[[ 979  222  321]\n",
        "\n",
        " [  62 4605  151]\n",
        "\n",
        " [  97  228 4835]]\n",
        "\n",
        "News Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.00      0.00      0.00         9\n",
        "           1       0.00      0.00      0.00        17\n",
        "           2       0.59      1.00      0.74        37\n",
        "\n",
        "    accuracy                           0.59        63\n",
        "\n",
        "   macro avg       0.20      0.33      0.25        63\n",
        "\n",
        "weighted avg       0.34      0.59      0.43        63\n",
        "\n",
        "\n",
        "[[ 0  0  9]\n",
        "\n",
        " [ 0  0 17]\n",
        "\n",
        " [ 0  0 37]]"
      ],
      "metadata": {
        "id": "ufbeO17xjyki"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mz0epRlEoob"
      },
      "source": [
        "# Sentiment Analysis- tesla"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALGORITHMS WORKS !!!"
      ],
      "metadata": {
        "id": "L_E86_qKSxVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "KsuScRGEXniM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load datasets\n",
        "tweets_df = pd.read_csv(\"/content/cleaned_tsla_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/cleaned_tsla_articles.csv\")\n",
        "\n",
        "# Ensure text column is string type\n",
        "tweets_df['Tweet'] = tweets_df['Tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "35c_SFF4Xv2F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "93ab13fe-2958-4015-a0d3-5543e86585df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# VADER Sentiment Analysis for Tweets\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "# Function to get sentiment score from VADER\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply VADER sentiment analysis to tweets\n",
        "tweets_df['sentiment'] = tweets_df['Tweet'].apply(get_vader_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "q-26gh4WXxx1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a7631eab-2ef1-4031-c78d-cf73d8b17abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! Results saved.\n"
          ]
        }
      ],
      "source": [
        "# TextBlob Sentiment Analysis for News\n",
        "\n",
        "# Function to get sentiment score from TextBlob\n",
        "def get_textblob_sentiment(text):\n",
        "    score = TextBlob(text).sentiment.polarity\n",
        "    return 1 if score > 0 else (-1 if score < 0 else 0)\n",
        "\n",
        "# Apply TextBlob sentiment analysis to news articles\n",
        "news_df['sentiment'] = news_df['text'].apply(get_textblob_sentiment)\n",
        "news_df['sentiment'] = news_df['sentiment'].astype(int)\n",
        "\n",
        "# Save sentiment-labeled datasets\n",
        "tweets_df.to_csv(\"sentiment_tsla_tweets.csv\", index=False)\n",
        "news_df.to_csv(\"sentiment_tsla_news.csv\", index=False)\n",
        "\n",
        "print(\"Sentiment analysis complete! Results saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Traditional ML methods for sentiment analysis"
      ],
      "metadata": {
        "id": "x1ysblnS2won"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_msft_news.csv\")\n",
        "\n",
        "tweets_df = tweets_df.sample(min(10000, len(tweets_df)), random_state=42)\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(\n",
        "    tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(\n",
        "    news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Function to train models\n",
        "def train_model(X_train, y_train, X_test, y_test, model):\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    predictions = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Model {model.__class__.__name__} Accuracy: {accuracy:.4f}\")\n",
        "    return pipeline\n",
        "\n",
        "# Models to Train\n",
        "models = {\n",
        "    'Naïve Bayes': MultinomialNB(),\n",
        "    'SVM': SVC(kernel='linear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
        "}\n",
        "\n",
        "# Train models for Tweets\n",
        "print(\"\\n Training Models for Tweet Sentiment Classification \")\n",
        "trained_models_tweets = {\n",
        "    name: train_model(X_tweets, y_tweets, X_test_tweets, y_test_tweets, model)\n",
        "    for name, model in models.items()\n",
        "}\n",
        "\n",
        "# Train models for News\n",
        "print(\"\\n Training Models for News Sentiment Classification \")\n",
        "trained_models_news = {\n",
        "    name: train_model(X_news, y_news, X_test_news, y_test_news, model)\n",
        "    for name, model in models.items()\n",
        "}\n"
      ],
      "metadata": {
        "id": "0WhdsIfQ4Gy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d741c01-620f-4a9e-ae01-094d3973885c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training Models for Tweet Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.7220\n",
            "Model SVC Accuracy: 0.8270\n",
            "Model DecisionTreeClassifier Accuracy: 0.7695\n",
            "Model RandomForestClassifier Accuracy: 0.7945\n",
            "\n",
            " Training Models for News Sentiment Classification \n",
            "Model MultinomialNB Accuracy: 0.6032\n",
            "Model SVC Accuracy: 0.6032\n",
            "Model DecisionTreeClassifier Accuracy: 0.5079\n",
            "Model RandomForestClassifier Accuracy: 0.5873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models for Tweet Sentiment Classification\n",
        "*   Model MultinomialNB Accuracy: 0.7215\n",
        "*   Model SVC Accuracy: 0.8285\n",
        "*   Model DecisionTreeClassifier Accuracy: 0.7720\n",
        "*   Model RandomForestClassifier Accuracy: 0.7880\n",
        "\n",
        "\n",
        "Training Models for News Sentiment Classification\n",
        "*   Model MultinomialNB Accuracy: 0.5873\n",
        "*   Model SVC Accuracy: 0.5556\n",
        "*   Model DecisionTreeClassifier Accuracy: 0.5397\n",
        "*   Model RandomForestClassifier Accuracy: 0.5238\n"
      ],
      "metadata": {
        "id": "YR9q619vF528"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ANN for Sentiment Analysis"
      ],
      "metadata": {
        "id": "DRJBd8JSFRzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load preprocessed datasets\n",
        "tweets_df = pd.read_csv(\"/content/sentiment_tsla_tweets.csv\")\n",
        "news_df = pd.read_csv(\"/content/sentiment_tsla_news.csv\")\n",
        "\n",
        "# Replace NaN values with an empty string in both tweet and news data\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].fillna('')\n",
        "news_df['text'] = news_df['text'].fillna('')\n",
        "\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)\n",
        "\n",
        "# Reduce dataset size to prevent memory overload\n",
        "tweets_df = tweets_df.sample(min(50000, len(tweets_df)), random_state=42)\n",
        "\n",
        "# Train-test split for Tweets\n",
        "X_tweets, X_test_tweets, y_tweets, y_test_tweets = train_test_split(tweets_df['cleaned_tweet'], tweets_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train-test split for News\n",
        "X_news, X_test_news, y_news, y_test_news = train_test_split(news_df['text'], news_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tweets = vectorizer.fit_transform(X_tweets).toarray()\n",
        "X_test_tweets = vectorizer.transform(X_test_tweets).toarray()\n",
        "X_news = vectorizer.fit_transform(X_news).toarray()\n",
        "X_test_news = vectorizer.transform(X_test_news).toarray()\n",
        "\n",
        "# Build ANN model\n",
        "def build_ann(input_dim):\n",
        "    model = Sequential([\n",
        "       Input(shape=(input_dim,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert Sentiment Labels into One-Hot Encoding\n",
        "y_tweets = to_categorical(y_tweets, num_classes=3)\n",
        "y_test_tweets = to_categorical(y_test_tweets, num_classes=3)\n",
        "y_news = to_categorical(y_news, num_classes=3)\n",
        "y_test_news = to_categorical(y_test_news, num_classes=3)\n",
        "\n",
        "# Train ANN model\n",
        "print(\"\\n Training ANN Model for Tweets Sentiment Classification \")\n",
        "ann_tweet_model = build_ann(X_tweets.shape[1])\n",
        "ann_tweet_model.fit(X_tweets, y_tweets, epochs=10, batch_size=32, validation_data=(X_test_tweets, y_test_tweets))\n",
        "\n",
        "# Train ANN for News\n",
        "print(\"\\n Training ANN Model for News Sentiment Classification \")\n",
        "ann_news_model = build_ann(X_news.shape[1])\n",
        "ann_news_model.fit(X_news, y_news, epochs=10, batch_size=32, validation_data=(X_test_news, y_test_news))\n",
        "\n",
        "y_test_tweets_labels = np.argmax(y_test_tweets, axis=1)\n",
        "y_pred_tweets = ann_tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)  # Convert probabilities to class labels\n",
        "print(\" ANN Tweet Sentiment Classification Report:\")   #  ANN Evaluation for Tweets\n",
        "print(classification_report(y_test_tweets_labels, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets_labels, y_pred_tweets))\n",
        "\n",
        "y_test_news_labels = np.argmax(y_test_news, axis=1)\n",
        "y_pred_news = ann_news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)  # Convert probabilities to class labels\n",
        "print(\"\\n ANN News Sentiment Classification Report:\")  # ANN Evaluation for News\n",
        "print(classification_report(y_test_news_labels, y_pred_news))\n",
        "print(confusion_matrix(y_test_news_labels, y_pred_news))\n"
      ],
      "metadata": {
        "id": "cMFlhzcjEe2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649598e1-cb11-419d-d9dc-67e5144e1d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training ANN Model for Tweets Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5911 - loss: 0.8692 - val_accuracy: 0.8128 - val_loss: 0.5148\n",
            "Epoch 2/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8580 - loss: 0.4208 - val_accuracy: 0.8320 - val_loss: 0.4719\n",
            "Epoch 3/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8844 - loss: 0.3514 - val_accuracy: 0.8327 - val_loss: 0.4725\n",
            "Epoch 4/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8999 - loss: 0.2985 - val_accuracy: 0.8360 - val_loss: 0.4799\n",
            "Epoch 5/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9123 - loss: 0.2628 - val_accuracy: 0.8336 - val_loss: 0.4912\n",
            "Epoch 6/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9258 - loss: 0.2304 - val_accuracy: 0.8295 - val_loss: 0.5232\n",
            "Epoch 7/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9365 - loss: 0.1993 - val_accuracy: 0.8299 - val_loss: 0.5504\n",
            "Epoch 8/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9496 - loss: 0.1666 - val_accuracy: 0.8281 - val_loss: 0.5817\n",
            "Epoch 9/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9594 - loss: 0.1431 - val_accuracy: 0.8276 - val_loss: 0.6255\n",
            "Epoch 10/10\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9661 - loss: 0.1173 - val_accuracy: 0.8253 - val_loss: 0.6596\n",
            "\n",
            " Training ANN Model for News Sentiment Classification \n",
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.3745 - loss: 1.0954 - val_accuracy: 0.4444 - val_loss: 1.0909\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6517 - loss: 1.0640 - val_accuracy: 0.5556 - val_loss: 1.0784\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7025 - loss: 1.0260 - val_accuracy: 0.5556 - val_loss: 1.0667\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6883 - loss: 0.9895 - val_accuracy: 0.5556 - val_loss: 1.0549\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6953 - loss: 0.9575 - val_accuracy: 0.5833 - val_loss: 1.0436\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7009 - loss: 0.9182 - val_accuracy: 0.5833 - val_loss: 1.0330\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6743 - loss: 0.8808 - val_accuracy: 0.5833 - val_loss: 1.0233\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6846 - loss: 0.8489 - val_accuracy: 0.5833 - val_loss: 1.0148\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7669 - loss: 0.7855 - val_accuracy: 0.5833 - val_loss: 1.0069\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7859 - loss: 0.7421 - val_accuracy: 0.5833 - val_loss: 1.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7effac33ed40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            " ANN Tweet Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86      3003\n",
            "           1       0.85      0.84      0.84      4236\n",
            "           2       0.76      0.75      0.76      2761\n",
            "\n",
            "    accuracy                           0.83     10000\n",
            "   macro avg       0.82      0.82      0.82     10000\n",
            "weighted avg       0.82      0.83      0.83     10000\n",
            "\n",
            "[[2624  186  193]\n",
            " [ 218 3556  462]\n",
            " [ 239  449 2073]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\n",
            " ANN News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        11\n",
            "           1       0.58      1.00      0.74        21\n",
            "           2       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.58        36\n",
            "   macro avg       0.19      0.33      0.25        36\n",
            "weighted avg       0.34      0.58      0.43        36\n",
            "\n",
            "[[ 0 11  0]\n",
            " [ 0 21  0]\n",
            " [ 0  4  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training ANN Model for Tweets Sentiment Classification\n",
        "Epoch 1/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 13s 10ms/step - accuracy: 0.5954 - loss: 0.8592 - val_accuracy: 0.8195 - val_loss: 0.5095\n",
        "Epoch 2/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 19s 9ms/step - accuracy: 0.8563 - loss: 0.4200 - val_accuracy: 0.8312 - val_loss: 0.4758\n",
        "Epoch 3/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 12s 9ms/step - accuracy: 0.8834 - loss: 0.3475 - val_accuracy: 0.8335 - val_loss: 0.4737\n",
        "Epoch 4/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 19s 8ms/step - accuracy: 0.8979 - loss: 0.3051 - val_accuracy: 0.8287 - val_loss: 0.4857\n",
        "Epoch 5/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 11s 9ms/step - accuracy: 0.9065 - loss: 0.2751 - val_accuracy: 0.8326 - val_loss: 0.5032\n",
        "Epoch 6/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 10s 8ms/step - accuracy: 0.9187 - loss: 0.2444 - val_accuracy: 0.8287 - val_loss: 0.5225\n",
        "Epoch 7/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 10s 8ms/step - accuracy: 0.9286 - loss: 0.2177 - val_accuracy: 0.8274 - val_loss: 0.5480\n",
        "Epoch 8/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 22s 9ms/step - accuracy: 0.9432 - loss: 0.1850 - val_accuracy: 0.8236 - val_loss: 0.5748\n",
        "Epoch 9/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 20s 9ms/step - accuracy: 0.9535 - loss: 0.1547 - val_accuracy: 0.8241 - val_loss: 0.6109\n",
        "Epoch 10/10\n",
        "1250/1250 ━━━━━━━━━━━━━━━━━━━━ 22s 10ms/step - accuracy: 0.9621 - loss: 0.1301 - val_accuracy: 0.8237 - val_loss: 0.6525\n",
        "\n",
        " Training ANN Model for News Sentiment Classification\n",
        "Epoch 1/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - accuracy: 0.3411 - loss: 1.0948 - val_accuracy: 0.5556 - val_loss: 1.0709\n",
        "Epoch 2/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.6497 - loss: 1.0626 - val_accuracy: 0.6111 - val_loss: 1.0583\n",
        "Epoch 3/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.6366 - loss: 1.0318 - val_accuracy: 0.5833 - val_loss: 1.0457\n",
        "Epoch 4/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.6529 - loss: 0.9907 - val_accuracy: 0.5833 - val_loss: 1.0327\n",
        "Epoch 5/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.6496 - loss: 0.9608 - val_accuracy: 0.5833 - val_loss: 1.0199\n",
        "Epoch 6/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.6454 - loss: 0.9265 - val_accuracy: 0.5833 - val_loss: 1.0069\n",
        "Epoch 7/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.6603 - loss: 0.8774 - val_accuracy: 0.5833 - val_loss: 0.9939\n",
        "Epoch 8/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.6713 - loss: 0.8481 - val_accuracy: 0.5833 - val_loss: 0.9822\n",
        "Epoch 9/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 0.6573 - loss: 0.8101 - val_accuracy: 0.5833 - val_loss: 0.9726\n",
        "Epoch 10/10\n",
        "5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - accuracy: 0.7021 - loss: 0.7406 - val_accuracy: 0.5833 - val_loss: 0.9644\n",
        "\n",
        "313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step\n",
        "\n",
        " ANN Tweet Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.85      0.89      0.87      3003\n",
        "           1       0.85      0.83      0.84      4236\n",
        "           2       0.76      0.74      0.75      2761\n",
        "\n",
        "    accuracy                           0.82     10000\n",
        "   macro avg       0.82      0.82      0.82     10000\n",
        "\n",
        "weighted avg       0.82      0.82      0.82     10000\n",
        "\n",
        "[[2659  167  177]\n",
        " [ 239 3534  463]\n",
        " [ 237  480 2044]]\n",
        "\n",
        "2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step\n",
        "\n",
        " ANN News Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.00      0.00      0.00        11\n",
        "           1       0.58      1.00      0.74        21\n",
        "           2       0.00      0.00      0.00         4\n",
        "\n",
        "    accuracy                           0.58        36\n",
        "   macro avg       0.19      0.33      0.25        36\n",
        "\n",
        "weighted avg       0.34      0.58      0.43        36\n",
        "\n",
        "[[ 0 11  0]\n",
        " [ 0 21  0]\n",
        " [ 0  4  0]]"
      ],
      "metadata": {
        "id": "W1bkesNyAF1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM for sentiment analysis"
      ],
      "metadata": {
        "id": "hzfKyY0Q2qBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IT WORKS !!!!!**"
      ],
      "metadata": {
        "id": "eCrzKdAZdGBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the preprocessed datasets for tweets and news\n",
        "tweets_df = pd.read_csv('/content/sentiment_tsla_tweets.csv')  # replace with your actual dataset path\n",
        "news_df = pd.read_csv('/content/sentiment_tsla_news.csv')  # replace with your actual news dataset path\n",
        "\n",
        "# Replace NaN values with an empty string in both tweet and news data\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].fillna('')\n",
        "news_df['text'] = news_df['text'].fillna('')\n",
        "\n",
        "tweets_df['cleaned_tweet'] = tweets_df['cleaned_tweet'].astype(str)\n",
        "news_df['text'] = news_df['text'].astype(str)\n",
        "\n",
        "\n",
        "# Encoding Sentiment Labels (assuming 'sentiment' column with Positive, Negative, Neutral)\n",
        "encoder = LabelEncoder()\n",
        "tweets_df['encoded_sentiment'] = encoder.fit_transform(tweets_df['sentiment'])\n",
        "news_df['encoded_sentiment'] = encoder.fit_transform(news_df['sentiment'])\n",
        "\n",
        "y_tweets = tweets_df['encoded_sentiment']\n",
        "y_news = news_df['encoded_sentiment']\n",
        "\n",
        "#Tokenizing the Text Data (using preprocessed 'cleaned_text')\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(tweets_df['cleaned_tweet'])\n",
        "X_tweets = tokenizer.texts_to_sequences(tweets_df['cleaned_tweet'])\n",
        "X_tweets = pad_sequences(X_tweets, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(news_df['text'])\n",
        "X_news = tokenizer.texts_to_sequences(news_df['text'])\n",
        "X_news = pad_sequences(X_news, maxlen=100)\n",
        "\n",
        "#Splitting Data into Training and Testing Sets\n",
        "X_train_tweets, X_test_tweets, y_train_tweets, y_test_tweets = train_test_split(X_tweets, y_tweets, test_size=0.2, random_state=42)\n",
        "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news, y_news, test_size=0.2, random_state=42)\n",
        "\n",
        "#Building ANN Model for Tweets Sentiment Classification\n",
        "tweet_model = Sequential()\n",
        "tweet_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "tweet_model.add(SpatialDropout1D(0.2))\n",
        "tweet_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "tweet_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "tweet_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "tweet_history = tweet_model.fit(X_train_tweets, y_train_tweets, epochs=5, batch_size=64, validation_data=(X_test_tweets, y_test_tweets), verbose=2)\n",
        "\n",
        "#Building ANN Model for News Sentiment Classification\n",
        "news_model = Sequential()\n",
        "news_model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "news_model.add(SpatialDropout1D(0.2))\n",
        "news_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "news_model.add(Dense(3, activation='softmax'))  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "news_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "news_history = news_model.fit(X_train_news, y_train_news, epochs=5, batch_size=64, validation_data=(X_test_news, y_test_news), verbose=2)\n",
        "\n",
        "# Step 9: Evaluating the Models\n",
        "# Evaluate Tweets Model\n",
        "y_pred_tweets = tweet_model.predict(X_test_tweets)\n",
        "y_pred_tweets = np.argmax(y_pred_tweets, axis=1)\n",
        "\n",
        "# Evaluate News Model\n",
        "y_pred_news = news_model.predict(X_test_news)\n",
        "y_pred_news = np.argmax(y_pred_news, axis=1)\n",
        "\n",
        "# Performance Metrics for Tweets\n",
        "print(\"Tweets Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_tweets, y_pred_tweets))\n",
        "print(confusion_matrix(y_test_tweets, y_pred_tweets))\n",
        "\n",
        "# Performance Metrics for News\n",
        "print(\"News Sentiment Classification Report:\")\n",
        "print(classification_report(y_test_news, y_pred_news))\n",
        "print(confusion_matrix(y_test_news, y_pred_news))"
      ],
      "metadata": {
        "id": "gx2A2ud-3pPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41943ea-1570-493f-c127-4314630bbf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "4544/4544 - 1334s - 294ms/step - accuracy: 0.8483 - loss: 0.4363 - val_accuracy: 0.8775 - val_loss: 0.3774\n",
            "Epoch 2/5\n",
            "4544/4544 - 1355s - 298ms/step - accuracy: 0.8803 - loss: 0.3644 - val_accuracy: 0.8867 - val_loss: 0.3531\n",
            "Epoch 3/5\n",
            "4544/4544 - 1327s - 292ms/step - accuracy: 0.8886 - loss: 0.3426 - val_accuracy: 0.8871 - val_loss: 0.3518\n",
            "Epoch 4/5\n",
            "4544/4544 - 1308s - 288ms/step - accuracy: 0.8923 - loss: 0.3281 - val_accuracy: 0.8883 - val_loss: 0.3496\n",
            "Epoch 5/5\n",
            "4544/4544 - 1304s - 287ms/step - accuracy: 0.8953 - loss: 0.3164 - val_accuracy: 0.8855 - val_loss: 0.3578\n",
            "Epoch 1/5\n",
            "3/3 - 7s - 2s/step - accuracy: 0.4437 - loss: 1.0925 - val_accuracy: 0.5833 - val_loss: 1.0673\n",
            "Epoch 2/5\n",
            "3/3 - 1s - 446ms/step - accuracy: 0.5845 - loss: 1.0506 - val_accuracy: 0.5833 - val_loss: 1.0140\n",
            "Epoch 3/5\n",
            "3/3 - 1s - 474ms/step - accuracy: 0.5845 - loss: 0.9891 - val_accuracy: 0.5833 - val_loss: 0.9804\n",
            "Epoch 4/5\n",
            "3/3 - 2s - 695ms/step - accuracy: 0.5845 - loss: 0.9610 - val_accuracy: 0.5833 - val_loss: 0.9789\n",
            "Epoch 5/5\n",
            "3/3 - 1s - 359ms/step - accuracy: 0.5845 - loss: 0.9229 - val_accuracy: 0.5833 - val_loss: 0.9701\n",
            "\u001b[1m2272/2272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 43ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step\n",
            "Tweets Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83     20075\n",
            "           1       0.89      0.94      0.91     21900\n",
            "           2       0.91      0.89      0.90     30720\n",
            "\n",
            "    accuracy                           0.89     72695\n",
            "   macro avg       0.88      0.88      0.88     72695\n",
            "weighted avg       0.89      0.89      0.89     72695\n",
            "\n",
            "[[16471  1578  2026]\n",
            " [  624 20633   643]\n",
            " [ 2401  1055 27264]]\n",
            "News Sentiment Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.00      0.00      0.00        11\n",
            "           2       0.58      1.00      0.74        21\n",
            "\n",
            "    accuracy                           0.58        36\n",
            "   macro avg       0.19      0.33      0.25        36\n",
            "weighted avg       0.34      0.58      0.43        36\n",
            "\n",
            "[[ 0  0  4]\n",
            " [ 0  0 11]\n",
            " [ 0  0 21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WAIT TIME FOR COMPILING IS ALMOST 2 HOURS**"
      ],
      "metadata": {
        "id": "Ucvv3z7uELdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/5\n",
        "\n",
        "4544/4544 - 1295s - 285ms/step - accuracy: 0.8476 - loss: 0.4395 - val_accuracy: 0.8785 - val_loss: 0.3745\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "4544/4544 - 1354s - 298ms/step - accuracy: 0.8808 - loss: 0.3646 - val_accuracy: 0.8867 - val_loss: 0.3514\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "4544/4544 - 1314s - 289ms/step - accuracy: 0.8887 - loss: 0.3425 - val_accuracy: 0.8873 - val_loss: 0.3500\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "4544/4544 - 1305s - 287ms/step - accuracy: 0.8927 - loss: 0.3280 - val_accuracy: 0.8895 - val_loss: 0.3490\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "4544/4544 - 1299s - 286ms/step - accuracy: 0.8956 - loss: 0.3159 - val_accuracy: 0.8879 - val_loss: 0.3549\n",
        "\n",
        "Epoch 1/5\n",
        "\n",
        "3/3 - 8s - 3s/step - accuracy: 0.3803 - loss: 1.0958 - val_accuracy: 0.5833 - val_loss: 1.0691\n",
        "\n",
        "Epoch 2/5\n",
        "\n",
        "3/3 - 1s - 377ms/step - accuracy: 0.5845 - loss: 1.0560 - val_accuracy: 0.5833 - val_loss: 1.0241\n",
        "\n",
        "Epoch 3/5\n",
        "\n",
        "3/3 - 1s - 254ms/step - accuracy: 0.5845 - loss: 0.9970 - val_accuracy: 0.5833 - val_loss: 0.9730\n",
        "\n",
        "Epoch 4/5\n",
        "\n",
        "3/3 - 1s - 395ms/step - accuracy: 0.5845 - loss: 0.9573 - val_accuracy: 0.5833 - val_loss: 0.9622\n",
        "\n",
        "Epoch 5/5\n",
        "\n",
        "3/3 - 1s - 434ms/step - accuracy: 0.5845 - loss: 0.9171 - val_accuracy: 0.5833 - val_loss: 0.9695\n",
        "\n",
        "WARNING:tensorflow:5 out of the last 363 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ab5ee69300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
        "\n",
        "2272/2272 ━━━━━━━━━━━━━━━━━━━━ 109s 48ms/step\n",
        "\n",
        "2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 470ms/step\n",
        "\n",
        "Tweets Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.81      0.83     20075\n",
        "           1       0.89      0.94      0.91     21900\n",
        "           2       0.90      0.90      0.90     30720\n",
        "\n",
        "    accuracy                           0.89     72695\n",
        "\n",
        "   macro avg       0.88      0.88      0.88     72695\n",
        "\n",
        "weighted avg       0.89      0.89      0.89     72695\n",
        "\n",
        "\n",
        "[[16164  1598  2313]\n",
        "\n",
        " [  571 20637   692]\n",
        "\n",
        " [ 1979   999 27742]]\n",
        "\n",
        "News Sentiment Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.00      0.00      0.00         4\n",
        "           1       0.00      0.00      0.00        11\n",
        "           2       0.58      1.00      0.74        21\n",
        "\n",
        "    accuracy                           0.58        36\n",
        "   macro avg       0.19      0.33      0.25        36\n",
        "\n",
        "weighted avg       0.34      0.58      0.43        36\n",
        "\n",
        "[[ 0  0  4]\n",
        " [ 0  0 11]\n",
        " [ 0  0 21]]\n"
      ],
      "metadata": {
        "id": "tlKzZwb_EP1c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZ_x93P_CL0"
      },
      "source": [
        "# Merging - Microsoft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "MOOOtCrJ_Erw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "stock_data = pd.read_csv(\"/content/msft_2019.csv\")\n",
        "tweets_sentiment = pd.read_csv(\"/content/sentiment_msft_tweets.csv\")\n",
        "news_sentiment = pd.read_csv(\"/content/sentiment_msft_news.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "3meXu3Bn_I4W"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to datetime\n",
        "stock_data['day_date'] = pd.to_datetime(stock_data['day_date'])\n",
        "tweets_sentiment['date'] = pd.to_datetime(tweets_sentiment['date'])\n",
        "news_sentiment['date'] = pd.to_datetime(news_sentiment['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "2kEJIXf7_nlC"
      },
      "outputs": [],
      "source": [
        "# Extract Month-Day format (MM-DD)\n",
        "stock_data['month_day'] = stock_data['day_date'].dt.strftime('%m-%d')\n",
        "tweets_sentiment['month_day'] = tweets_sentiment['date'].dt.strftime('%m-%d')\n",
        "news_sentiment['month_day'] = news_sentiment['date'].dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Nu4pkD6WMPd0"
      },
      "outputs": [],
      "source": [
        "# # Drop original 'date' columns (Optional)\n",
        "# stock_data.drop(columns=['day_date'], inplace=True)\n",
        "# tweets_sentiment.drop(columns=['date'], inplace=True)\n",
        "# news_sentiment.drop(columns=['date'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "yGlNKkuHW9uR"
      },
      "outputs": [],
      "source": [
        "# Stock & Tweets\n",
        "stock_tweet_merged = stock_data.merge(tweets_sentiment, on='month_day', how='left')\n",
        "stock_tweet_merged.fillna(0, inplace=True)\n",
        "stock_tweet_merged = stock_tweet_merged.rename(columns={'sentiment_vader': 'tweet_sentiment'})\n",
        "stock_tweet_merged.to_csv(\"Merge_MSFT_TWEET.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "yn7ufmAZL28I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a0c4e646-ab70-4df2-9d19-f48cb39b26f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Merging done! Two separate files created: one for Tweets, one for News.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-225be7571668>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
            "  stock_news_merged.fillna(0, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Stock & News\n",
        "stock_news_merged = stock_data.merge(news_sentiment, on='month_day', how='left')\n",
        "stock_news_merged.fillna(0, inplace=True)\n",
        "stock_news_merged = stock_news_merged.rename(columns={'sentiment': 'news_sentiment'})\n",
        "stock_news_merged.to_csv(\"Merge_MSFT_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Merging done! Two separate files created: one for Tweets, one for News.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GdhrQULVY5w"
      },
      "source": [
        "# Merging - Tesla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "DQzd4PPiMgjx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets (assuming they are already preprocessed)\n",
        "stock_data = pd.read_csv(\"/content/tsla_2019.csv\")\n",
        "tweets_sentiment = pd.read_csv(\"/content/sentiment_tsla_tweets.csv\")\n",
        "news_sentiment = pd.read_csv(\"/content/sentiment_tsla_news.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "4t4Q5R54VjSf"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to datetime\n",
        "stock_data['day_date'] = pd.to_datetime(stock_data['day_date'])\n",
        "tweets_sentiment['date'] = pd.to_datetime(tweets_sentiment['date'])\n",
        "news_sentiment['date'] = pd.to_datetime(news_sentiment['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "WVJFCM22Vk2l"
      },
      "outputs": [],
      "source": [
        "# Extract Month-Day format (MM-DD)\n",
        "stock_data['month_day'] = stock_data['day_date'].dt.strftime('%m-%d')\n",
        "tweets_sentiment['month_day'] = tweets_sentiment['date'].dt.strftime('%m-%d')\n",
        "news_sentiment['month_day'] = news_sentiment['date'].dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock & Tweets\n",
        "stock_tweet_merged = stock_data.merge(tweets_sentiment, on='month_day', how='left')\n",
        "stock_tweet_merged.fillna(0, inplace=True)\n",
        "stock_tweet_merged = stock_tweet_merged.rename(columns={'sentiment_vader': 'tweet_sentiment'})\n",
        "stock_tweet_merged.to_csv(\"Merge_TSLA_TWEET.csv\", index=False)"
      ],
      "metadata": {
        "id": "3HuglepuUq9p"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stock & News\n",
        "stock_news_merged = stock_data.merge(news_sentiment, on='month_day', how='left')\n",
        "stock_news_merged.fillna(0, inplace=True)\n",
        "stock_news_merged = stock_news_merged.rename(columns={'sentiment': 'news_sentiment'})\n",
        "stock_news_merged.to_csv(\"Merge_TSLA_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Merging done! Two separate files created: one for Tweets, one for News.\")"
      ],
      "metadata": {
        "id": "Ut3k4nQaUuRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9b806e32-6508-4931-d739-c955eff990d9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Merging done! Two separate files created: one for Tweets, one for News.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-df03a4931140>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
            "  stock_news_merged.fillna(0, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpHk36XNrwZL"
      },
      "source": [
        "# Stock Data Analysis (Microsoft)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Load and preprocess datasets\n",
        "stock_tweet = pd.read_csv(\"/content/Merge_MSFT_TWEET.csv\")\n",
        "stock_news = pd.read_csv(\"/content/Merge_MSFT_NEWS.csv\")\n",
        "\n",
        "# Convert date column to datetime\n",
        "stock_tweet['month_day'] = pd.to_datetime(stock_tweet['month_day'], format='%m-%d')\n",
        "stock_news['month_day'] = pd.to_datetime(stock_news['month_day'], format='%m-%d')\n",
        "\n",
        "# Sort data chronologically\n",
        "stock_tweet = stock_tweet.sort_values('month_day')\n",
        "stock_news = stock_news.sort_values('month_day')\n",
        "\n",
        "# Feature Engineering: Moving Averages & Volatility\n",
        "for df in [stock_tweet, stock_news]:\n",
        "    df['5_day_avg'] = df['close_value'].rolling(window=5).mean()\n",
        "    df['10_day_avg'] = df['close_value'].rolling(window=10).mean()\n",
        "    df['volatility'] = df['close_value'].pct_change().rolling(window=5).std()\n",
        "    # Handle sentiment column name correctly\n",
        "    if 'sentiment' in df.columns:\n",
        "        df['sentiment_7d_avg'] = df['sentiment'].rolling(window=7).mean()  # Tweets Sentiment\n",
        "    elif 'news_sentiment' in df.columns:\n",
        "        df['sentiment_7d_avg'] = df['news_sentiment'].rolling(window=7).mean()  # News Sentiment\n",
        "    df.fillna(0, inplace=True)  # Fill missing values\n",
        "\n",
        "# Save processed data\n",
        "stock_tweet.to_csv(\"MSFT_FEATURED_TWEET.csv\", index=False)\n",
        "stock_news.to_csv(\"MSFT_FEATURED_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Feature Engineering Completed!\")"
      ],
      "metadata": {
        "id": "u-udlLdQ0j5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "922e66fc-eb8f-4d03-d748-2b2199b02b5b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature Engineering Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling Features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Load Tweet Data\n",
        "tweet_data = pd.read_csv(\"MSFT_FEATURED_TWEET.csv\")\n",
        "scaled_data = scaler.fit_transform(tweet_data[['close_value', 'sentiment_7d_avg', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM (Using 60-day history)\n",
        "X, y = [], []\n",
        "for i in range(60, len(scaled_data)):\n",
        "    X.append(scaled_data[i-60:i])\n",
        "    y.append(scaled_data[i, 0])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Build Improved LSTM Model for Tweets\n",
        "model_tweet = Sequential()\n",
        "model_tweet.add(LSTM(units=100, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model_tweet.add(Dropout(0.3))\n",
        "model_tweet.add(LSTM(units=100))\n",
        "model_tweet.add(Dropout(0.3))\n",
        "model_tweet.add(Dense(units=1))\n",
        "\n",
        "model_tweet.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_tweet.fit(X, y, epochs=20, batch_size=32, verbose=2)\n",
        "\n",
        "# Save Model\n",
        "model_tweet.save(\"LSTM_MSFT_TWEET.h5\")\n",
        "print(\" LSTM Model Trained for Tweet Sentiment!\")"
      ],
      "metadata": {
        "id": "Jagdrk5rWK8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for above compiling is almost 1 hour 10 mins or so**"
      ],
      "metadata": {
        "id": "Yrcf2iKWFvt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load News Data\n",
        "news_data = pd.read_csv(\"MSFT_FEATURED_NEWS.csv\")\n",
        "scaled_news_data = scaler.fit_transform(news_data[['close_value', 'sentiment_7d_avg', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X_news, y_news = [], []\n",
        "for i in range(60, len(scaled_news_data)):\n",
        "    X_news.append(scaled_news_data[i-60:i])\n",
        "    y_news.append(scaled_news_data[i, 0])\n",
        "\n",
        "X_news, y_news = np.array(X_news), np.array(y_news)\n",
        "\n",
        "# Build Improved LSTM Model for News\n",
        "model_news = Sequential()\n",
        "model_news.add(LSTM(units=100, return_sequences=True, input_shape=(X_news.shape[1], X_news.shape[2])))\n",
        "model_news.add(Dropout(0.3))\n",
        "model_news.add(LSTM(units=100))\n",
        "model_news.add(Dropout(0.3))\n",
        "model_news.add(Dense(units=1))\n",
        "\n",
        "model_news.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_news.fit(X_news, y_news, epochs=20, batch_size=32, verbose=2)\n",
        "\n",
        "# Save Model\n",
        "model_news.save(\"LSTM_MSFT_NEWS.h5\")\n",
        "print(\" LSTM Model Trained for News Sentiment!\")"
      ],
      "metadata": {
        "id": "lmGhYB19Hsdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Models using MAE & RMSE\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Predictions\n",
        "tweet_pred = model_tweet.predict(X)\n",
        "news_pred = model_news.predict(X_news)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    print(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "evaluate_model(y, tweet_pred, \"LSTM Tweet Model\")\n",
        "evaluate_model(y_news, news_pred, \"LSTM News Model\")"
      ],
      "metadata": {
        "id": "fHmV6i3TMtch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of Predictions vs Actual\n",
        "def plot_predictions(actual, predicted, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(actual, label=\"Actual Prices\", color='blue')\n",
        "    plt.plot(predicted, label=\"Predicted Prices\", color='red', linestyle=\"dashed\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Stock Price\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_predictions(y, tweet_pred, \"LSTM Predictions for Tweet Sentiment Impact on Stock Prices\")\n",
        "plot_predictions(y_news, news_pred, \"LSTM Predictions for News Sentiment Impact on Stock Prices\")\n",
        "\n",
        "print(\" Visualization Complete!\")"
      ],
      "metadata": {
        "id": "q4XmAMCYVQ7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the date column is in datetime format\n",
        "stock_tweet['month_day'] = pd.to_datetime(stock_tweet['month_day'])\n",
        "\n",
        "# Create a figure\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot Sentiment Score with its own y-axis\n",
        "ax1.plot(stock_tweet['month_day'], stock_tweet['sentiment'].rolling(10).mean(),\n",
        "         label='Sentiment (10-day Avg)', color='blue')\n",
        "ax1.set_ylabel(\"Sentiment Score\", color='blue')\n",
        "ax1.set_ylim(-1, 1)  # Limit sentiment scores to a reasonable range\n",
        "\n",
        "# Create a second y-axis for Stock Price\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(stock_tweet['month_day'], stock_tweet['close_value'],\n",
        "         label='Stock Price', color='orange', alpha=0.7)\n",
        "ax2.set_ylabel(\"Stock Price\", color='orange')\n",
        "\n",
        "# Improve formatting\n",
        "ax1.set_xlabel(\"Date\")\n",
        "plt.title(\"Microsoft Stock Price vs Sentiment Trend\")\n",
        "\n",
        "# Add legends\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rKbBg0tG_0_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Data Analysis (Tesla)"
      ],
      "metadata": {
        "id": "W6lUyopWDhaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Load and preprocess datasets\n",
        "stock_tweet = pd.read_csv(\"/content/Merge_TSLA_TWEET.csv\")\n",
        "stock_news = pd.read_csv(\"/content/Merge_TSLA_NEWS.csv\")\n",
        "\n",
        "# Convert date column to datetime\n",
        "stock_tweet['month_day'] = pd.to_datetime(stock_tweet['month_day'], format='%m-%d')\n",
        "stock_news['month_day'] = pd.to_datetime(stock_news['month_day'], format='%m-%d')\n",
        "\n",
        "# Sort data chronologically\n",
        "stock_tweet = stock_tweet.sort_values('month_day')\n",
        "stock_news = stock_news.sort_values('month_day')\n",
        "\n",
        "# Feature Engineering: Moving Averages & Volatility\n",
        "for df in [stock_tweet, stock_news]:\n",
        "    df['5_day_avg'] = df['close_value'].rolling(window=5).mean()\n",
        "    df['10_day_avg'] = df['close_value'].rolling(window=10).mean()\n",
        "    df['volatility'] = df['close_value'].pct_change().rolling(window=5).std()\n",
        "    # Handle sentiment column name correctly\n",
        "    if 'sentiment' in df.columns:\n",
        "        df['sentiment_7d_avg'] = df['sentiment'].rolling(window=7).mean()  # Tweets Sentiment\n",
        "    elif 'news_sentiment' in df.columns:\n",
        "        df['sentiment_7d_avg'] = df['news_sentiment'].rolling(window=7).mean()  # News Sentiment\n",
        "    df.fillna(0, inplace=True)  # Fill missing values\n",
        "\n",
        "# Save processed data\n",
        "stock_tweet.to_csv(\"TSLA_FEATURED_TWEET.csv\", index=False)\n",
        "stock_news.to_csv(\"TSLA_FEATURED_NEWS.csv\", index=False)\n",
        "\n",
        "print(\" Feature Engineering Completed!\")"
      ],
      "metadata": {
        "id": "NUqsjBYBPrdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tweet Data\n",
        "tweet_data = pd.read_csv(\"/content/TSLA_FEATURED_TWEET.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(tweet_data[['close_value', 'sentiment_7d_avg', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X, y = [], []\n",
        "for i in range(60, len(scaled_data)):\n",
        "    X.append(scaled_data[i-60:i])\n",
        "    y.append(scaled_data[i, 0])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_tweet = Sequential()\n",
        "model_tweet.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(LSTM(units=50))\n",
        "model_tweet.add(Dropout(0.2))\n",
        "model_tweet.add(Dense(units=1))\n",
        "\n",
        "model_tweet.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_tweet.fit(X, y, epochs=7, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_tweet.save(\"LSTM_TSLA_TWEET.h5\")\n",
        "\n",
        "print(\" LSTM Model Trained for Tweet Sentiment!\")"
      ],
      "metadata": {
        "id": "Zs8reiH1Dooh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wait time for above compiling is almost 1hr 40 mins**"
      ],
      "metadata": {
        "id": "ecSFBf8uqZH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load News Data\n",
        "news_data = pd.read_csv(\"/content/TSLA_FEATURED_NEWS.csv\")\n",
        "\n",
        "# Scale Features\n",
        "scaled_news_data = scaler.fit_transform(news_data[['close_value', 'sentiment_7d_avg', '5_day_avg', '10_day_avg', 'volatility']])\n",
        "\n",
        "# Prepare Data for LSTM\n",
        "X_news, y_news = [], []\n",
        "for i in range(60, len(scaled_news_data)):\n",
        "    X_news.append(scaled_news_data[i-60:i])\n",
        "    y_news.append(scaled_news_data[i, 0])\n",
        "\n",
        "X_news, y_news = np.array(X_news), np.array(y_news)\n",
        "\n",
        "# Build LSTM Model\n",
        "model_news = Sequential()\n",
        "model_news.add(LSTM(units=50, return_sequences=True, input_shape=(X_news.shape[1], X_news.shape[2])))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(LSTM(units=50))\n",
        "model_news.add(Dropout(0.2))\n",
        "model_news.add(Dense(units=1))\n",
        "\n",
        "model_news.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_news.fit(X_news, y_news, epochs=25, batch_size=32)\n",
        "\n",
        "# Save Model\n",
        "model_news.save(\"LSTM_TSLA_NEWS.h5\")\n",
        "\n",
        "print(\"LSTM Model Trained for News Sentiment!\")"
      ],
      "metadata": {
        "id": "G2HAp1_UDtoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Predictions\n",
        "tweet_pred = model_tweet.predict(X)\n",
        "news_pred = model_news.predict(X_news)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    print(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "evaluate_model(y, tweet_pred, \"LSTM Tweet Model\")\n",
        "evaluate_model(y_news, news_pred, \"LSTM News Model\")"
      ],
      "metadata": {
        "id": "Fvt0-ZeQDuQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of Predictions vs Actual\n",
        "def plot_predictions(actual, predicted, title):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(actual, label=\"Actual Prices\", color='blue')\n",
        "    plt.plot(predicted, label=\"Predicted Prices\", color='red', linestyle=\"dashed\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Stock Price\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_predictions(y, tweet_pred, \"LSTM Predictions for Tweet Sentiment Impact on Stock Prices\")\n",
        "plot_predictions(y_news, news_pred, \"LSTM Predictions for News Sentiment Impact on Stock Prices\")\n",
        "\n",
        "print(\"Visualization Complete!\")"
      ],
      "metadata": {
        "id": "fzvsvJanDALP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oviKTfSTwEUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "collapsed_sections": [
        "RSSbeztl4SuC",
        "rtIxEm3w4aUw",
        "v7oM2YYybjpJ",
        "l1SpoBmabqms",
        "n-WWdulg7vQ_",
        "mmI7toQa7yyK",
        "bwsJ668S--1y",
        "8z3_YOhb2Zj-",
        "YVbBVat5FBW-",
        "-yDBO8Uy2frr",
        "1Mz0epRlEoob",
        "x1ysblnS2won",
        "DRJBd8JSFRzg",
        "hzfKyY0Q2qBJ",
        "RwZ_x93P_CL0",
        "6GdhrQULVY5w",
        "W6lUyopWDhaD"
      ],
      "mount_file_id": "11AmYuY6p0-2vnU8ZxORnVG2nVYHtb-Nj",
      "authorship_tag": "ABX9TyM/Dfy+sfCQB8C0d4z+hRsK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}